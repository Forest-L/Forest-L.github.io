<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>iptables on 一切皆有可能</title>
    <link>https://Forest-L.github.io/tags/iptables/</link>
    <description>Recent content in iptables on 一切皆有可能</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <copyright>Copyright © 2008–2020</copyright>
    <lastBuildDate>Tue, 07 Jul 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://Forest-L.github.io/tags/iptables/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>iptables指令将容器内部端口映射到外部宿主机端口指南</title>
      <link>https://Forest-L.github.io/post/the-iptables-directive-is-a-guide-to-mapping-ports-inside-containers-to-ports-on-external-hosts/</link>
      <pubDate>Tue, 07 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/the-iptables-directive-is-a-guide-to-mapping-ports-inside-containers-to-ports-on-external-hosts/</guid>
      <description>&lt;h4 id=&#34;背景&#34;&gt;背景：&lt;/h4&gt;
&lt;p&gt;docker run 某个容器，忘记了-p/-P 映射端口操作时，怎么把容器端口映射到主机上呢？以下描述的是如何通过iptables指令把容器端口映射到外部宿主机端口操作，防止容器重新创建。
宿主机docker启了一个容器，在容器里面又部署了一个pod，而部署pod这个服务是后续操作的，宿主机docker启容器时没有把端口映射出来，如何通过宿主机去访问pod服务。&lt;/p&gt;
&lt;h2 id=&#34;1相关联的认知&#34;&gt;1、相关联的认知&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;docker run -p: 指令中的小p为具体的宿主机端口映射到容器内部开放的网络端口上。&lt;/li&gt;
&lt;li&gt;docker run -P: 指令中的大P为随机选择一个宿主机端口映射到容器内部开放的网络端口上。&lt;/li&gt;
&lt;li&gt;docker run -p: 可以绑定多IP和端口（跟多个-p）。&lt;/li&gt;
&lt;li&gt;kubectl expose &amp;ndash;type=nodePort: 指令将容器内部端口映射到主机上(宿主机为随机端口，范围30000-32767/在service中编辑修改为具体端口)。&lt;/li&gt;
&lt;li&gt;kubectl expose &amp;ndash;type=lb: 指令，为直接将容器服务暴露出去。&lt;/li&gt;
&lt;li&gt;kubectl ingress: 它允许你基于路径或者子域名来路由流量到后端服务,7层协议http/https。&lt;/li&gt;
&lt;li&gt;kubectl port-forward: 将容器端口转发至本地端口，也可以转发TCP流量。&lt;/li&gt;
&lt;li&gt;kubectl kube-proxy: 只能转发HTTP流量。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2docker与iptables关系&#34;&gt;2、docker与iptables关系&lt;/h2&gt;
&lt;p&gt;源地址变换规则、目标地址变换规则、自定义限制外部ip规则、docker容器间通信iptables规则、docker网络与ip-forward和具体的用iptables指令将容器内部端口映射到外部宿主机端口操作指令。&lt;/p&gt;
&lt;h3 id=&#34;21源ip地址变换规则&#34;&gt;2.1、源ip地址变换规则&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Docker安装完成后，将默认在宿主机系统上增加一些iptables规则，以用于Docker容器和容器之间以及和外界的通信，可以使用iptables-save命令查看。&lt;/li&gt;
&lt;li&gt;其中nat表中的POSTROUTING链有这么一条规则&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE

参数说明：
-s ：源地址172.17.0.0/16
-o：指定数据报文流出接口为docker0
-j ：动作为MASQUERADE（地址伪装）
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;上面这条规则关系着Docker容器和外界的通信，含义是：
判断源地址为172.17.0.0/16的数据包（即Docker容器发出的数据），当不是从docker0网卡发出时做SNAT（源地址转换）。
这样一来，从Docker容器访问外网的流量，在外部看来就是从宿主机上发出的，外部感觉不到Docker容器的存在。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;22目标地址变换规则&#34;&gt;2.2、目标地址变换规则&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;从Docker容器访问外网的流量，在外部看来就是从宿主机上发出的，外部感觉不到Docker容器的存在。那么，外界想到访问Docker容器的服务时，同样需要相应的iptables规则.&lt;/li&gt;
&lt;li&gt;以启动tomcat容器，将其8080端口映射到宿主机上的8080端口为例,然后通过iptables-save查看：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;docker run -itd --name  tomcat -p 8080:8080 tomcat:latest
#iptables-save
*nat
-A POSTROUTING -s 172.18.0.2/32 -d 172.18.0.2/32 -p tcp -m tcp --dport 8080 -j MASQUERADE
...
*filter
-A DOCKER -d 172.18.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 8080 -j ACCEPT
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;可以看到，在nat、filter的Docker链中分别增加了一条规则&lt;/li&gt;
&lt;li&gt;这两条规则将访问宿主机8080端口的流量转发到了172.17.0.4的8080端口上（即真正提供服务的Docker容器IP和端口）。所以外界访问Docker容器是通过iptables做DNAT（目的地址转换）实现的。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;23自定义限制外部ip规则&#34;&gt;2.3、自定义限制外部ip规则&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Docker的forward规则默认允许所有的外部IP访问容器&lt;/li&gt;
&lt;li&gt;可以通过在filter的DOCKER链上添加规则来对外部的IP访问做出限制&lt;/li&gt;
&lt;li&gt;只允许源IP192.168.0.0/16的数据包访问容器，需要添加如下规则：
&lt;code&gt;iptables -I DOCKER -i docker0 ! -s 192.168.0.0/16 -j DROP&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;24docker容器间通信iptables规则&#34;&gt;2.4、docker容器间通信iptables规则&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;不仅仅是与外界间通信，Docker容器之间互个通信也受到iptables规则限制。&lt;/li&gt;
&lt;li&gt;同一台宿主机上的Docker容器默认都连在docker0网桥上，它们属于一个子网，这是满足相互通信的第一步。&lt;/li&gt;
&lt;li&gt;Docker daemon启动参数&amp;ndash;icc(icc参数表示是否允许容器间相互通信)设置为false时会在filter的FORWARD链中增加一条ACCEPT的规则（&amp;ndash;icc=true）：
&lt;code&gt;-A FORWARD -i docker0 -o docker0 -j ACCEPT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;当Docker datemon启动参数&amp;ndash;icc设置为false时，以上规则会被设置为DROP，Docker容器间的相互通信就被禁止,默认是ACCEPT。&lt;/li&gt;
&lt;li&gt;这种情况下，想让两个容器通信就需要在docker run时使用&amp;ndash;link选项。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;25docker网络与ip-forward&#34;&gt;2.5、docker网络与ip-forward&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;在Docker容器和外界通信的过程中，还涉及了数据包在多个网卡间的转发，如从docker0网卡转发到宿主机ens160网卡，这需要内核将ip-forward功能打开&lt;/li&gt;
&lt;li&gt;即将ip_forward系统参数设1：echo 1 &amp;gt; /proc/sys/net/ipv4/ip_forward&lt;/li&gt;
&lt;li&gt;Docker daemon启动的时候默认会将其设为1（&amp;ndash;ip-forward=true）&lt;/li&gt;
&lt;li&gt;永久生效的ip转发
&lt;code&gt;vim /etc/sysctl.conf&lt;/code&gt;
&lt;code&gt;net.ipv4.ip_forward = 1&lt;/code&gt;
&lt;code&gt;sysctl -p /etc/sysctl.conf&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;26iptables指令映射&#34;&gt;2.6、iptables指令映射&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;需要执行三条指令,其中就修改两个参数:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;iptables -t nat -A DOCKER -p tcp --dport ${YOURPORT} -j DNAT --to-destination ${CONTAINERIP}:${YOURPORT}

iptables -t nat -A POSTROUTING -j MASQUERADE -p tcp --source ${CONTAINERIP} --destination ${CONTAINERIP} --dport ${YOURPORT}

iptables -A DOCKER -j ACCEPT -p tcp --destination ${CONTAINERIP} --dport ${YOURPORT}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;${CONTAINERIP} 就是对应容器的ip地址，比如我的容器ip地址是 172.18.0.2 ，（容器的IP可以通过如下方式查看：a.在容器中：ip addr;b.在宿主机中: docker inspect 容器名 |grep IPAddress ）所以我就把上述的参数换成我的IP地址。&lt;/li&gt;
&lt;li&gt;${YOURPORT} 就是要映射出来的端口，我配置的是一个console平台，其端口是30880&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3注意地方及参考&#34;&gt;3、注意地方及参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如果容器是pod形式启的，上面iptables指令映射不适合，其中有对docker链的操作。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/19335444/how-do-i-assign-a-port-mapping-to-an-existing-docker-container&#34;&gt;映射port至存在的docker容器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/zh/docs/concepts/services-networking/service/&#34;&gt;k8s如何访问&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>ipv6地址搭建K8s</title>
      <link>https://Forest-L.github.io/post/ipv6-address-setup-k8s/</link>
      <pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/ipv6-address-setup-k8s/</guid>
      <description>&lt;h1 id=&#34;ipv6地址搭建k8s&#34;&gt;ipv6地址搭建K8s&lt;/h1&gt;
&lt;h3 id=&#34;环境&#34;&gt;环境&lt;/h3&gt;
&lt;p&gt;centos: 7.7
k8s: v1.16.0&lt;/p&gt;
&lt;h3 id=&#34;提前准备&#34;&gt;提前准备&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;修改主机名
&lt;code&gt;hostnamectl set-hostname node1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;添加ipv6地址及主机名
&lt;code&gt;vi /etc/hosts&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;添加操作系统的ipv6的参数，且使参数生效&lt;code&gt;sysctl -p&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vi /etc/sysctl.conf
net.ipv6.conf.all.disable_ipv6 = 0
net.ipv6.conf.default.disable_ipv6 = 0
net.ipv6.conf.lo.disable_ipv6 = 0
net.ipv6.conf.all.forwarding=1
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;开启ipv6,添加如下内容&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vi /etc/sysconfig/network
NETWORKING_IPV6=yes
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;开启网卡的ipv6,添加如下内容，最后执行reboot生效。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vi /etc/sysconfig/network-scripts/ifcfg-eth0
IPV6INIT=yes
IPV6_AUTOCONF=yes
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;关闭防火墙&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;systemctl stop firewalld
systemctl disable firewalld
setenforce 0
vi /etc/selinux/config
SELINUX=disabled
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;关闭虚拟内存,添加如下内容，最后通过执行sysctl -p /etc/sysctl.d/k8s.conf生效。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;swapoff -a
vi /etc/sysctl.d/k8s.conf 添加下面一行：
vm.swappiness=0
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;安装docker&#34;&gt;安装docker&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;yum install -y yum-utils device-mapper-persistent-data lvm2
yum install wget -y
wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo
sudo sed -i &#39;s+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+&#39; /etc/yum.repos.d/docker-ce.repo
yum makecache fast
yum install docker-ce -y
systemctl enable docker;systemctl restart docker
docker的配置vi /etc/docker/daemon.json
{
&amp;quot;insecure-registry&amp;quot;:[&amp;quot;0.0.0.0/0&amp;quot;],
&amp;quot;ipv6&amp;quot;: true,
&amp;quot;fixed-cidr-v6&amp;quot;: &amp;quot;2001:db8:1::/64&amp;quot;,
&amp;quot;host&amp;quot;:[&amp;quot;unix:///var/run/docker.sock&amp;quot;,&amp;quot;tcp://:::2375&amp;quot;],
&amp;quot;log-level&amp;quot;:&amp;quot;debug&amp;quot;
}
systemctl restart docker
echo &amp;quot;1&amp;quot; &amp;gt;/proc/sys/net/bridge/bridge-nf-call-iptables
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;安装kubectlkubeadm和kubelet插件&#34;&gt;安装kubectl、kubeadm和kubelet插件&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;添加k8s下载源
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

指定版本的安装
yum install kubelet-1.16.0 kubeadm-1.16.0 kubectl-1.16.0 -y
systemctl enable kubelet &amp;amp;&amp;amp; sudo systemctl start kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;初始化的准备&#34;&gt;初始化的准备&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;查看安装过程需要哪些镜像
&lt;code&gt;kubeadm config images list --kubernetes-version=v1.16.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;通过脚本下载所需的镜像。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vi images.sh 
#!/bin/bash
images=(kube-proxy:v1.16.0 kube-scheduler:v1.16.0 kube-controller-manager:v1.16.0 kube-apiserver:v1.16.0 etcd:3.3.15-0 pause:3.1 coredns:1.6.2)
for imageName in ${images[@]} ; do
docker pull gcr.azk8s.cn/google-containers/$imageName
docker tag gcr.azk8s.cn/google-containers/$imageName k8s.gcr.io/$imageName
docker rmi gcr.azk8s.cn/google-containers/$imageName
done
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;执行如下指令下载：&lt;code&gt;chmod +x images.sh &amp;amp;&amp;amp; ./images.sh&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;拷贝kubeadm.yaml文件，需要注意advertiseAddress参数为本机ipv6地址&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vi kubeadm.yaml 
apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: &amp;quot;2402:e7c0:0:a00:ffff:ffff:fffe:fffb&amp;quot;
  bindPort: 6443
nodeRegistration:
  taints:
  - effect: PreferNoSchedule
    key: node-role.kubernetes.io/master
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.16.0
networking:
  podSubnet: 1100::/52
  serviceSubnet: fd00:4000::/112
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;执行如下安装指令：&lt;code&gt;kubeadm init --config=kubeadm.yaml&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;如果使1.16.0之前版本需要安装指令后面添加如下参数执行：&lt;code&gt;--ignore-preflight-errors=HTTPProxy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;如果以下安装有问题，需要重置，先执行&lt;code&gt;kubeadm reset&lt;/code&gt;,再执行以上&lt;code&gt;kubeadm init&lt;/code&gt;指令，安装成功之后，需要做如下操作：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl taint node node1 node-role.kubernetes.io/master-
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;安装网络插件如calico&#34;&gt;安装网络插件，如calico&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;下载calico.yaml文件&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;curl https://docs.projectcalico.org/v3.11/manifests/calico.yaml -O
需要修改及添加的内容为,总共三处：
 &amp;quot;ipam&amp;quot;: {
     &amp;quot;type&amp;quot;: &amp;quot;calico-ipam&amp;quot;,
     &amp;quot;assign_ipv4&amp;quot;: &amp;quot;false&amp;quot;,
     &amp;quot;assign_ipv6&amp;quot;: &amp;quot;true&amp;quot;,
     &amp;quot;ipv4_pools&amp;quot;: [&amp;quot;172.16.0.0/16&amp;quot;, &amp;quot;default-ipv4-ippool&amp;quot;],
     &amp;quot;ipv6_pools&amp;quot;: [&amp;quot;1100::/52&amp;quot;, &amp;quot;default-ipv6-ippool&amp;quot;]
  },

- name: CALICO_IPV4POOL_CIDR
  value: &amp;quot;172.16.0.0/16&amp;quot;
- name: IP6
  value: &amp;quot;autodetect&amp;quot;
- name: CALICO_IPV6POOL_CIDR
  value: &amp;quot;1100::/52&amp;quot;

# Disable IPv6 on Kubernetes.
- name: FELIX_IPV6SUPPORT
  value: &amp;quot;true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;calico的镜像,可以提前下载
calico/cni:v3.11.1
calico/pod2daemon-flexvol:v3.11.1
calico/node:v3.11.1
calico/kube-controllers:v3.11.1&lt;/li&gt;
&lt;li&gt;执行calico，&lt;code&gt;kubectl apply -f calico.yaml&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;验证&#34;&gt;验证：&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl get pod --all-namespaces -o wide&lt;/code&gt;
&lt;code&gt;kubectl get nodes -o wide&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;部署tomcat应用验证&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl run tomcat  --image=tomcat:8.0  --port=8080
kubectl get pod
kubectl expose deployment tomcat  --port=8080 --target-port=8080 --type=NodePort
# kubectl get svc
NAME         TYPE        CLUSTER-IP        EXTERNAL-IP   PORT(S)          AGE
kubernetes   ClusterIP   fd00:4000::1      &amp;lt;none&amp;gt;        443/TCP          33m
tomcat       NodePort    fd00:4000::bf3e   &amp;lt;none&amp;gt;        8080:30693/TCP   22m

curl -6g [2402:e7c0:0:a00:ffff:ffff:fffe:fffb]:32012
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;参考&#34;&gt;参考&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kubernetes.org.cn/5173.html&#34;&gt;https://www.kubernetes.org.cn/5173.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>k8s之etcd数据的备份与恢复</title>
      <link>https://Forest-L.github.io/post/backup-and-restore-etcd-data-of-k8s/</link>
      <pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/backup-and-restore-etcd-data-of-k8s/</guid>
      <description>&lt;h1 id=&#34;k8s之etcd备份与恢复&#34;&gt;k8s之etcd备份与恢复&lt;/h1&gt;
&lt;p&gt;etcd是一款开源的分布式一致性键值存储。目前有版本为V3以上，但是它的API又有v2和v3之分，以至于操作指令也不一样。
查看etcd版本&lt;code&gt;etcdctl --version&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;说 明 : &lt;/font&gt;
若使用 v3 备份数据时存在 v2 的数据则不影响恢复&lt;/p&gt;
&lt;p&gt;若使用 v2 备份数据时存在 v3 的数据则恢复失败&lt;/p&gt;
&lt;h3 id=&#34;1对于api2备份与恢复方法&#34;&gt;1、对于API2备份与恢复方法&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;snap: 存放快照数据,etcd防止WAL文件过多而设置的快照，存储etcd数据状态。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;wal: 存放预写式日志,最大的作用是记录了整个数据变化的全部历程。在etcd中，所有数据的修改在提交前，都要先写入到WAL中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;备份指令：
&lt;code&gt;etcdctl backup --data-dir /home/etcd/ --backup-dir /home/etcd_backup&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;恢复指令：
&lt;code&gt;etcd -data-dir=/home/etcd_backup/ -force-new-cluster&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;恢复时会覆盖 snapshot 的元数据(member ID 和 cluster ID)，所以需要启动一个新的集群。&lt;/p&gt;
&lt;h3 id=&#34;2对于api3备份与恢复方法&#34;&gt;2、对于API3备份与恢复方法&lt;/h3&gt;
&lt;p&gt;在使用 API 3 时需要使用环境变量 ETCDCTL_API 明确指定。
&lt;code&gt;export ETCDCTL_API=3&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;21备份数据&#34;&gt;2.1备份数据&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;etcdctl --endpoints localhost:2379 \
   --cert=/etc/ssl/etcd/ssl/node-master.pem \
   --key=/etc/ssl/etcd/ssl/node-master-key.pem \
   --cacert=/etc/ssl/etcd/ssl/ca.pem \snapshot save \
   /var/backups/kube_etcd/snapshot.db
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;font color=#DC143C &gt;说 明 : &lt;/font&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;/var/backups/kube_etcd这个目录是根宿主机的/var/lib/etcd目录相映射的，所以备份在这个目录在对应的宿主机上也是能看见的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这些证书对应文件可以直接在etcd容器内通过ps aux|more看见
其中–cert-file对应–cert，–key对应–key-file –cacert对应–trusted-ca-file&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;22恢复数据&#34;&gt;2.2恢复数据&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;停止三台master节点的kube-apiserver，指令为：
&lt;code&gt;mv /etc/kubernetes/manifests/kube-apiserver.yaml /etc/kubernetes/kube-apiserver.yaml&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在三个master节点停止 etcd 服务,指令为：
&lt;code&gt;systemctl stop etcd&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在三个master节点转移并备份当前 etcd 集群数据,指令为：
&lt;code&gt;mv /var/lib/etcd /var/lib/etcd.bak&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将最新的etcd备份数据恢复至三个master节点，其中master_ip为不同master主机的IP
指令为：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;export ETCDCTL_API=3 和
etcdctl snapshot restore /var/backups/kube_etcd/etcd-******/snapshot.db \
   --endpoints=master_ip:2379 \
   --cert=/etc/ssl/etcd/ssl/node-master.pem \
   --key=/etc/ssl/etcd/ssl/node-master-key.pem \
   --cacert=/etc/ssl/etcd/ssl/ca.pem \
   --data-dir=/var/lib/etcd
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;执行 etcd 恢复命令,指令为：
&lt;code&gt;systemctl restart etcd&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;重启 kube-apiserver,指令为：
&lt;code&gt;mv /etc/kubernetes/kube-apiserver.yaml /etc/kubernetes/manifests/kube-apiserver.yaml &lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;检查是否正常,指令为：
&lt;code&gt;kubectl get pod  --all-namespaces&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;检查etcd集群状态及成员指令为：
&lt;code&gt;etcdctl --endpoints=https://192.168.0.91:2379 --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-ks-allinone.pem --key=/etc/ssl/etcd/ssl/node-ks-allinone-key.pem member list&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>k8s1.15.3在ubuntu系统部署</title>
      <link>https://Forest-L.github.io/post/k8s1-15-3-install-on-the-ubuntu/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s1-15-3-install-on-the-ubuntu/</guid>
      <description>&lt;h1 id=&#34;k8s1153在ubuntu系统部署&#34;&gt;k8s1.15.3在ubuntu系统部署&lt;/h1&gt;
&lt;p&gt;国内环境下，k8s1.15.3在ubuntu系统部署，相关的镜像以及docker的deb包和k8s核心组件的deb包在以下百度链接下。
&lt;a href=&#34;https://pan.baidu.com/s/1FqfkBiRfa03xaKbmKnqI2w&#34;&gt;k8s介质&lt;/a&gt;
提取码：05ef&lt;/p&gt;
&lt;h4 id=&#34;配置&#34;&gt;配置&lt;/h4&gt;
&lt;p&gt;2核4G
k8s：v1.15.3
ubuntu:18.04&lt;/p&gt;
&lt;h3 id=&#34;1-前期准备&#34;&gt;1. 前期准备&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;关闭ufw防火墙,Ubuntu默认未启用,无需设置。
&lt;code&gt;sudo ufw disable&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;禁用SELINUX （ubuntu19.04默认不安装）
&lt;code&gt;sudo setenforce 0&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;开启数据包转发,修改/etc/sysctl.conf，开启ipv4转发
&lt;code&gt;net.ipv4.ip_forward=1 注释取消&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;防火墙修改FORWARD链默认策略
&lt;code&gt;sudo iptables -P FORWARD ACCEPT&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;禁用swap
&lt;code&gt;sudo swapoff -a&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置iptables参数，使得流经网桥的流量也经过iptables/netfilter防火墙&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sudo tee /etc/sysctl.d/k8s.conf &amp;lt;&amp;lt;-&#39;EOF&#39;
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sudo sysctl --system
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;2docker安装&#34;&gt;2.docker安装&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;安装deb包,通过dpkg指令&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;dpkg -i containerd.io_1.2.5-1_amd64.deb &amp;amp;&amp;amp; \
dpkg -i docker-ce-cli_18.09.5~3-0~ubuntu-bionic_amd64.deb &amp;amp;&amp;amp; \
dpkg -i docker-ce_18.09.5~3-0~ubuntu-bionic_amd64.deb
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;docker使用加速器（阿里云加速器）&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;tee /etc/docker/daemon.json &amp;lt;&amp;lt;- &#39;EOF&#39;
{
&amp;quot;registry-mirrors&amp;quot;: [&amp;quot;https://5xcgs6ii.mirror.aliyuncs.com&amp;quot;]
}
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;设置docker开机自启动
&lt;code&gt;sudo systemctl enable docker &amp;amp;&amp;amp; sudo systemctl start docker&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3安装kubeadmkubeletkubectl&#34;&gt;3.安装kubeadm、kubelet、kubectl&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;通过dpkg -i 来安装k8s核心组件，指令如下&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;dpkg -i cri-tools_1.13.0-00_amd64.deb  kubernetes-cni_0.7.5-00_amd64.deb socat_1.7.3.2-2ubuntu2_amd64.deb conntrack_1%3a1.4.4+snapshot20161117-6ubuntu2_amd64.deb kubelet_1.15.3-00_amd64.deb  kubectl_1.15.3-00_amd64.deb kubeadm_1.15.3-00_amd64.deb
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;设置开机自启动
&lt;code&gt;sudo systemctl enable kubelet &amp;amp;&amp;amp; sudo systemctl start kubelet&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4kubeadm-init初始化集群&#34;&gt;4.kubeadm init初始化集群&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;先要将需要的镜像解压
&lt;code&gt;docker load -i k8s1153.tar&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;查看Kubernetes需要哪些镜像
&lt;code&gt;kubeadm config images list --kubernetes-version=v1.15.3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;注意apiserver-advertise-address要换成本机的IP&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sudo kubeadm init --apiserver-advertise-address=192.168.11.21 --pod-network-cidr=172.16.0.0/16 --service-cidr=10.233.0.0/16 --kubernetes-version=v1.15.3
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;创建kubectl使用的kubeconfig文件
&lt;code&gt;mkdir -p $HOME/.kube&lt;/code&gt;
&lt;code&gt;sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config&lt;/code&gt;
&lt;code&gt;sudo chown $(id -u):$(id -g) $HOME/.kube/config&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建flannel的pod，命令如下
以下两个文件在百度下链接下。
&lt;code&gt;kubectl create -f kube-flannel.yml&lt;/code&gt;
&lt;code&gt;kubectl apply -f weave-net.yml&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;5检查集群及重新添加节点&#34;&gt;5.检查集群及重新添加节点&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;检查node是否ready
&lt;code&gt;kubectl get nodes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;检查pod是否running
&lt;code&gt;kubectl get pod --all-namespaces -o wide&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;添加节点，如果忘记token了，可以在master上面执行如下指令获取
&lt;code&gt;kubeadm token list&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;添加节点，需要提前在新的机器上安装kubelet等服务及需要把相关的镜像拷贝过去解压。最后通过如下指令添加：
&lt;code&gt;kubeadm join –token=4fccd2.b0e0f8918bd95d3e 192.168.11.21:6443&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;参考文档&#34;&gt;参考文档&lt;/h5&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/qq_42346414/article/details/89949380&#34;&gt;参考&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>k8s之helm部署及用法</title>
      <link>https://Forest-L.github.io/post/helm-deployment-and-guide/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/helm-deployment-and-guide/</guid>
      <description>&lt;h1 id=&#34;k8s之helm部署及用法&#34;&gt;k8s之helm部署及用法&lt;/h1&gt;
&lt;p&gt;Helm是Kubernetes的一个包管理工具，用来简化Kubernetes应用的部署和管理。可以把Helm比作CentOS的yum工具。所以可以把该包在不同环境下部署起来,前提需要部署k8s环境。&lt;/p&gt;
&lt;h2 id=&#34;1-helm部署&#34;&gt;1. helm部署&lt;/h2&gt;
&lt;p&gt;Helm由两部分组成，客户端helm和服务端tiller。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tiller运行在Kubernetes集群上，管理chart安装的release&lt;/li&gt;
&lt;li&gt;helm是一个命令行工具，可在本地运行，一般运行在CI/CD Server上。一般我们用helm操作&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;11-客户端helm和服务端tiller安装以1921681120为例&#34;&gt;1.1 客户端helm和服务端tiller安装，以192.168.11.20为例&lt;/h3&gt;
&lt;p&gt;下载地址：https://github.com/helm/helm/releases
这里可以下载的是helm v2.14.1，解压缩后将可执行文件helm拷贝到/usr/local/bin下。这样客户端helm就在这台机器上安装完成了。
通过&lt;code&gt;helm version&lt;/code&gt;显示客户端安装好了，但是服务端没有好.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm version
Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.14.1&amp;quot;, GitCommit:&amp;quot;5270352a09c7e8b6e8c9593002a73535276507c0&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}
Error: Get http://localhost:8080/api/v1/namespaces/kube-system/pods?labelSelector=app%3Dhelm%2Cname%3Dtiller: dial tcp [::1]:8080: connect: connection refused
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;为了安装服务端tiller，还需要在这台机器上配置好kubectl工具和kubeconfig文件，确保kubectl工具可以在这台机器上访问apiserver且正常使用。
&lt;code&gt;kubectl get cs&lt;/code&gt;
使用helm在k8s上部署tiller：
&lt;code&gt;helm init --service-account tiller --skip-refresh&lt;/code&gt;
&lt;font color=#DC143C &gt;说明:&lt;/font&gt;
如果网络原因不能访问gcr.io，可以通过helm init –service-account tiller –tiller-image &lt;your-docker-registry&gt;/tiller:2.7.2 –skip-refresh使用私有镜像仓库中的tiller镜像。ps:lilinlinlin/tiller:2.7.2
tiller默认被部署在k8s集群中的kube-system这个namespace下。
&lt;code&gt;kubectl get pod -n kube-system -l app=helm&lt;/code&gt;
再次helm version可以打印客户端和服务端的版本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm version
Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.7.2&amp;quot;, GitCommit:&amp;quot;8
Server: &amp;amp;version.Version{SemVer:&amp;quot;v2.7.2&amp;quot;,
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2-kubernetes-rbac配置&#34;&gt;2. kubernetes RBAC配置&lt;/h2&gt;
&lt;p&gt;因为我们将tiller部署在Kubernetes 1.8上，Kubernetes APIServer开启了RBAC访问控制，所以我们需要创建tiller使用的service account: tiller并分配合适的角色给它。
这里简单起见直接分配cluster-admin这个集群内置的ClusterRole给它。创建&lt;code&gt;vi helm-rbac.yaml&lt;/code&gt;文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f helm-rbac.yaml&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3添加国内helm源&#34;&gt;3.添加国内helm源&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;helm repo add stable http://mirror.azure.cn/kubernetes/charts/&lt;/code&gt;
&lt;code&gt;helm repo add apphub https://apphub.aliyuncs.com&lt;/code&gt;
更新chart repo: &lt;code&gt;helm repo update&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;4-helm的基本使用&#34;&gt;4. helm的基本使用&lt;/h2&gt;
&lt;p&gt;下面我们开始尝试创建一个chart，这个chart用来部署一个简单的服务&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm create hello-test
Creating hello-test

tree hello-test/
hello-test/
├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── ingress.yaml
│   ├── NOTES.txt
│   └── service.yaml
└── values.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;charts目录中是本chart依赖的chart，当前是空的&lt;/li&gt;
&lt;li&gt;Chart.yaml这个yaml文件用于描述Chart的基本信息，如名称版本等&lt;/li&gt;
&lt;li&gt;templates是Kubernetes manifest文件模板目录，模板使用chart配置的值生成Kubernetes manifest文件。&lt;/li&gt;
&lt;li&gt;templates/NOTES.txt 纯文本文件，可在其中填写chart的使用说明&lt;/li&gt;
&lt;li&gt;value.yaml 是chart配置的默认值
在 values.yaml 中，可以看到，默认创建的是一个 Nginx 应用。为了方便外网访问测试，将 values.yaml 中 service 的属性修改为:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;service:
  type: NodePort
  port: 30080
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;41-部署应用&#34;&gt;4.1 部署应用&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;helm install ./hello-test&lt;/code&gt;
但实际上可以这样部署为,&lt;strong&gt;.tgz为chart包，&lt;/strong&gt;.yaml类似与values.yaml把变量文件定义出来。
&lt;code&gt;helm upgrade --install &amp;lt;name&amp;gt; **.tgz **.yaml --namespace &amp;lt;namespace-name&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;42-查看部署应用&#34;&gt;4.2 查看部署应用&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;helm list&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;43-删除部署应用&#34;&gt;4.3 删除部署应用&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;helm delete &amp;lt;name&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;44-打包chart&#34;&gt;4.4 打包chart&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;helm package &amp;lt;name&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考：&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kubernetes.org.cn/3435.html&#34;&gt;https://www.kubernetes.org.cn/3435.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>kubekey源码解读</title>
      <link>https://Forest-L.github.io/post/kubekey-source-code-interpretation/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/kubekey-source-code-interpretation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>kubernetes一致性认证的提交操作指南</title>
      <link>https://Forest-L.github.io/post/kubernetes-compliance-certification-submission-instructions/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/kubernetes-compliance-certification-submission-instructions/</guid>
      <description>&lt;p&gt;软件一致性尤为重要，它可以避免分裂，使众厂商将精力聚焦于共同推动软件发展而不是自成一家。2017年CNCF启动了Kubernetes一致性认证计划，CNCF提供一套测试工具，各厂商按照操作指导进行测试自身的产品，将测试报告上传给CNCF社区，CNCF审核测试报告后，会给符合条件的企业颁发一个证书。
大致流程：&lt;img src=&#34;https://ww1.sinaimg.cn/large/006bbiLEgy1gfsyxpcmmej30mx09zwew.jpg&#34; alt=&#34;操作流程图.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-环境信息&#34;&gt;1. 环境信息&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;亚太机器ubuntu（16.04.6）两台，192.168.0.3/192.168.0.4&lt;/li&gt;
&lt;li&gt;k8s1.18.3（1master+1node）（至少两台机器）&lt;/li&gt;
&lt;li&gt;sonobuoy0.18.3&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2k8s及云平台的部署&#34;&gt;2.k8s及云平台的部署&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;参考官网链接部署&lt;a href=&#34;https://github.com/kubesphere/kubekey&#34;&gt;K8s及云平台的部署&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;K8s版本为1.18.3及KubeSphere版本为v3.0.0，指令如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;curl -O -k https://kubernetes.pek3b.qingstor.com/tools/kubekey/kk
chmod +x kk
./kk create cluster --with-kubernetes v1.18.3 --with-kubesphere
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3-sonobuoy组件部署及运行&#34;&gt;3. sonobuoy组件部署及运行&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;sonobuoy二进制文件下载&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;wget https://github.com/vmware-tanzu/sonobuoy/releases/download/v0.18.3/sonobuoy_0.18.3_linux_amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;解压，&lt;code&gt;tar -xzvf sonobuoy_0.18.3_linux_amd64.tar.gz&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;配置环境变量，sonobuoy执行文件拷贝到/usr/local/bin下 &lt;code&gt;cp sonobuoy /usr/local/bin/&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-sonobuoy执行及相关指令&#34;&gt;4. sonobuoy执行及相关指令&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;在集群中部署一个sonobuoy的pod，&amp;ndash;mode=certified-conformance参数在Kubernetesv1.16(Sonobuoy v0.16)需要添加，指令为：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sonobuoy run --mode=certified-conformance
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;sonobuoy运行状态&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sonobuoy status
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;详细的日志&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sonobuoy logs
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;通过sonobuoy status显示“completed”，可以通过如下指令获取输出结果，需要提交的内容在plugins/e2e/results/global/{e2e.log,junit_01.xml}目录下&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sonobuoy retrieve
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;删除sonobuoy组件&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sonobuoy delete
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;5-提交的pr包含内容&#34;&gt;5. 提交的pr包含内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;5.1 fork k8s-conformance代码到你GitHub账号下，然后git clone到本地。&lt;/li&gt;
&lt;li&gt;5.2 在本地找到对应的k8s版本号，然后在里面建相关的名字即可，比如在v1.18版本下创建KubeSphere目录。&lt;/li&gt;
&lt;li&gt;5.3 自己项目目录下，需要包含以下四个文件，e2e.log、junit_01.xml、PRODUCT.yaml和README.md&lt;/li&gt;
&lt;li&gt;5.4 e2e.log和junit_01.xml两个文件是通过上面四步骤下解压的两个文件。&lt;/li&gt;
&lt;li&gt;5.5 PRODUCT.yaml包含的内容大致为：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vendor: 组织结构
name: 项目名
version: 版本号
website_url: 项目官方浏览页
repo_url: 项目官方镜像仓库地址
documentation_url: 项目官方文档
product_logo_url: 项目log图标
type: 开源/非开源
description: 项目的描述
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;5.6 README.md包含的内容大致为：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;To reproduce:
本身项目的安装方法
sonobuoy项目的安装方法
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;5.7 提交代码到自己GitHub账号下，然后提pr即可。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;6-参考文章&#34;&gt;6. 参考文章&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cncf/k8s-conformance/blob/master/instructions.md&#34;&gt;https://github.com/cncf/k8s-conformance/blob/master/instructions.md&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>tcpdump抓包实战教程</title>
      <link>https://Forest-L.github.io/post/tcpdump-package-capture-tutorial/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/tcpdump-package-capture-tutorial/</guid>
      <description>&lt;h1 id=&#34;tcpdump抓包实战教程&#34;&gt;tcpdump抓包实战教程&lt;/h1&gt;
&lt;p&gt;做 web 开发，接口对接过程中，分析 http 请求报文数据包格式是否正确，定位问题，省去无用的甩锅过程，再比如抓取 tcp/udp 报文，分析 tcp 连接过程中的三次握手和四次挥手。windows使用wireshark工具，Linux使用的是tcpdump工具，也可以生成.pcap文件在wireshark图形化工具上分析。&lt;/p&gt;
&lt;h3 id=&#34;命令行&#34;&gt;命令行&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;-i 选择网卡接口&lt;/li&gt;
&lt;li&gt;-n： 不解析主机名&lt;/li&gt;
&lt;li&gt;-nn：不解析端口&lt;/li&gt;
&lt;li&gt;port 80： 抓取80端口上面的数据&lt;/li&gt;
&lt;li&gt;tcp： 抓取tcp的包&lt;/li&gt;
&lt;li&gt;udp：抓取udp的包&lt;/li&gt;
&lt;li&gt;-w： 保存成pcap文件&lt;/li&gt;
&lt;li&gt;dst：目的ip&lt;/li&gt;
&lt;li&gt;src：源ip&lt;/li&gt;
&lt;li&gt;-c：&amp;lt;数据包数目&amp;gt;&lt;/li&gt;
&lt;li&gt;-s0表示可按包长显示完整的包&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tcp标志位&#34;&gt;tcp标志位&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SYN，显示为S，同步标志位，用于建立会话连接，同步序列号；&lt;/li&gt;
&lt;li&gt;ACK，显示为.，确认标志位，对已接收的数据包进行确认；&lt;/li&gt;
&lt;li&gt;FIN，显示为F，完成标志位，表示我已经没有数据要发送了，即将关闭连接；&lt;/li&gt;
&lt;li&gt;RESET，显示为R，重置标志位，用于连接复位、拒绝错误和非法的数据包；&lt;/li&gt;
&lt;li&gt;PUSH，显示为P，推送标志位，表示该数据包被对方接收后应立即交给上层应用，而不在缓冲区排队；&lt;/li&gt;
&lt;li&gt;URGENT，显示为U，紧急标志位，表示数据包的紧急指针域有效，用来保证连接不被阻断，并督促中间设备尽&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;nginx连接问题&#34;&gt;nginx连接问题。&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;抓取nginx(端口30880)交互的包：我们知道nginx交互其实是tcp协议，因此使用如下命令
&lt;code&gt;tcpdump -i eth0 tcp and port 30880 -n -nn -C 20 -W 50 -s 0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;tcp建立连接：先在服务的机器上执行如下命令，接着把multinode.ks.dev.chenshaowen.com:30880的url在浏览器访问即可。以下标志位含义可以参考上面描述。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[root@master ~]#tcpdump -i eth0 tcp and port 30880 -n -nn -C 20 -W 50 -s 0 
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes
10:44:36.391603 IP 139.198.254.12.61476 &amp;gt; 192.168.12.2.30880: Flags [S], seq 3950767038, win 64240, options [mss 1360,nop,wscale 8,nop,nop,sackOK], length 0
10:44:36.391864 IP 192.168.12.2.30880 &amp;gt; 139.198.254.12.61476: Flags [S.], seq 1197638836, ack 3950767039, win 29200, options [mss 1460,nop,nop,sackOK,nop,wscale 7], length 0
10:44:36.447960 IP 139.198.254.12.61478 &amp;gt; 192.168.12.2.30880: Flags [.], ack 1, win 515, length 0
10:44:36.448242 IP 139.198.254.12.61476 &amp;gt; 192.168.12.2.30880: Flags [.], ack 1, win 515, length 0
10:44:36.592856 IP 192.168.12.2.30880 &amp;gt; 139.198.254.12.61476: Flags [P.], seq 5250:6642, ack 1511, win 252, length 1392
10:44:36.647585 IP 139.198.254.12.61476 &amp;gt; 192.168.12.2.30880: Flags [.], ack 6642, win 515, length 0
10:44:41.592442 IP 192.168.12.2.30880 &amp;gt; 139.198.254.12.61476: Flags [F.], seq 6642, ack 1511, win 252, length 0
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;简单分析&#34;&gt;简单分析&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;当url一打开，tcpdump就有数据显示。看到的S标志位，建立会话连接；接着看到.标志位，对接受包进行确认；然后看到P标志位，表示数据包被对方接受上交给上层应用；最后看到F标志位，表示完成，没有数据要发送。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;非服务端tcpdump客户端的数据&#34;&gt;非服务端tcpdump客户端的数据&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;一般情况下，我们都是在服务端使用tcpdump工具抓包的；如果需要在非服务端使用tcpdump抓包可以通过网络流量镜像方式，使服务端的流量到目标地址上。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;某服务不正常tcpdump测试结果&#34;&gt;某服务不正常tcpdump测试结果&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;抓取某不正常的服务(端口30881)交互的包：我们知道nginx交互其实是tcp协议，因此使用如下命令
&lt;code&gt;tcpdump -i eth0 tcp and port 30881 -n -nn -C 20 -W 50 -s 0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;tcp建立连接：先在服务的机器上执行如下命令，接着把multinode.ks.dev.chenshaowen.com:30881的url在浏览器访问即可。以下标志位含义可以参考上面描述。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[root@master ~]#tcpdump -i eth0 tcp and port 30881 -n -nn -C 20 -W 50 -s 0 
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes
10:56:00.569543 IP 139.198.254.12.61608 &amp;gt; 192.168.12.2.30881: Flags [S], seq 1702724216, win 64240, options [mss 1360,nop,wscale 8,nop,nop,sackOK], length 0
10:56:00.569723 IP 192.168.12.2.30881 &amp;gt; 139.198.254.12.61608: Flags [R.], seq 0, ack 1702724217, win 0, length 0
10:56:00.571570 IP 139.198.254.12.61609 &amp;gt; 192.168.12.2.30881: Flags [S], seq 97188145, win 64240, options [mss 1360,nop,wscale 8,nop,nop,sackOK], length 0
10:56:00.571637 IP 192.168.12.2.30881 &amp;gt; 139.198.254.12.61609: Flags [R.], seq 0, ack 97188146, win 0, length 0
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;简单分析-1&#34;&gt;简单分析&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;当url一打开，一开始出现S标志位，表示建立会话连接；接着出现R标志位，表示重置，用于连接复位，拒绝错误和非法的数据包；最后有出现S标志位，再次建立会话连接，一直S与R交替出现，说明该服务不正常。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;参考文章&#34;&gt;参考文章&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://dreamgoing.github.io/tcpdump%E5%AE%9E%E6%88%98.html&#34;&gt;https://dreamgoing.github.io/tcpdump%E5%AE%9E%E6%88%98.html&lt;/a&gt;
&lt;a href=&#34;https://klionsec.github.io/2017/01/31/tcpdump-sniffer-pass/#menu&#34;&gt;https://klionsec.github.io/2017/01/31/tcpdump-sniffer-pass/#menu&lt;/a&gt;
&lt;a href=&#34;https://hubinwei.me/2018/07/25/tcpdump%E6%8A%93%E5%8C%85%E7%BB%83%E4%B9%A0/&#34;&gt;https://hubinwei.me/2018/07/25/tcpdump%E6%8A%93%E5%8C%85%E7%BB%83%E4%B9%A0/&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ubuntu快速安装wecenter</title>
      <link>https://Forest-L.github.io/post/ubuntu-quickly-installs-wecenter/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/ubuntu-quickly-installs-wecenter/</guid>
      <description>&lt;h1 id=&#34;ubuntu快速安装wecenter&#34;&gt;ubuntu快速安装wecenter&lt;/h1&gt;
&lt;p&gt;以镜像的形式安装wecenter，简化了nginx和php环境的单独安装。WeCenter（wecenter.com）是一款建立知识社区的开源程序（免费版），专注于企业和行业社区内容的整理、归类、检索和分享，是知识化问答社区的首选软件。后台使用PHP开发，MVC架构，前端使用Bootstrap框架。&lt;/p&gt;
&lt;h2 id=&#34;准备&#34;&gt;准备&lt;/h2&gt;
&lt;p&gt;mysql需要搭建，参考：
&lt;a href=&#34;https://lilinlinlin.github.io/2019/08/13/docker%E9%83%A8%E7%BD%B2mysql5-7/#more&#34;&gt;https://lilinlinlin.github.io/2019/08/13/docker%E9%83%A8%E7%BD%B2mysql5-7/#more&lt;/a&gt;
镜像：wecenter/wecenter:3.3.2&lt;/p&gt;
&lt;h2 id=&#34;wecenter安装&#34;&gt;wecenter安装&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;docker run --name wecenter1 -p 8081:80  -d wecenter/wecenter:3.3.2&lt;/code&gt;
通过外网ip加端口访问,链接为：
&lt;code&gt;eip:8081/install&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;安装成功浏览器打开之后&#34;&gt;安装成功，浏览器打开之后&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/006bbiLEgy1g5ygifdatkj30xv0q8abj.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;点击下一步，进入配置系统，正确填入数据库的主机、账号、密码和数据库的名称（默认wecenter），点开始安装。&lt;/li&gt;
&lt;li&gt;上步成功之后，管理员配置，用户名admin，密码自己定义。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考文档&#34;&gt;参考文档：&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://help.websoft9.com/lamp-guide/installation/wecenter/install.html&#34;&gt;http://help.websoft9.com/lamp-guide/installation/wecenter/install.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>基于keepalived-haproxy部署kubesphere高可用方案</title>
      <link>https://Forest-L.github.io/post/deploy-kubesphere-high-availability-solution-based-on-keepalived-haproxy/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/deploy-kubesphere-high-availability-solution-based-on-keepalived-haproxy/</guid>
      <description>&lt;h1 id=&#34;基于keepalivedhaproxy部署kubesphere高可用方案&#34;&gt;基于keepalived+haproxy部署kubesphere高可用方案&lt;/h1&gt;
&lt;p&gt;通过keepalived + haproxy实现的，其中keepalived提供一个VIP，通过VIP关联所有的Master节点；然后haproxy提供端口转发功能。由于VIP还是存在Master的机器上的，默认配置API Server的端口是6443，所以我们需要将另外一个端口关联到这个VIP上，一般用8443。&lt;/p&gt;
&lt;h4 id=&#34;环境信息&#34;&gt;环境信息：&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;青云机器操作系统：centos7.5
master1:192.168.0.10
master2:192.168.0.11
master3:192.168.0.12
node:192.168.0.6
vip:192.168.0.200
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;提前说明及遇到过坑&#34;&gt;提前说明及遇到过坑&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;keepalived提供VIP时，需提前规划的vip不能ping通。&lt;/li&gt;
&lt;li&gt;腾讯云服务器上不提供由keepalived方式产生VIP，需要提前在云平台界面HAVIP上创建，创建出的vip再在keepalived.conf中配置。（特别注意这点）。&lt;/li&gt;
&lt;li&gt;通过keepalived服务，vip只能在其中的某一个master中看到，如果ip a方式在每个master都看到，说明keepalived有问题。&lt;/li&gt;
&lt;li&gt;keepalived+haproxy正常安装之后，检查node节点可以和vip通信。&lt;/li&gt;
&lt;li&gt;common.yaml配置文件需要填写正确的ip和转发的端口。&lt;/li&gt;
&lt;li&gt;hosts.ini文件master需要添加master1、master2和master3。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;keepalived安装和配置三台master机器都要安装&#34;&gt;keepalived安装和配置，三台master机器都要安装。&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;安装keepalived, &lt;code&gt;yum install -y keepalived&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;/etc/keepalived/keepalived.conf文件下修改配置,需要修改自己场景的vIP,填写正确服务器的网卡名如：eth0。&lt;/li&gt;
&lt;li&gt;其中killall组件，还需要安装yum install psmisc -y。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;global_defs {
    router_id lb-backup
}
vrrp_script check_haproxy {
  script &amp;quot;/usr/bin/killall -0 haproxy&amp;quot;
  interval 2
  weight 2
}
vrrp_instance VI-kube-master {
    state MASTER
    priority 110
    dont_track_primary
    interface eth0
    virtual_router_id 90
    advert_int 3
    virtual_ipaddress {
        192.168.0.200
    }
	track_script {
    check_haproxy
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启keepalived服务及断电自启：&lt;code&gt;systemctl enable keepalived;systemctl restart keepalived&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;haproxy安装和配置三台master机器都要安装&#34;&gt;haproxy安装和配置，三台master机器都要安装。&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;安装haproyx，&lt;code&gt;yum install -y haproxy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;/etc/haproxy/haproxy.cfg 文件下修改配置，server服务端分别为master的IP值。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;global
        log /dev/log    local0
        log /dev/log    local1 notice
        chroot /var/lib/haproxy
        #stats socket /run/haproxy/admin.sock mode 660 level admin
        stats timeout 30s
        user haproxy
        group haproxy
        daemon
        nbproc 1

defaults
        log     global
        timeout connect 5000
        timeout client  50000
        timeout server  50000

listen kube-master
        bind 0.0.0.0:8443
        mode tcp
        option tcplog
        balance roundrobin
        server master1 192.168.0.10:6443  check inter 10000 fall 2 rise 2 weight 1
        server master2 192.168.0.11:6443  check inter 10000 fall 2 rise 2 weight 1
        server master3 192.168.0.12:6443  check inter 10000 fall 2 rise 2 weight 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启haproxy服务及断电自启：&lt;code&gt;systemctl enable haproxy;systemctl restart haproxy&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;hostsini配置和commonyaml配置&#34;&gt;hosts.ini配置和common.yaml配置。&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hosts.ini配置实例如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[all]
master1 ansible_connection=local  ip=192.168.0.10
master2  ansible_host=192.168.0.11  ip=192.168.0.11  ansible_ssh_pass=****
master3  ansible_host=192.168.0.12  ip=192.168.0.12  ansible_ssh_pass=****
node1  ansible_host=192.168.0.6  ip=192.168.0.6  ansible_ssh_pass=****

[kube-master]
master1
master2
master3

[kube-node]
node1


[etcd]
master1
master2
master3

[k8s-cluster:children]
kube-node
kube-master 
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;common.yaml的lb配置，注意此处填写VIP，及转发的端口8443。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# apiserver_loadbalancer_domain_name: &amp;quot;lb.kubesphere.local&amp;quot;
loadbalancer_apiserver:
  address: 192.168.0.200
  port: 8443
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;安装kubesphere及结果&#34;&gt;安装kubesphere及结果&lt;/h3&gt;
&lt;p&gt;kubesphere-all-v2.1.0/scripts目录下，执行./install.sh，选择2+yes即可，然后等待脚本的安装。
node正常的结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes -o wide
NAME      STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION          CONTAINER-RUNTIME
master1   Ready    master   95m   v1.15.5   192.168.0.10   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-862.el7.x86_64   docker://18.9.7
master2   Ready    master   88m   v1.15.5   192.168.0.11   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-862.el7.x86_64   docker://18.9.7
master3   Ready    master   88m   v1.15.5   192.168.0.12   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-862.el7.x86_64   docker://18.9.7
node1     Ready    worker   86m   v1.15.5   192.168.0.6    &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-862.el7.x86_64   docker://18.9.7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;apiserver中vip生效的结果&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;telnet 192.168.0.200 8443
Trying 192.168.0.200...
Connected to 192.168.0.200.
Escape character is &#39;^]&#39;.
^CConnection closed by foreign host.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>排查linux服务器的工具</title>
      <link>https://Forest-L.github.io/post/tools-to-troubleshoot-linux-servers/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/tools-to-troubleshoot-linux-servers/</guid>
      <description>&lt;p&gt;当服务器遇到问题，或者提前查看服务器质量怎么样，一般可以通过以下几点分析：服务器整体情况，CPU使用情况，内存，磁盘，磁盘io，网络io等。&lt;/p&gt;
&lt;h3 id=&#34;不同环境插件安装&#34;&gt;不同环境插件安装&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;centos&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;yum install sysstat -y&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ubuntu&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;apt install sysstat -y&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;1整体分析之top&#34;&gt;1、整体分析之top&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;执行top指令&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# top
top - 10:41:30 up 6 days, 11:23,  1 user,  load average: 0.98, 0.66, 0.57
Tasks: 304 total,   1 running, 200 sleeping,   0 stopped,   2 zombie
%Cpu(s):  3.1 us,  0.7 sy,  0.0 ni, 95.5 id,  0.6 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem : 32937840 total, 25930000 free,  2666144 used,  4341696 buff/cache
KiB Swap:        0 total,        0 free,        0 used. 30586896 avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
28484 root      20   0 1468632 1.280g  68832 S  39.1  4.1 260:25.96 kube-apiserver
25442 root      20   0 10.277g 308160  80692 S   9.3  0.9 166:39.40 etcd
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;第 1 行：系统时间、运行时间、登录终端数、系统负载（三个数值分别为1分钟、5分钟、15分钟内的平均值，数值越小意味着负载越低）。&lt;/li&gt;
&lt;li&gt;第 2 行：进程总数、运行中的进程数、睡眠中的进程数、停止的进程数、僵死的进程数。一般情况下，只要没有僵死的进程，就没啥大问题。&lt;/li&gt;
&lt;li&gt;第 3 行：用户占用资源百分比、系统内核占用资源百分比、改变过优先级的进程资源百分比、空闲的资源百分比等。&lt;/li&gt;
&lt;li&gt;第 4 行：物理内存总量、内存空闲量、内存使用量、作为内核缓存的内存量。&lt;/li&gt;
&lt;li&gt;第 5 行：虚拟内存总量、虚拟内存空闲量、虚拟内存使用量、已被提前加载的内存量。&lt;/li&gt;
&lt;li&gt;第 6 行里面主要看 PID 和 COMMAND 这两个参数，其中 PID 就是进程 ID ， COMMAND 就是执行的命令，能够看到比较靠前的两个进程都是k8s进程。&lt;/li&gt;
&lt;li&gt;干掉僵尸进程的指令，&lt;code&gt;ps -A -ostat,ppid | grep -e &#39;^[Zz]&#39; | awk &#39;{print $2}&#39; | xargs kill -HUP &amp;gt; /dev/null 2&amp;gt;&amp;amp;1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;在当前这个界面，按下数字键盘 1 能够看到各个 CPU 的详细利用率&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;top - 10:46:53 up 6 days, 11:29,  1 user,  load average: 0.15, 0.53, 0.57
Tasks: 303 total,   1 running, 200 sleeping,   0 stopped,   2 zombie
%Cpu0  :  1.7 us,  1.3 sy,  0.0 ni, 96.3 id,  0.7 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu1  :  2.0 us,  0.7 sy,  0.0 ni, 97.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu2  :  0.7 us,  0.3 sy,  0.0 ni, 99.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;2cpu分析之vmstat&#34;&gt;2、cpu分析之vmstat&lt;/h3&gt;
&lt;p&gt;一般 vmstat 工具的使用是通过两个数字参数来完成的，第一个参数是采样的时间间隔，单位是秒，第二个参数是采样的次数，这次的命令是：vmstat -n 3 2 意思就是隔 3 秒取样一次，一共取样 2 次。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;执行vmstat指令&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# vmstat -n 3 2
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
0  0      0 25910152 684436 3669440    0    0     1    38    4    0  5  2 89  1  3
3  0      0 25910008 684436 3669488    0    0     0   441 8905 15261  1  1 97  1  0
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;21procs&#34;&gt;2.1、procs:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;r ：运行和等待 CPU 时间片的进程数，一般来说整个系统的运行队列不要超过总核数的 2 倍，要不然系统压力太大了。&lt;/li&gt;
&lt;li&gt;b : 等待资源的进程数，比如正在等待磁盘 IO ，网络 IO 这种。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;22cpu&#34;&gt;2.2、cpu：&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;us  ：用户进程消耗 CPU 时间百分比， us 值高的话，说明用户进程消耗 CPU 时间比较长，如果长期大于 50% 的话，那就说明程序还有需要优化的地方。&lt;/li&gt;
&lt;li&gt;sy ：内核进程消耗的 CPU 时间百分比。&lt;/li&gt;
&lt;li&gt;us + sy 参考值为 80% ，如果大于 80% 的话，说明可能存在 CPU 不足。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3内存分析之free&#34;&gt;3、内存分析之free&lt;/h3&gt;
&lt;p&gt;一般我们使用free -m即可&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# free -m
              total        used        free      shared  buff/cache   available
Mem:          32165        2608       25354          13        4203       29861
Swap:             0           0           0
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;如果应用程序可用内存/系统物理内存大于 70% 的话，说明内存是充足的，没啥问题，但是如果小于 20% 的话，就要考虑增加内存了。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4磁盘分析之df&#34;&gt;4、磁盘分析之df&lt;/h3&gt;
&lt;p&gt;排查磁盘问题，首先要排查磁盘空间够不够，df和du就可以。df查看磁盘使用情况；du查看目录占用磁盘情况及子文件占用情况。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# df -hT
Filesystem     Type      Size  Used Avail Use% Mounted on
udev           devtmpfs   16G     0   16G   0% /dev
tmpfs          tmpfs     3.2G   14M  3.2G   1% /run
/dev/vda2      ext4       99G  9.4G   84G  11% /
tmpfs          tmpfs      16G     0   16G   0% /dev/shm
tmpfs          tmpfs     5.0M     0  5.0M   0% /run/lock
tmpfs          tmpfs      16G     0   16G   0% /sys/fs/cgroup

root@master2:~# du -sh
516M    .
root@master2:~# du -sh *
16K     1.sh
434M    etcd
82M     etcd.tar.gz
4.0K    etcd.txt
root@master2:~# du -h --max-depth=1
4.0K    ./.cache
8.0K    ./.gnupg
8.0K    ./.ansible
434M    ./etcd
948K    ./.kube
8.0K    ./.ssh
8.0K    ./.vim
516M    .
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;5磁盘io分析之iostat&#34;&gt;5、磁盘io分析之iostat&lt;/h3&gt;
&lt;p&gt;在对数据库进行操作时，第一要考虑就是磁盘io操作，因为相对来说，如果在某个时间段给磁盘进行大量的写入操作会造成程序等待时间长，导致客户端那边好久都没啥反应。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# iostat -xdk 3 2
Linux 4.15.0-115-generic (master2)      09/11/2020      _x86_64_        (16 CPU)

Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
loop0            0.00    0.00      0.02      0.00     0.00     0.00   0.00   0.00    1.75    0.00   0.00    18.07     0.00   0.16   0.00
loop1            0.05    0.00      0.07      0.00     0.00     0.00   0.00   0.00    1.66    0.00   0.00     1.49     0.00   0.06   0.00
vda              0.74   19.38     18.68    154.10     0.00    10.14   0.07  34.35    6.74   17.52   0.28    25.39     7.95   0.79   1.60

Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
loop0            0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00
loop1            0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00
vda              0.00    7.00      0.00     61.33     0.00     6.00   0.00  46.15    0.00    1.33   0.00     0.00     8.76   0.00   0.00
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;rkB/s ：每秒读取数据量 kB 。&lt;/li&gt;
&lt;li&gt;wkB/s ：每秒写入数据量 kB 。&lt;/li&gt;
&lt;li&gt;svctm ：I/O 请求的平均服务时间，单位毫秒。&lt;/li&gt;
&lt;li&gt;util ：一秒中有百分之几的时间用于 I/O 操作，如果接近 100% 说明磁盘带宽跑满了，这个时候就要优化程序或者增加磁盘了。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;6网络io分析之sar&#34;&gt;6、网络io分析之sar&lt;/h3&gt;
&lt;p&gt;网络 IO 的话，可以通过 sar -n DEV 3 2 这条命令来看，和上面的差不多，意思就是每隔 3 秒取样一次，一共取样 2 次。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# sar -n DEV 3 2
Linux 4.15.0-115-generic (master2)      09/11/2020      _x86_64_        (16 CPU)

11:06:46 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
11:06:49 AM      eth0    854.33    859.33    199.93    210.99      0.00      0.00      0.00      0.00
11:06:49 AM     tunl0    113.67    108.33     10.92      9.86      0.00      0.00      0.00      0.00
11:06:49 AM calide035c655d8     98.33    108.33     11.04     12.63      0.00      0.00      0.00      0.00
11:06:49 AM        lo     58.00     58.00     13.18     13.18      0.00      0.00      0.00      0.00
11:06:49 AM cali3d31c61f18c      6.00      3.33      4.41      2.08      0.00      0.00      0.00      0.00
11:06:49 AM nodelocaldns      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
11:06:49 AM kube-ipvs0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
11:06:49 AM cali7d0b83575fc      7.00      8.00      2.46      0.81      0.00      0.00      0.00      0.00
11:06:49 AM cali056accc6554     24.33     21.00      2.93     10.81      0.00      0.00      0.00      0.00
11:06:49 AM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00

11:06:49 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
11:06:52 AM      eth0    623.67    620.00    109.84    127.33      0.00      0.00      0.00      0.00
11:06:52 AM     tunl0     97.33     90.00      6.64      8.74      0.00      0.00      0.00      0.00
11:06:52 AM calide035c655d8     93.67    104.00     10.52      8.68      0.00      0.00      0.00      0.00
11:06:52 AM        lo     65.67     65.67     11.60     11.60      0.00      0.00      0.00      0.00
11:06:52 AM cali3d31c61f18c      6.00      3.67      5.62      2.78      0.00      0.00      0.00      0.00
11:06:52 AM nodelocaldns      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
11:06:52 AM kube-ipvs0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
11:06:52 AM cali7d0b83575fc      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
11:06:52 AM cali056accc6554      3.67      2.33      0.24      0.39      0.00      0.00      0.00      0.00
11:06:52 AM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00

Average:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
Average:         eth0    739.00    739.67    154.89    169.16      0.00      0.00      0.00      0.00
Average:        tunl0    105.50     99.17      8.78      9.30      0.00      0.00      0.00      0.00
Average:    calide035c655d8     96.00    106.17     10.78     10.65      0.00      0.00      0.00      0.00
Average:           lo     61.83     61.83     12.39     12.39      0.00      0.00      0.00      0.00
Average:    cali3d31c61f18c      6.00      3.50      5.02      2.43      0.00      0.00      0.00      0.00
Average:    nodelocaldns      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:    kube-ipvs0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:    cali7d0b83575fc      3.50      4.00      1.23      0.40      0.00      0.00      0.00      0.00
Average:    cali056accc6554     14.00     11.67      1.58      5.60      0.00      0.00      0.00      0.00
Average:      docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;IFACE ：LAN 接口&lt;/li&gt;
&lt;li&gt;rxpck/s ：每秒钟接收的数据包&lt;/li&gt;
&lt;li&gt;txpck/s ：每秒钟发送的数据包&lt;/li&gt;
&lt;li&gt;rxKB/s ：每秒接收的数据量，单位 KByte&lt;/li&gt;
&lt;li&gt;txKB/s ：每秒发出的数据量，单位 KByte&lt;/li&gt;
&lt;li&gt;rxcmp/s ：每秒钟接收的压缩数据包&lt;/li&gt;
&lt;li&gt;txcmp/s ：每秒钟发送的压缩数据包&lt;/li&gt;
&lt;li&gt;rxmcst/s：每秒钟接收的多播数据包&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>提前预防K8s集群资源不足的处理方式配置</title>
      <link>https://Forest-L.github.io/post/prevent-the-configuration-of-the-k8s-cluster-from-under-resourcing-in-advance/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/prevent-the-configuration-of-the-k8s-cluster-from-under-resourcing-in-advance/</guid>
      <description>&lt;h1 id=&#34;提前预防k8s集群资源不足的处理方式配置&#34;&gt;提前预防K8s集群资源不足的处理方式配置&lt;/h1&gt;
&lt;p&gt;在管理集群的时候我们常常会遇到资源不足的情况，在这种情况下我们要保证整个集群可用，并且尽可能减少应用的损失。根据该问题提出以下两种方案：一种为优化kubelet参数，另一种为脚本化诊断处理。&lt;/p&gt;
&lt;h2 id=&#34;概念解释&#34;&gt;概念解释&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;CPU 的使用时间是可压缩的，换句话说它本身无状态，申请资源很快，也能快速正常回收。&lt;/li&gt;
&lt;li&gt;内存（memory）大小是不可压缩的，因为它是有状态的（内存里面保存的数据），申请资源很慢（需要计算和分配内存块的空间），并且回收可能失败（被占用的内存一般不可回收）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;优化kubelet参数&#34;&gt;优化kubelet参数&lt;/h2&gt;
&lt;p&gt;优化kubelet参数通过k8s资源表示、节点资源配置及kubelet参数设置、应用优先级和资源动态调整这几个方面来介绍。k8s资源表示为yaml文件中如何添加requests和limites参数。节点资源配置及kubelet参数设置描述为一个node上面资源配置情况，从而来优化kubelet参数。应用优先级描述为当资源不足时，优先保留那些pod不被驱逐。资源动态调整描述为运算能力的增减，如：HPA 、VPA和Cluster Auto Scaler。&lt;/p&gt;
&lt;h3 id=&#34;k8s资源表示&#34;&gt;k8s资源表示&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;在k8s中，资源表示配置字段是 spec.containers[].resource.limits/request.cpu/memory。yaml格式如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;spec:
  template:
    ...
    spec:
      containers:
        ...
        resources:
          limits:
            cpu: &amp;quot;1&amp;quot;
            memory: 1000Mi
          requests:
            cpu: 20m
            memory: 100Mi
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;资源动态调整&#34;&gt;资源动态调整&lt;/h3&gt;
&lt;p&gt;动态调整的思路：应用的实际流量会不断变化，因此使用率也是不断变化的，为了应对应用流量的变化，我们应用能够自动调整应用的资源。比如在线商品应用在促销的时候访问量会增加，我们应该自动增加 pod 运算能力来应对；当促销结束后，有需要自动降低 pod 的运算能力防止浪费。
运算能力的增减有两种方式：改变单个 pod 的资源，已经增减 pod 的数量。这两种方式对应了 kubernetes 的 HPA 和 VPA和Cluster Auto Scaler。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HPA: 横向 pod 自动扩展的思路是这样的：kubernetes 会运行一个 controller，周期性地监听 pod 的资源使用情况，当高于设定的阈值时，会自动增加 pod 的数量；当低于某个阈值时，会自动减少 pod 的数量。自然，这里的阈值以及 pod 的上限和下限的数量都是需要用户配置的。&lt;/li&gt;
&lt;li&gt;VPA: VPA 调整的是单个 pod 的 request 值（包括 CPU 和 memory）VPA 包括三个组件：
（1）Recommander：消费 metrics server 或者其他监控组件的数据，然后计算 pod 的资源推荐值
（2）Updater：找到被 vpa 接管的 pod 中和计算出来的推荐值差距过大的，对其做 update 操作（目前是 evict，新建的 pod 在下面 admission controller 中会使用推荐的资源值作为 request）
（3）Admission Controller：新建的 pod 会经过该 Admission Controller，如果 pod 是被 vpa 接管的，会使用 recommander 计算出来的推荐值&lt;/li&gt;
&lt;li&gt;CLuster Auto Scaler：能够根据整个集群的资源使用情况来增减节点。Cluster Auto Scaler 就是监控这个集群因为资源不足而 pending 的 pod，根据用户配置的阈值调用公有云的接口来申请创建机器或者销毁机器。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;节点资源配置及kubelet参数设置&#34;&gt;节点资源配置及kubelet参数设置&lt;/h3&gt;
&lt;p&gt;节点资源的配置一般分为 2 种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;资源预留：为系统进程和 k8s 进程预留资源&lt;/li&gt;
&lt;li&gt;pod 驱逐：节点资源到达一定使用量，开始驱逐 pod&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Node Capacity&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;kube-reserved&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;system-reserved&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;eviction-threshold&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Allocatable&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Node Capacity：Node的所有硬件资源&lt;/li&gt;
&lt;li&gt;kube-reserved：给kube组件预留的资源：kubelet,kube-proxy以及docker等&lt;/li&gt;
&lt;li&gt;system-reserved：给system进程预留的资源&lt;/li&gt;
&lt;li&gt;eviction-threshold：kubelet eviction的阈值设定&lt;/li&gt;
&lt;li&gt;Allocatable：真正scheduler调度Pod时的参考值（保证Node上所有Pods的request resource不超过Allocatable）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;allocatable的值即对应 describe node 时看到的allocatable容量，pod 调度的上限&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;计算公式：节点上可配置值 = 总量 - 预留值 - 驱逐阈值

Allocatable = Capacity - Reserved(kube+system) - Eviction Threshold
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以上配置均在kubelet 中添加，涉及的参数有：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--kube-reserved=cpu=200m,memory=250Mi \
--system-reserved=cpu=200m,memory=250Mi \
--eviction-hard=memory.available&amp;lt;5%,nodefs.available&amp;lt;10%,imagefs.available&amp;lt;10% \
--eviction-soft=memory.available&amp;lt;10%,nodefs.available&amp;lt;15%,imagefs.available&amp;lt;15% \
--eviction-soft-grace-period=memory.available=2m,nodefs.available=2m,imagefs.available=2m \
--eviction-max-pod-grace-period=120 \
--eviction-pressure-transition-period=30s \
--eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=500Mi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以上配置均为百分比，举例：&lt;/p&gt;
&lt;p&gt;以2核4GB内存40GB磁盘空间的配置为例，Allocatable是1.6 CPU，3.3Gi 内存，25Gi磁盘。当pod的总内存消耗大于3.3Gi或者磁盘消耗大于25Gi时，会根据相应策略驱逐pod。
Allocatable = 4Gi - 250Mi -250Mi - 4Gi*5% = 3.3Gi&lt;/p&gt;
&lt;p&gt;（1） 配置 k8s组件预留资源的大小，CPU、Mem&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;指定为k8s系统组件（kubelet、kube-proxy、dockerd等）预留的资源量，
如：--kube-reserved=cpu=1,memory=2Gi,ephemeral-storage=1Gi。
这里的kube-reserved只为非pod形式启动的kube组件预留资源，假如组件要是以static pod（kubeadm）形式启动的，那并不在这个kube-reserved管理并限制的cgroup中，而是在kubepod这个cgroup中。
（ephemeral storage需要kubelet开启feature-gates，预留的是临时存储空间（log，EmptyDir），生产环境建议先不使用）
ephemeral-storage是kubernetes1.8开始引入的一个资源限制的对象，kubernetes 1.10版本中kubelet默认已经打开的了,到目前1.11还是beta阶段，主要是用于对本地临时存储使用空间大小的限制，如对pod的empty dir、/var/lib/kubelet、日志、容器可读写层的使用大小的限制。
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（2）配置 系统守护进程预留资源的大小（预留的值需要根据机器上容器的密度做一个合理的值）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;含义：为系统守护进程(sshd, udev等)预留的资源量，
如：--system-reserved=cpu=500m,memory=1Gi,ephemeral-storage=1Gi。
注意，除了考虑为系统进程预留的量之外，还应该为kernel和用户登录会话预留一些内存。
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（3）配置 驱逐pod的硬阈值&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;含义：设置进行pod驱逐的阈值，这个参数只支持内存和磁盘。
通过--eviction-hard标志预留一些内存后，当节点上的可用内存降至保留值以下时，
kubelet 将会对pod进行驱逐。
配置：--eviction-hard=memory.available&amp;lt;5%,nodefs.available&amp;lt;10%,imagefs.available&amp;lt;10%
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（4）配置 驱逐pod的软阈值&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--eviction-soft=memory.available&amp;lt;10%,nodefs.available&amp;lt;15%,imagefs.available&amp;lt;15%
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（5）定义达到软阈值之后，持续时间超过多久才进行驱逐&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--eviction-soft-grace-period=memory.available=2m,nodefs.available=2m,imagefs.available=2m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（6）驱逐pod前最大等待时间=min(pod.Spec.TerminationGracePeriodSeconds, eviction-max-pod-grace-period)，单位为秒&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--eviction-max-pod-grace-period=120
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（7）至少回收的资源量&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=500Mi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（8）防止波动,kubelet 多久才上报节点的状态&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--eviction-pressure-transition-period=30s
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;应用优先级&#34;&gt;应用优先级&lt;/h3&gt;
&lt;p&gt;当资源不足时，配置了如上驱逐参数，pod之间的驱逐顺序是怎样的呢？以下描述设置不同优先级来确保集群中核心的组件不被驱逐还正常运行，OOM 的优先级如下,pod oom 值越低，也就越不容易被系统杀死。：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;BestEffort Pod &amp;gt; Burstable Pod &amp;gt; 其它进程（内核init进程等） &amp;gt; Guaranteed Pod &amp;gt; kubelet/docker 等 &amp;gt; sshd 等进程
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;kubernetes 把 pod 分成了三个 QoS 等级，而其中和limits和requests参数有关：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Guaranteed：oom优先级最低，可以考虑数据库应用或者一些重要的业务应用。除非 pods 使用超过了它们的 limits，或者节点的内存压力很大而且没有 QoS 更低的 pod，否则不会被杀死。&lt;/li&gt;
&lt;li&gt;Burstable：这种类型的 pod 可以多于自己请求的资源（上限有 limit 指定，如果 limit 没有配置，则可以使用主机的任意可用资源），但是重要性认为比较低，可以是一般性的应用或者批处理任务。&lt;/li&gt;
&lt;li&gt;Best Effort：oom优先级最高，集群不知道 pod 的资源请求情况，调度不考虑资源，可以运行到任意节点上（从资源角度来说），可以是一些临时性的不重要应用。pod 可以使用节点上任何可用资源，但在资源不足时也会被优先杀死。
Pod 的 requests 和 limits 是如何对应到这三个 QoS 等级上的，可以用下面一张表格概括：&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;request是否配置&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;limits是否配置&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;两者的关系&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Qos&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;requests=limits&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Guaranteed&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;所有容器的cpu和memory都必须配置相同的requests和limits&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;request&amp;lt;limit&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Burstable&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;只要有容器配置了cpu或者memory的request和limits就行&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;否&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Burstable&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;只要有容器配置了cpu或者memory的request就行&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;否&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Guaranteed/Burstable&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;如果配置了limits，k8s会自动把对应资源的request设置和limits一样。如果所有容器所有资源都配置limits，那就是Guaranteed;如果只有部分配置了limits，就是Burstable&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;否&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;否&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Best Effort&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;所有的容器都没有配置资源requests或limits&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;request和limits相同，可以参考资源动态调整中的VPA设置合理值。&lt;/li&gt;
&lt;li&gt;如果只配置了limits，没有配置request，k8s会把request值和limits值一样。&lt;/li&gt;
&lt;li&gt;如果只配置了request，没有配置limits，该pod共享node上可用的资源，实际上很反对这样设置。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;
&lt;p&gt;动态地资源调整通过 kubelet 驱逐程序进行的，但需要和应用优先级配合使用才能达到很好的效果，否则可能驱逐集群中核心组件。&lt;/p&gt;
&lt;h2 id=&#34;脚本化诊断处理&#34;&gt;脚本化诊断处理&lt;/h2&gt;
&lt;p&gt;什么叫脚本化诊断处理呢？它的含义为：当集群中的某台机器资源（一般指memory）用到85%-90%时，脚本自动检查到且该节点为不可调度。缺点为：背离了资源动态调整中CLuster Auto Scaler特点。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;集群中每台机器都可以执行kubectl指令：
如果没有设置，可将master机器上的$HOME/.kube/config文件拷贝到node机器上。&lt;/li&gt;
&lt;li&gt;可以通过&lt;a href=&#34;https://github.com/Forest-L/shell-operator&#34;&gt;shell-operator&lt;/a&gt;自动诊断机器资源且做cordon操作处理&lt;/li&gt;
&lt;li&gt;脚本中关键说明
（1）获取本地IP：ip a | grep &amp;lsquo;state UP&amp;rsquo; -A2| grep inet | grep -v inet6 | grep -v 127 | sed &amp;rsquo;s/^[ \t]*//g&amp;rsquo; | cut -d &#39; &#39; -f2 | cut -d &amp;lsquo;/&amp;rsquo; -f1
（2）获取本地ip对应的node名：kubectl get nodes -o  wide | grep &amp;ldquo;本地ip&amp;rdquo; | awk &amp;lsquo;{print $1}&amp;rsquo;
（3）不可调度：kubectl cordon node &amp;lt;node名&amp;gt;
（4）获取总内存： free -m | awk &amp;lsquo;NR==2{print $2}&amp;rsquo;
（5）获取使用内存： free -m | awk &amp;lsquo;NR==2{print $3}&amp;rsquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考文档&#34;&gt;参考文档&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/zh/docs/tasks/administer-cluster/out-of-resource/&#34;&gt;https://kubernetes.io/zh/docs/tasks/administer-cluster/out-of-resource/&lt;/a&gt;
&lt;a href=&#34;https://cizixs.com/2018/06/25/kubernetes-resource-management/&#34;&gt;https://cizixs.com/2018/06/25/kubernetes-resource-management/&lt;/a&gt;
&lt;a href=&#34;https://segmentfault.com/a/1190000021402192&#34;&gt;https://segmentfault.com/a/1190000021402192&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>构建arm-x86架构的docker-image操作指南</title>
      <link>https://Forest-L.github.io/post/docker-image-operation-guide-for-building-arm-x86-architecture/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/docker-image-operation-guide-for-building-arm-x86-architecture/</guid>
      <description>&lt;h1 id=&#34;构建armx86架构的docker-image操作指南&#34;&gt;构建arm/x86架构的docker image操作指南&lt;/h1&gt;
&lt;p&gt;由于arm环境越来越受欢迎，镜像不单单满足x86结构的docker镜像，还需要arm操作系统的镜像，以下说明在x86机器上如何build一个arm结构的镜像，使用buildx指令来同时构建arm/x86结构的镜像。&lt;/p&gt;
&lt;h2 id=&#34;1启动一台ubuntu的机器并安装docker-1903&#34;&gt;1.	启动一台ubuntu的机器，并安装docker 19.03&lt;/h2&gt;
&lt;p&gt;在测试过程中发现 Centos7.5 有下面的问题，这里我们直接绕过
&lt;a href=&#34;https://github.com/multiarch/qemu-user-static/issues/38&#34;&gt;issue&lt;/a&gt;
docker安装参考&lt;a href=&#34;https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/&#34;&gt;docker安装&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;2运行下列命令安装并测试qemu&#34;&gt;2.	运行下列命令安装并测试qemu&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;查看机器的架构&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;uname -m
x86_64
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;正常测试docker启动一个arm镜像容器&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -t arm64v8/ubuntu uname -m
standard_init_linux.go:211: exec user process caused &amp;quot;exec format error&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;添加特权模式安装qemu，且启动一个arm镜像容器&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm --privileged multiarch/qemu-user-static --reset -p yes
docker run --rm -t arm64v8/ubuntu uname -m
aarch64
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3--启用docker--buildx-命令&#34;&gt;3.	  启用docker  buildx 命令&lt;/h2&gt;
&lt;p&gt;docker buildx 为跨平台构建 docker 镜像所使用的命令。目前为实验特性，可以设置dokcer cli的配置，将实验特性开启。&lt;/p&gt;
&lt;p&gt;将下面配置添加到CLI配置文件当中~/.docker/config.json&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;experimental&amp;quot;: &amp;quot;enabled&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;4创建新的builder实例默认的docker实例不支持镜像导出&#34;&gt;4.	创建新的builder实例（默认的docker实例不支持镜像导出）&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;docker buildx create --name ks-all&lt;/code&gt;
&lt;code&gt;docker buildx use ks-all&lt;/code&gt;
&lt;code&gt;docker buildx inspect --bootstrap&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;执行下面命令可以看到 builder 已经创建好，并且支持多种平台的构建。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker buildx ls
NAME/NODE DRIVER/ENDPOINT             STATUS  PLATFORMS
ks-all *  docker-container
  ks-all0 unix:///var/run/docker.sock running linux/amd64, linux/arm64, linux/riscv64, linux/ppc64le, linux/s390x, linux/386, linux/arm/v7, linux/arm/v6
default   docker
  default default                     running linux/amd64, linux/386
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;5执行构建命令以ks-installer为例&#34;&gt;5.	执行构建命令（以ks-installer为例）&lt;/h2&gt;
&lt;p&gt;在 ks-installer目录下执行命令可以构建 arm64与amd64的镜像，并自动推送到镜像仓库中。
&lt;code&gt;docker buildx build -f /root/ks-installer/Dockerfile --output=type=registry --platform linux/arm64  -t lilinlinlin/ks-installer:2.1.0-arm64 .&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;(需要注意现在 ks-installer 的 Dockerfile中 go build 命令带有 GOOS GOARCH等，这些要删除)&lt;/p&gt;
&lt;p&gt;构建成功之后，可以在dockerhub下图当中可以看到是支持两种arch&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;DIGEST                       OS/ARCH                          COMPRESSED SIZE
97dd2142cac6                 linux/amd64                       111.13 MB
ce366ad696cb                 linux/arm64                       111.13 MB
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;6-构建并保存为tar-文件&#34;&gt;6.	 构建并保存为tar 文件&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;可以参考 buildx 的官方文档
&lt;a href=&#34;https://github.com/docker/buildx#-o---outputpath-typetypekeyvalue&#34;&gt;https://github.com/docker/buildx#-o---outputpath-typetypekeyvalue&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker buildx build  --output=type=docker,dest=/root/ks-installer.tar --platform  linux/arm64 -t lilinlinlin/ks-installer:2.1.0-arm64 ./pkg/db/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;构建tar包时需要注意output的类型需要是docker，而不是tar&lt;/p&gt;
&lt;h2 id=&#34;更多参考&#34;&gt;更多参考&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/docker/buildx&#34;&gt;https://github.com/docker/buildx&lt;/a&gt;
&lt;a href=&#34;https://github.com/multiarch/qemu-user-static&#34;&gt;https://github.com/multiarch/qemu-user-static&lt;/a&gt;
&lt;a href=&#34;https://community.arm.com/developer/tools-software/tools/b/tools-software-ides-blog/posts/getting-started-with-docker-for-arm-on-linux&#34;&gt;https://community.arm.com/developer/tools-software/tools/b/tools-software-ides-blog/posts/getting-started-with-docker-for-arm-on-linux&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>测量网络速度工具之iperf认知</title>
      <link>https://Forest-L.github.io/post/iperf-cognition-as-a-network-speed-measurement-tool/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/iperf-cognition-as-a-network-speed-measurement-tool/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;iperf工具是测量服务器网络速度工具，它通过测量服务器可以处理的最大网络吞吐量来测试网络速度，在遇到网络问题时特别有用。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;1下载源代码服务端和客户端都要安装&#34;&gt;1、下载源代码（服务端和客户端都要安装）&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;wget https://iperf.fr/download/source/iperf-2.0.8-source.tar.gz&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;安装编译环境
&lt;code&gt;yum install gcc-c++ -y&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;解压并安装iperf&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;tar -xvf iperf-2.0.8-source.tar.gz

cd iperf-2.0.8/

./configure &amp;amp;&amp;amp; make &amp;amp;&amp;amp; make install
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;2测试&#34;&gt;2、测试&lt;/h3&gt;
&lt;h4 id=&#34;21服务端执行iperf指令&#34;&gt;2.1、服务端执行iperf指令&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;iperf -s -p 12345 -i 1&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;-s表示以服务器模式运行。&lt;/li&gt;
&lt;li&gt;-p设置服务监听端口，测试时该端口在服务上没有被占用即可。&lt;/li&gt;
&lt;li&gt;-i设置每次报告之间的时间间隔，单位为s。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;iperf -s -p 12345 -i 1
------------------------------------------------------------
Server listening on TCP port 12345
TCP window size: 85.3 KByte (default)
------------------------------------------------------------
[  4] local 192.168.0.8 port 12345 connected with 192.168.0.10 port 52648
[ ID] Interval       Transfer     Bandwidth
[  4]  0.0- 1.0 sec   119 MBytes   999 Mbits/sec
[  4]  1.0- 2.0 sec  60.3 MBytes   506 Mbits/sec
[  4]  2.0- 3.0 sec  61.7 MBytes   517 Mbits/sec
[  4]  3.0- 4.0 sec  60.9 MBytes   511 Mbits/sec
[  4]  4.0- 5.0 sec  59.9 MBytes   503 Mbits/sec
[  4]  5.0- 6.0 sec  61.1 MBytes   512 Mbits/sec
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;22客户端执行iperf指令&#34;&gt;2.2、客户端执行iperf指令&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;iperf -c XX.XX.XX.XX -p 1234 -i 1&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;其中XX.XX.XX.XX为服务端的ip。&lt;/li&gt;
&lt;li&gt;-p要和服务端设置的相同。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;iperf -c 192.168.0.8 -p 12345 -i 1
------------------------------------------------------------
Client connecting to 192.168.0.8, TCP port 12345
TCP window size: 45.0 KByte (default)
------------------------------------------------------------
[  3] local 192.168.0.10 port 52648 connected with 192.168.0.8 port 12345
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   123 MBytes  1.03 Gbits/sec
[  3]  1.0- 2.0 sec  60.1 MBytes   504 Mbits/sec
[  3]  2.0- 3.0 sec  62.0 MBytes   520 Mbits/sec
[  3]  3.0- 4.0 sec  60.4 MBytes   506 Mbits/sec
[  3]  4.0- 5.0 sec  60.2 MBytes   505 Mbits/sec
[  3]  5.0- 6.0 sec  60.6 MBytes   509 Mbits/sec
[  3]  6.0- 7.0 sec  62.1 MBytes   521 Mbits/sec
[  3]  7.0- 8.0 sec  59.4 MBytes   498 Mbits/sec
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.huaweicloud.com/kunpeng/software/iperf.html&#34;&gt;iperf&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>脚本化触发Kubernetes集群事件的工具-Shell-operator</title>
      <link>https://Forest-L.github.io/post/scripting-the-tool-shell-operator-that-triggers-kubernetes-cluster-events/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/scripting-the-tool-shell-operator-that-triggers-kubernetes-cluster-events/</guid>
      <description>&lt;p&gt;Shell-operator是用于在Kubernetes集群中运行事件驱动脚本工具。Shell-operator通过脚本作为事件触发的钩子（hook），在Kubernetes集群事件和Shell脚本之间提供了一个转化层。触发钩子包含add, update和delete。以pod add为例通俗的话说，当新创建了一个pod时，会自动触发脚本中else部分。&lt;/p&gt;
&lt;h4 id=&#34;shell-operator特点&#34;&gt;Shell-operator特点：&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;轻松管理Kubernetes集群：可以是bash，python和kubectl。&lt;/li&gt;
&lt;li&gt;Kubernetes对象事件：钩子触发包含add, update或delete事件。&lt;/li&gt;
&lt;li&gt;对象选择器和属性过滤器：可以监视一组特定的对象并检测其属性的变化。&lt;/li&gt;
&lt;li&gt;配置简单：钩子绑定语法格式为yaml/json输出。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1环境准备只要kubernetes环境即可&#34;&gt;1、环境准备（只要Kubernetes环境即可）&lt;/h2&gt;
&lt;p&gt;Kubernetes: v1.18.3&lt;/p&gt;
&lt;h2 id=&#34;2快速开始包含bash和python实例&#34;&gt;2、快速开始（包含bash和python实例）&lt;/h2&gt;
&lt;p&gt;目录结构&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 shell-operator]# tree
.
├── Dockerfile
├── hooks
│   └── pods-hook.sh
└── shell-operator-pod.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;21shell-operator最简设置步骤为&#34;&gt;2.1、Shell-operator最简设置步骤为：&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;用钩子（脚本）构建的镜像。&lt;/li&gt;
&lt;li&gt;在Kubernetes集群中创建必要的RBAC对象。&lt;/li&gt;
&lt;li&gt;使用构建的镜像运行一个pod/deployment。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;22用钩子脚本构建镜像&#34;&gt;2.2、用钩子脚本构建镜像&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;钩子是一个脚本，当执行&amp;ndash;config选项时，配置将以yaml/json格式输出。&lt;/li&gt;
&lt;li&gt;以下创建一个简单的operator将来监视所有namespaces下的所有pod，并记录新pod的名字。&lt;/li&gt;
&lt;li&gt;包含pods-hook.sh和Dockerfile&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以bash脚本为例&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pods-hook.sh&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;#!/usr/bin/env bash

if [[ $1 == &amp;quot;--config&amp;quot; ]] ; then
  cat &amp;lt;&amp;lt;EOF
configVersion: v1
kubernetes:
- apiVersion: v1
  kind: Pod
  executeHookOnEvent: [&amp;quot;Added&amp;quot;]
EOF
else
  podName=$(jq -r .[0].object.metadata.name $BINDING_CONTEXT_PATH)
  echo &amp;quot;Pod &#39;${podName}&#39; added&amp;quot;
fi
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;添加执行权限。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;chmod +x pods-hook.sh&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于flant/shell-operator:latest的基础镜像构建新的Dockerfile。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;FROM flant/shell-operator:latest
ADD pods-hook.sh /hooks
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;构建一个新的镜像。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker build -t lilinlinlin/shell-operator:monitor-pods&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;推镜像至dockerhub, 仓库名根据自身的情况而定，也可以不操作此步骤。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker push lilinlinlin/shell-operator:monitor-pods&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;23创建rbac对象&#34;&gt;2.3、创建RBAC对象&lt;/h3&gt;
&lt;p&gt;需要监视所有namespaces下的pods，意味着我们需要shell-operator的特定RBAC定义。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create namespace example-monitor-pods
kubectl create serviceaccount monitor-pods-acc --namespace example-monitor-pods
kubectl create clusterrole monitor-pods --verb=get,watch,list --resource=pods
kubectl create clusterrolebinding monitor-pods --clusterrole=monitor-pods --serviceaccount=example-monitor-pods:monitor-pods-acc
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;24在集群中部署shell-operator&#34;&gt;2.4、在集群中部署shell-operator&lt;/h3&gt;
&lt;p&gt;shell-operator-pod.yaml文件为&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: shell-operator
spec:
  containers:
  - name: shell-operator
    image: lilinlinlin/shell-operator:monitor-pods
    imagePullPolicy: IfNotPresent
  serviceAccountName: monitor-pods-acc
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;部署shell-operator时，pods-hook.sh脚本中if的部分就会被执行。新创建pod时，else部分就会被执行。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl -n example-monitor-pods apply -f shell-operator-pod.yaml&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3测试验证效果&#34;&gt;3、测试验证效果&lt;/h2&gt;
&lt;p&gt;部署一个nginx服务,查看日志。会出现Pod nginx-****** added字样，说明监视生效了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl run nginx --image=nginx
kubectl -n example-monitor-pods logs pod/shell-operator -f
...
INFO[0027] queue task HookRun:main                       operator.component=handleEvents queue=main
INFO[0030] Execute hook                                  binding=kubernetes hook=pods-hook.sh operator.component=taskRunner queue=main task=HookRun
INFO[0030] Pod &#39;nginx-775dd7f59c-hr7kj&#39; added  binding=kubernetes hook=pods-hook.sh output=stdout queue=main task=HookRun
INFO[0030] Hook executed successfully                    binding=kubernetes hook=pods-hook.sh operator.component=taskRunner queue=main task=HookRun
...
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;4python实例&#34;&gt;4、python实例&lt;/h2&gt;
&lt;p&gt;实例中直接运行else部分。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、hooks/00-hook.py
#!/usr/bin/env python

import sys

if __name__ == &amp;quot;__main__&amp;quot;:
    if len(sys.argv)&amp;gt;1 and sys.argv[1] == &amp;quot;--config&amp;quot;:
        print &#39;{&amp;quot;configVersion&amp;quot;:&amp;quot;v1&amp;quot;, &amp;quot;onStartup&amp;quot;: 10}&#39;
    else:
        print &amp;quot;OnStartup Python powered hook&amp;quot;

2、Dockerfile
FROM flant/shell-operator:latest-alpine3.11
RUN apk --no-cache add python
ADD hooks /hooks

3、shell-operator-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: shell-operator
spec:
  containers:
  - name: shell-operator
    image: registry.mycompany.com/shell-operator:startup-python
    imagePullPolicy: IfNotPresent

4、运行

docker build -t &amp;quot;registry.mycompany.com/shell-operator:startup-python&amp;quot; .
kubectl create ns example-startup-python
kubectl -n example-startup-python apply -f shell-operator-pod.yaml
kubectl -n example-startup-python logs -f po/shell-operator
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;5清理环境&#34;&gt;5、清理环境&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;kubectl delete ns example-monitor-pods
kubectl delete clusterrole monitor-pods
kubectl delete clusterrolebinding monitor-pods
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;6参考&#34;&gt;6、参考&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/flant/shell-operator&#34;&gt;shell-operator&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>k8s-service-认知</title>
      <link>https://Forest-L.github.io/post/k8s-service-cognize/</link>
      <pubDate>Mon, 29 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s-service-cognize/</guid>
      <description></description>
    </item>
    
    <item>
      <title>k8s-ConfigMap认知</title>
      <link>https://Forest-L.github.io/post/k8s-configmap-cognize/</link>
      <pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s-configmap-cognize/</guid>
      <description>&lt;h1 id=&#34;k8s-configmap-认知&#34;&gt;k8s configMap 认知&lt;/h1&gt;
&lt;p&gt;许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息，而ConfigMap作用是保存配置信息，格式为键值对，可以单独一个key/value使用，也可以多个key/value构成的文件使用。数据不包含敏感信息的字符串。ConfigMap必须在Pod引用它之前创建;Pod只能使用同一个命名空间内的ConfigMap。&lt;/p&gt;
&lt;h2 id=&#34;1-常见configmap创建方式&#34;&gt;1 常见configMap创建方式&lt;/h2&gt;
&lt;h3 id=&#34;11-从key-value字符串创建configmap&#34;&gt;1.1 从key-value字符串创建ConfigMap&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl create configmap config1 --from-literal=config.test=good&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;12-从目录创建rootconfigmaptest1和rootconfigmaptest2-中&#34;&gt;1.2 从目录创建,/root/configmap/test1和/root/configmap/test2 中&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;vi test1
a:a1
vi test2
b:b1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create configmap special-config --from-file=/root/configmap/&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;13-通过pod形式创建最常用的方式configtestyaml内容如下&#34;&gt;1.3 通过pod形式创建,最常用的方式configtest.yaml内容如下：&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;kind: ConfigMap
apiVersion: v1
metadata:
  name: configtest
  namespace: default
data:
  test.property.1: a1
  test.property.2: b2
  test.property.file: |-
    property.1=a1
    property.2=b2
    property.3=c3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f configtest.yaml&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-常见查看configmap方式&#34;&gt;2. 常见查看configmap方式&lt;/h2&gt;
&lt;p&gt;以上面所示的第三种方式创建的configmap为例，名为：configtest。configmap可以简称cm。&lt;/p&gt;
&lt;h3 id=&#34;21--o-json格式&#34;&gt;2.1 &amp;ldquo;-o json&amp;quot;格式，&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl get cm configtest -o json&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;22-go模板的格式&#34;&gt;2.2 go模板的格式&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;-o go-template=&#39;{{.data}}&#39;格式
kubectl get configmap configtest -o go-template=&#39;{{.data}}&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3-常见使用configmap场景&#34;&gt;3. 常见使用configmap场景&lt;/h2&gt;
&lt;p&gt;通过多种方式在Pod中使用，比如设置环境变量、设置容器命令行参数、在Volume中创建配置文件等。
&lt;font color=#DC143C &gt;说 明 : &lt;/font&gt;
镜像以：buysbox镜像为例，如果能连国外的网，选择gcr.io/google_containers/busybox，不能连国外的网但能国内的外网使用：busybox:1.28.4这个镜像。
下面所有command里面都加了sleep,方便大家进容器查看配置是否起作用。&lt;/p&gt;
&lt;h3 id=&#34;31-configmap先创建好以下创建两种类型&#34;&gt;3.1 configmap先创建好,以下创建两种类型。&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm&lt;/code&gt;
&lt;code&gt;kubectl create configmap env-config --from-literal=log_level=INFO&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;32-环境参数注意busybox镜像选择及sleep时间test-podyaml&#34;&gt;3.2 环境参数，注意busybox镜像选择及sleep时间,test-pod.yaml。&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ &amp;quot;/bin/sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;sleep 60 ;env&amp;quot; ]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.type
      envFrom:
        - configMapRef:
            name: env-config
  restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f test-pod.yaml&lt;/code&gt;
进入对应的容器里，输入ENV则会包含如下结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SPECIAL_LEVEL_KEY=very
SPECIAL_TYPE_KEY=charm
log_level=INFO
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;33-命令行参数注意busybox镜像选择及sleep时间dapi-test-podyaml&#34;&gt;3.3 命令行参数，注意busybox镜像选择及sleep时间,dapi-test-pod.yaml。&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ &amp;quot;/bin/sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;sleep 60 ;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&amp;quot; ]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.type
  restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f dapi-test-pod.yaml&lt;/code&gt;
进入容器中，执行&lt;code&gt;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&lt;/code&gt;指令得到结果：
very charm&lt;/p&gt;
&lt;h3 id=&#34;34-volume将configmap作为文件或目录直接挂载注意busybox镜像选择及sleep时间当存在同名文件时直接覆盖掉vol-test-podyaml&#34;&gt;3.4 volume将ConfigMap作为文件或目录直接挂载，注意busybox镜像选择及sleep时间,当存在同名文件时，直接覆盖掉，vol-test-pod.yaml。&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: vol-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ &amp;quot;/bin/sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;sleep 60 ;cat /etc/config/special.how&amp;quot; ]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: special-config
  restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f vol-test-pod.yaml&lt;/code&gt;
进入容器，cat的结果：very&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>