<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>K8s on 一切皆有可能</title>
    <link>https://Forest-L.github.io/tags/k8s/</link>
    <description>Recent content in K8s on 一切皆有可能</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <copyright>Copyright © 2008–2020</copyright>
    <lastBuildDate>Sun, 27 Dec 2020 19:08:06 +0800</lastBuildDate>
    
	<atom:link href="https://Forest-L.github.io/tags/k8s/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>K8s中声明式的部署</title>
      <link>https://Forest-L.github.io/post/declarative-deployment/</link>
      <pubDate>Sun, 27 Dec 2020 19:08:06 +0800</pubDate>
      
      <guid>https://Forest-L.github.io/post/declarative-deployment/</guid>
      <description>&lt;p&gt;声明式Deployment模式的核心是Kubernetes 的Deployment资源。这个抽象封装了一组容器的升级和回滚过程，并使其执行成为一种可重复和自动化的活动。&lt;/p&gt;
&lt;h2 id=&#34;存在问题&#34;&gt;存在问题&lt;/h2&gt;
&lt;p&gt;我们可以以自助服务的方式将隔离的环境作为命名空间进行调配，并通过调度器将服务调度在这些环境中，只需最少的人工干预。但是随着微服务的数量越来越多，不断地用新的版本更新和更换也成了越来越大的负担。&lt;/p&gt;
&lt;p&gt;将服务升级到下一个版本涉及的活动包括启动新版本的Pod，优雅地停止旧版本的Pod，等待并验证它已经成功启动，有时在失败的情况下将其全部回滚到以前的版本。这些活动的执行方式有两种，一种是允许有一定的停机时间，但不允许同时运行并发的服务版本，另一种是没有停机时间，但在更新过程中由于两个版本的服务都在运行，导致资源使用量增加。手动执行这些步骤可能会导致人为错误，而正确地编写脚本则需要花费大量的精力，这两点都会使发布过程迅速变成瓶颈。&lt;/p&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;p&gt;幸运的是，Kubernetes也已经自动完成了这项活动。使用Deployment的概念，我们可以描述我们的应用程序应该如何更新，使用不同的策略，并调整更新过程的各个方面。如果您考虑到每次发布周期都要为每个微服务实例进行多次部署的话（根据团队和项目的不同，可能从几分钟到几个月），这是Kubernetes的另一个省力的自动化。&lt;/p&gt;
&lt;p&gt;在第2章中，我们已经看到，为了有效地完成工作，调度器需要主机上有足够的资源、适当的调度策略以及容器充分定义了资源配置文件。同样，为了使部署正确地完成其工作，它希望容器成为良好的云原生。部署的核心是可预测地启动和停止一组Pod的能力。为了达到预期的工作效果，容器本身通常会监听和符合生命周期事件（如SIGTERM；请参见第5章，托管生命周期），并且还提供第4章云原生中所述的健康检查endpoints，即健康探针，以指示它们是否成功启动。&lt;/p&gt;
&lt;p&gt;如果一个容器准确地覆盖了这两个领域，平台就可以干净利落地关闭旧的容器，并通过启动更新的实例来替换它们。然后，更新过程的所有剩余方面都可以以声明的方式定义，并作为一个原子动作执行，具有预定义的步骤和预期的结果。让我们看看容器更新行为的选项吧&lt;/p&gt;
&lt;p&gt;==注意：==&lt;/p&gt;
&lt;h4 id=&#34;使用-kubectl-进行命令式-rolling-updates已被废弃&#34;&gt;使用 kubectl 进行命令式 Rolling Updates已被废弃&lt;/h4&gt;
&lt;p&gt;Kubernetes从一开始就支持滚动更新。最早的实现是势在必行的，客户端kubectl告诉服务器每个更新步骤要做什么。&lt;/p&gt;
&lt;p&gt;虽然kubectl rolling-update 命令仍然存在，但由于这种命令式的方法存在以下缺点，所以它已经被高度废弃。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kubectl rolling-update 不是描述预期的最终状态，而是发布命令让系统进入预期状态。&lt;/li&gt;
&lt;li&gt;替换容器和ReplicationControllers的整个协调逻辑由kubectl执行，它在发生更新过程时，会监控并与API服务器交互，将固有服务器端的责任转移到客户端。&lt;/li&gt;
&lt;li&gt;您可能需要不止一条命令来使系统进入期望状态。这些命令必须是自动的，并且在不同的环境中可以重复使用。&lt;/li&gt;
&lt;li&gt;随着时间的推移，别人可能会覆盖你的修改。&lt;/li&gt;
&lt;li&gt;更新过程必须记录下来，并在服务推进的同时保持更新。&lt;/li&gt;
&lt;li&gt;要想知道我们部署了什么，唯一的方法就是检查系统的状态。有时候，当前系统的状态可能并不是理想的状态，在这种情况下，我们必须使部署文档相互关联。&lt;/li&gt;
&lt;li&gt;取而代之的是，引入Deployment资源对象来支持声明式更新，完全由Kubernetes后端管理。由于声明式更新有如此多的优势，而命令式更新支持终将消失，我们在这个模式中只关注声明式更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;rolling-deployment&#34;&gt;Rolling Deployment&lt;/h4&gt;
&lt;p&gt;Kubernetes中更新应用的声明方式是通过Deployment的概念。在幕后，Deployment创建了一个ReplicaSet，支持基于集合的标签选择器。同时，Deployment抽象允许通过RollingUpdate（默认）和Recreate等策略来塑造更新过程行为。例1-1显示了Deployment配置的滚动更新策略重要部分。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;例1-1，Deployment for a rolling update
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: random-generator
spec: 
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    selector:
      matchLabels:
        app: random-generator
    template:
      metadata:
        labels:
          app: random-generator
      spec: 
        containers:
        - image: k8spatterns/random-generator:1.0
          name: random-generator 
          readinessProbe:
            exec:
              command: [&amp;quot;stat&amp;quot;,&amp;quot;/random-generator-ready&amp;quot; ]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;滚动更新（RollingUpdate）策略行为确保了更新过程中没有停机时间。在幕后，Deployment实现类似这样的动作来执行，通过创建新的ReplicaSets和用新容器替换旧容器。通过Deployment，这里有一个增强是可以控制新容器滚动的速度。Deployment对象允许你通过maxSurge和maxUnavailable字段来控制可用和超出Pod的范围。图1-1展示了滚动更新过程。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/006bbiLEgy1glyrd6sisrj30eh04s3z1.jpg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;要触发声明式更新，你有三个选项:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用 kubectl replace 将整个部署替换为新版本的部署。&lt;/li&gt;
&lt;li&gt;补丁(kubectl patch)或交互式编辑(kubectl edit)部署，以设置新版本的新容器镜像。&lt;/li&gt;
&lt;li&gt;使用 kubectl set image 来设置部署中的新镜像。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;请参阅我们的示例仓库中的完整示例，其中演示了这些命令的用法，并展示了如何使用kubectl rollout监控或回滚升级。&lt;/p&gt;
&lt;p&gt;除了解决前面提到的命令部署服务方式的弊端外，Deployment还带来了以下好处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deployment是一个Kubernetes资源对象，其状态完全由Kubernetes内部管理。整个更新过程在服务器端进行，无需客户端交互。&lt;/li&gt;
&lt;li&gt;Deployment的声明性使您可以看到已部署的状态应该是怎样的，而不是到达那里的必要步骤。&lt;/li&gt;
&lt;li&gt;Deployment定义是一个可执行的对象，在上生产之前会在多个环境中进行测试。&lt;/li&gt;
&lt;li&gt;更新过程也会被完全记录下来，版本上有暂停、继续和回滚到以前版本的选项。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;fixed-deployment&#34;&gt;Fixed Deployment&lt;/h4&gt;
&lt;p&gt;滚动更新（RollingUpdate）策略对于在更新过程中确保零停机时间非常有用。然而，这种方法的副作用是，在更新过程中，容器的两个版本同时运行。这可能会给服务消费者带来问题，特别是当更新过程在服务API中引入了向后不兼容的变化，而客户端又无法处理这些变化时。对于这种情况，有Recreate策略，如图1-2所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/006bbiLEgy1glys496y3lj30eg052t9b.jpg&#34; alt=&#34;fixed Deployment.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Recreate策略的效果是将maxUnavailable设置为已声明的复制数。这意味着它首先杀死当前版本的所有容器，然后在旧容器被驱逐时同时启动所有新容器。这意味着它首先杀死当前版本的所有容器，然后在旧容器被驱逐时同时启动所有新容器。这一系列操作的结果是，当所有拥有旧版本的容器被停止时，会有一些停机时间，而且没有新的容器准备好处理传入的请求。从积极的一面来看，不会有两个版本的容器同时运行，简化了服务消费者的生活，一次只处理一个版本。&lt;/p&gt;
&lt;h4 id=&#34;蓝绿发布&#34;&gt;蓝绿发布&lt;/h4&gt;
&lt;p&gt;蓝绿部署是一种用于在最小化停机时间和降低风险的生产环境中部署软件的发布策略。Kubernetes&amp;rsquo;Deployment抽象是一个基本概念，它可以让你定义Kubernetes如何将不可变容器从一个版本过渡到另一个版本。我们可以将Deployment基元作为一个构件，与其他Kubernetes基元一起，实现这种更高级的蓝绿部署的发布策略。&lt;/p&gt;
&lt;p&gt;如果没有使用Service Mesh或Knative等扩展，则需要手动完成蓝绿部署。技术上，它的工作原理是通过创建第二个Deployment，容器的最新版本（我们称它为绿色）还没有服务于任何请求。在这一阶段，原始Deployment中的旧Pod副本（称为蓝色）仍在运行并服务于实时请求。&lt;/p&gt;
&lt;p&gt;一旦我们确信新版本的Pods是健康的，并且准备好处理实时请求，我们就会将流量从旧的Pod副本切换到新的副本。Kubernetes中的这个活动可以通过更新服务选择器来匹配新容器（标记为绿色）来完成。如图1-3所示，一旦绿色容器处理了所有的流量，就可以删除蓝色容器，释放资源用于未来的蓝绿部署。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/006bbiLEgy1glysteayffj30f105bmxr.jpg&#34; alt=&#34;蓝绿发布.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;蓝绿方式的一个好处是，只有一个版本的应用在服务请求，这降低了Service消费者处理多个并发版本的复杂性。缺点是它需要两倍的应用容量，同时蓝色和绿色容器都在运行。另外，在过渡期间，可能会出现长时间运行的进程和数据库状态漂移的重大并发症。&lt;/p&gt;
&lt;h4 id=&#34;金丝雀发布&#34;&gt;金丝雀发布&lt;/h4&gt;
&lt;p&gt;金丝雀发布是一种通过用新的实例替换一小部分旧的实例来软性地将一个应用程序的新版本部署到生产中的方法。这种技术通过只让部分消费者达到更新的版本来降低将新版本引入生产的风险。当我们对新版本的服务以及它在小样本用户中的表现感到满意时，我们就用新版本替换所有的旧实例。图1-4显示了一个金丝雀版本的运行情况。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/006bbiLEgy1glyt4zxuo9j30ey0580t8.jpg&#34; alt=&#34;金丝雀发布.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;在Kubernetes中，可以通过为新的容器版本（最好使用Deployment）创建一个新的ReplicaSet来实现这一技术，该ReplicaSet的副本数量较少，可以作为Canary实例使用。在这个阶段，服务应该将一些消费者引导到更新的Pod实例上。一旦我们确信使用新 ReplicaSet 的一切都能按预期工作，我们就会将新的 ReplicaSet 规模化，旧的 ReplicaSet 则降为零。从某种程度上来说，我们是在进行一个可控的、经过用户测试的增量式推广。&lt;/p&gt;
&lt;h2 id=&#34;讨论&#34;&gt;讨论&lt;/h2&gt;
&lt;p&gt;Deoloyment本质是 Kubernetes 将手动更新应用程序的繁琐过程转化为可重复和自动化的声明式活动的一个例子。开箱即用的部署策略（rolling和recreate）控制用新容器替换旧容器，而发布策略（bluegreen和金丝雀）则控制新版本如何提供给服务消费者。后两种发布策略是基于人对过渡触发器的决定，因此不是完全自动化的，而是需要人的交互。图1-5显示了部署和发布策略的摘要，显示了过渡期间的实例数。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/006bbiLEgy1glyw6xng8fj30d70a1dgn.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;每个软件都是不同的，部署复杂的系统通常需要额外的步骤和检查。本章讨论的技术涵盖了Pod更新过程，但不包括更新和回滚其他Pod依赖项，如ConfigMaps、Secrete或其他依赖服务。&lt;/p&gt;
&lt;p&gt;截至目前，Kubernetes有一个建议，允许在部署过程中使用钩子。Pre和Post钩子将允许在Kubernetes执行部署策略之前和之后执行自定义命令。这些命令可以在部署进行时执行额外的操作，另外还可以中止、重试或继续部署。这些命令是向新的自动化部署和发布策略迈出的良好一步。目前，一种行之有效的方法是用脚本化去更高层次上编写更新过程，本节中讨论的Deployment和其他属性来管理服务及其依赖关系的更新过程。&lt;/p&gt;
&lt;p&gt;无论你使用何种部署策略，Kubernetes都必须知道你的应用Pod何时启动并运行，以执行所需的步骤序列达到定义的目标部署状态。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>K8s中可预测的需求</title>
      <link>https://Forest-L.github.io/post/predictable-demands/</link>
      <pubDate>Sat, 26 Dec 2020 15:38:59 +0800</pubDate>
      
      <guid>https://Forest-L.github.io/post/predictable-demands/</guid>
      <description>&lt;p&gt;在共享云环境中成功部署、管理和共存应用的基础，取决于识别和声明应用资源需求和运行时依赖性。这个Predictable Demands模式是关于你应该如何声明应用需求，无论是硬性的运行时依赖还是资源需求。声明你的需求对于Kubernetes在集群中为你的应用找到合适的位置至关重要。&lt;/p&gt;
&lt;h2 id=&#34;存在问题&#34;&gt;存在问题&lt;/h2&gt;
&lt;p&gt;Kubernetes可以管理用不同编程语言编写的应用，只要该应用可以在容器中运行。然而，不同的语言有不同的资源需求。通常情况下，编译后的语言运行速度更快，而且经常是
与即时运行时或解释语言相比，需要更少的内存。考虑到很多同类别的现代编程语言对资源的要求都差不多，从资源消耗的角度来看，更重要的是领域、应用的业务逻辑和实际实现细节。&lt;/p&gt;
&lt;p&gt;很难预测容器可能需要多少资源才能发挥最佳功能，而知道服务运行的预期资源是开发人员（通过测试发现）。有些服务的CPU和内存消耗情况是固定的，有些服务则是瞬间的。有些服务需要持久性存储来存储数据；有些传统服务需要在主机上固定端口号才能正常工作。定义所有这些应用特性并将其传递给管理平台是云原生应用的基本前提。&lt;/p&gt;
&lt;p&gt;除了资源需求外，应用运行时还对平台管理的能力有依赖性，如数据存储或应用配置。&lt;/p&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;p&gt;了解容器的运行时要求很重要，主要有两个原因。首先，在定义了所有的运行时依赖和资源需求设想后，Kubernetes可以智能地决定在集群上的哪里运行容器以获得最有效的硬件利用率。在大量优先级不同的进程共享资源的环境中，要想成功共存，唯一的办法就是提前了解每个进程的需求。然而，智能投放只是硬币的一面。&lt;/p&gt;
&lt;p&gt;容器资源配置文件必不可少的第二个原因是容量规划。根据具体的服务需求和服务总量，我们可以针对不同的环境做一些容量规划，得出性价比最高的主机配置文件，来满足整个集群的需求。服务资源配置文件和容量规划相辅相成，才能长期成功地进行集群管理。&lt;/p&gt;
&lt;p&gt;在深入研究资源配置文件之前，我们先来看看如何声明运行时依赖关系。&lt;/p&gt;
&lt;h4 id=&#34;运行时依赖&#34;&gt;运行时依赖&lt;/h4&gt;
&lt;p&gt;最常见的运行时依赖之一是用于保存应用程序状态的文件存储。容器文件系统是短暂的，当容器关闭时就会丢失。Kubernetes提供了volume作为Pod级的存储实用程序，可以在容器重启后幸存。&lt;/p&gt;
&lt;p&gt;最直接的卷类型是emptyDir，只要Pod存活，它就会存活，当Pod被删除时，它的内容也会丢失。卷需要有其他类型的存储机制支持，才能有一个在Pod重启后仍能存活的卷。如果你的应用程序需要向这种长时间的存储设备读写文件，你必须在容器定义中使用volumes明确声明这种依赖性。
如例1-1所示。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;如例1-1，依赖于PV
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    volumeMounts:
    - mountPath:&amp;quot;/logs&amp;quot;
      name: log-volume
  volumes:
  - name: log-volume
    persistentVolumeClaim:
      claimName: random-generator-log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;调度器会评估Pod所需要的卷类型，这将影响Pod的调度位置。如果Pod需要的卷不是由集群上的任何节点提供的，那么Pod根本不会被调度。卷是运行时依赖性的一个例子，它影响Pod可以运行什么样的基础设施，以及Pod是否可以被调度。&lt;/p&gt;
&lt;p&gt;当你要求Kubernetes通过hostPort方式暴露容器端口为主机上特定端口时，也会发生类似的依赖关系。hostPort的使用在节点上创建了另一个运行时依赖性，并限制了Pod的调度位置。 hostPort在集群中的每个节点上保留了端口，并限制每个节点最多调度一个Pod。由于端口冲突，你可以扩展到Kubernetes集群中有多少节点就有多少Pod。&lt;/p&gt;
&lt;p&gt;另一种类型的依赖是配置。几乎每个应用程序都需要一些配置信息，Kubernetes提供的推荐解决方案是通过ConfigMaps。你的服务需要有一个消耗设置的策略&amp;ndash;无论是通过环境变量还是文件系统。无论是哪种情况，这都会引入你的容器对名为ConfigMaps的运行时依赖性。如果没有创建所有预期的 ConfigMaps，则容器被调度在节点上，但它们不会启动。ConfigMaps和Secrets在第19章Configuratio资源中进行了更详细的解释，例1-2展示了如何将这些资源用作运行时依赖。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;实例1-2所示，依赖于ConfigMap
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    env:
    - name: PATTERN
      valueFrom:
        configMapKeyRef:
          name: random-generator-config
          key: pattern
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;与ConfigMaps类似的概念是Secrets，它提供了一种略微更安全的方式将特定环境的配置分发到容器中。使用Secret的方式与使用ConfigMap的方式相同，它引入了从容器到namespace的相同依赖性。&lt;/p&gt;
&lt;p&gt;虽然ConfigMap和Secret对象的创建是我们必须执行的简单管理任务，但集群节点提供了存储和端口。其中一些依赖性限制了Pod被调度的位置（如果有的话），而其他依赖性则限制了Pod的运行。
可能会阻止Pod的启动。在设计带有这种依赖关系的容器化应用程序时，一定要考虑它们创建之后运行时的约束。&lt;/p&gt;
&lt;h4 id=&#34;资源配置文件&#34;&gt;资源配置文件&lt;/h4&gt;
&lt;p&gt;指定容器的依赖性，如ConfigMap、Secret和卷，是很直接的。我们需要更多的思考和实验来确定容器的资源需求。在Kubernetes的上下文中，计算资源被定义为可以被容器请求、分配给容器并从容器中获取的东西。资源分为可压缩的（即可以节制的，如CPU，或网络带宽）和不可压缩的（即不能节制的，如内存）。&lt;/p&gt;
&lt;p&gt;区分可压缩资源和不可压缩资源很重要。如果你的容器消耗了太多的可压缩资源（如CPU），它们就会被节流，但如果它们使用了太多的不可压缩资源（如内存），它们就会被杀死（因为没有其他方法可以要求应用程序释放分配的内存）。&lt;/p&gt;
&lt;p&gt;根据你的应用程序的性质和实现细节，你必须指定所需资源的最小量（称为请求）和它可以增长到的最大量（限制）。每个容器定义都可以以请求和限制的形式指定它所需要的CPU和内存量。在一个高层次上，请求/限制的概念类似于软/硬限制。例如，同样地，我们通过使用-Xms和-Xmx命令行选项来定义Java应用程序的堆大小。&lt;/p&gt;
&lt;p&gt;调度器将Pod调度到节点时，使用的是请求量（但不是限制）。对于一个给定的Pod，调度器只考虑那些仍有足够能力容纳Pod及其所有请求资源量相加容器的节点。从这个意义上说，每个容器的请求字段会影响到Pod可以被调度或不被调度的位置。例1-3显示了如何为Pod指定这种限制。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;实例1-3，资源限制
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec: 
  containers: 
  - image: k8spatterns/random-generator:1.0   name: random-generator 
    resources:
      requests:  
        cpu: 100m 
        memory: 100Mi 
      limits:  
        cpu: 200m 
        memory: 200Mi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;根据您是指定请求、限制，还是两者都指定，平台提供不同的服务质量（QoS）。&lt;/p&gt;
&lt;h5 id=&#34;best-effort&#34;&gt;Best-Effort&lt;/h5&gt;
&lt;p&gt;没有为其容器设置任何请求和限制的Pod。这样的Pod被认为是最低优先级的，当Pod的节点用完不可压缩资源时，会首先被干掉。&lt;/p&gt;
&lt;h5 id=&#34;burstable&#34;&gt;Burstable&lt;/h5&gt;
&lt;p&gt;已定义请求和限制的Pod，但它们并不相等（而且限制比预期的请求大）。这样的Pod有最小的资源保证，但也愿意在可用的情况下消耗更多的资源，直至其极限。当节点面临不可压缩的资源压力时，如果没有Best-Effort Pods剩余，这些Pod很可能被干掉。&lt;/p&gt;
&lt;h5 id=&#34;guaranteed&#34;&gt;Guaranteed&lt;/h5&gt;
&lt;p&gt;拥有同等数量请求和限制资源的Pod。这些是优先级最高的Pod，保证不会在Best-Effort和Burstable Pods之前被干掉。&lt;/p&gt;
&lt;p&gt;所以你为容器定义的资源特性或省略资源特性会直接影响到它的QoS，并定义了Pod在资源不足时的相对重要性。在定义你的Pod资源需求时，要考虑到这个后果。&lt;/p&gt;
&lt;h4 id=&#34;pod优先级&#34;&gt;Pod优先级&lt;/h4&gt;
&lt;p&gt;我们解释了容器资源声明如何也定义了Pod的QoS，并影响Kubelet在资源不足时干掉Pod中容器的顺序。另一个相关的功能，在写这篇文章的时候还在测试阶段，就是Pod优先和优先权。Pod优先级允许表明一个Pod相对于其他Pod的重要性，这影响了Pod的调度顺序。让我们在例子1-4中看到它的作用。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;实例1-4，pod优先级
apiVersion: scheduling.k8s.io/v1beta1
kind: PriorityClass
metadata: 
  name: high-priority 
value: 1000 
globalDefault: false
description: This is a very high priority Pod class
---
apiVersion: v1
kind: Pod
metadata: 
  name: random-generator 
  labels: 
    env: random-generator
spec: 
  containers: 
  - image: k8spatterns/random-generator:1.0 
    name: random-generator   
  priorityClassName: high-priority
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们创建了一个PriorityClass，这是一个非命名空间的对象，用于定义一个基于整数的优先级。我们的PriorityClass被命名为high-priority，优先级为1,000。现在我们可以通过它的名字将这个优先级分配给Pods，如priorityClassName：high-riority。PriorityClass是一种表示Pods相对重要性的机制，数值越高表示Pods越重要。&lt;/p&gt;
&lt;p&gt;启用Pod Priority功能后，它会影响调度器将Pod调度在节点上的顺序。首先，优先权进入许可控制器使用priorityClass Name字段来填充新Pod的优先权值。当有多个Pod等待调度时，调度器按最高优先级对待放Pod队列进行排序。在调度队列中，任何待定的Pod都会被选在其他优先级较低的待定Pod之前，如果没有阻止其调度的约束条件，该Pod就会被调度。&lt;/p&gt;
&lt;p&gt;下面是关键部分。如果没有足够容量的节点来调度Pod，调度器可以从节点上抢占（移除）优先级较低的Pod，以释放资源，调度优先级较高的Pod。因此，如果满足其他所有调度要求，优先级较高的Pod可能比优先级较低的Pod更早被调度。这种算法有效地使集群管理员能够控制哪些Pod是更关键的工作负载，并通过允许调度器驱逐优先级较低的Pod，以便在工作节点上为优先级较高的Pod腾出空间，将它们放在第一位。如果一个Pod不能被调度，调度器就会继续调度其他优先级较低的Pod。&lt;/p&gt;
&lt;p&gt;Pod QoS（前面已经讨论过了）和Pod优先级是两个正交的特性，它们之间没有联系，只有一点点重叠。QoS主要被Kubelet用来在可用计算资源较少时保持节点稳定性。==Kubelet在驱逐前首先考虑QoS，然后考虑Pods的PriorityClass。另一方面，调度器驱逐逻辑在选择抢占目标时完全忽略了Pods的QoS==。调度器试图挑选一组优先级最低的Pod，满足优先级较高的Pod等待调度的需求。&lt;/p&gt;
&lt;p&gt;当Pod具有指定的优先级时，它可能会对其他被驱逐的Pod产生不良影响。例如，当一个Pod的优雅终止策略受到重视，第10章中讨论的PodDisruptionBudget，单服务没有得到保证，这可能会打破一个依赖多数Pod数的较低优先级集群应用。&lt;/p&gt;
&lt;p&gt;另一个问题是恶意或不知情的用户创建了优先级最高的Pods，并驱逐了所有其他Pods。为了防止这种情况发生，ResourceQuota已经扩展到支持PriorityClass，较大的优先级数字被保留给通常不应该被抢占或驱逐的关键系统Pods。&lt;/p&gt;
&lt;p&gt;总而言之，Pod优先级应谨慎使用，因为用户指定的数字优先级，指导调度器和Kubelet调度或干掉哪些Pod，会受到用户的影响。任何改变都可能影响许多Pod，并可能阻止平台提供可预测的服务级别协议。&lt;/p&gt;
&lt;h4 id=&#34;项目资源&#34;&gt;项目资源&lt;/h4&gt;
&lt;p&gt;Kubernetes是一个自助服务平台，开发者可以在指定的隔离环境上运行他们认为合适的应用。然而，在一个共享的多租户平台中工作，也需要存在特定的边界和控制单元，以防止一些用户消耗平台的所有资源。其中一个这样的工具是ResourceQuota，它为限制命名空间中的聚合资源消耗提供了约束。通过ResourceQuotas，集群管理员可以限制消耗的计算资源（CPU、内存）和存储的总和。它还可以限制命名空间中创建的对象（如ConfigMaps、Secrets、Pods或Services）的总数。&lt;/p&gt;
&lt;p&gt;这方面的另一个有用的工具是LimitRange，它允许为每种类型的资源设置资源使用限制。除了指定不同资源类型的最小和最大允许量以及这些资源的默认值外，还可以控制请求和限制之间的比例，也就是所谓的超额承诺水平。表1-1给出了如何选择请求和限额的可能值的例子。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;表1-1. Limit和request ranges
Type       Resource  Min    Max  Default limit  Default request  Lim/req ratio  
Container  CPU       500m   2    500m           250m             4
Container  Memory    250Mi  2Gi  500Mi          250Mi            4
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;LimitRanges对于控制容器资源配置非常有用，这样就不会出现需要的资源超过集群节点所能提供的资源的容器。它还可以防止集群用户创建消耗大量资源的容器，使节点不能为其他容器分配资源。考虑到请求(而不是限制)是调度器用来调度的主要容器特性，LimitRequestRatio允许你控制容器的请求和限制之间的差距有多大。在请求和限制之间有很大的综合差距，会增加节点上超负荷的机会，并且当许多容器同时需要比最初请求更多的资源时，可能会降低应用性能。&lt;/p&gt;
&lt;h4 id=&#34;容量规划&#34;&gt;容量规划&lt;/h4&gt;
&lt;p&gt;考虑到容器在不同的环境中可能会有不同的资源情况，以及不同数量的实例，显然，多用途环境的容量规划并不简单。例如，为了获得最佳的硬件利用率，在一个非生产集群上，你可能主要拥有Best-Effort和Burstable容器。在这样的动态环境中，很多容器都是同时启动和关闭的，即使有容器在资源不足的时候被平台干掉，也不会致命。在生产集群上，我们希望事情更加稳定和可预测，容器可能主要是Guaranteed类型，还有一些Burstable。如果一个容器被杀死，那很可能是一个信号，说明集群的容量应该增加。&lt;/p&gt;
&lt;p&gt;当然，在现实生活中，你使用Kubernetes这样的平台，更可能的原因是还有很多服务需要管理，有些服务即将退出，有些服务还在设计开发阶段。即使是一个不断移动的目标，根据前面描述的类似方法，我们可以计算出每个环境中所有服务所需要的资源总量。&lt;/p&gt;
&lt;h2 id=&#34;讨论&#34;&gt;讨论&lt;/h2&gt;
&lt;p&gt;容器不仅对隔离进程和作为打包方式有用。在确定了资源概况后，它们也是成功进行产能规划的基石。进行一些早期测试，以发现每个容器的资源需求，并将该信息作为未来产能规划和预测的基础。&lt;/p&gt;
&lt;p&gt;然而，更重要的是，资源配置文件是应用程序与Kubernetes沟通的方式，以协助调度和管理决策。如果你的应用不提供任何请求或限制，Kubernetes能做的就是把你的容器当作不透明的盒子，当集群满了的时候就会丢掉。所以，每一个应用或多或少都要考虑和提供这些资源声明。&lt;/p&gt;
&lt;p&gt;现在你已经知道了如何确定我们应用的大小，在第3章 &amp;ldquo;声明式部署 &amp;ldquo;中，你将学习多种策略来让我们的应用在Kubernetes上安装和更新。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>k8s分布式存储总结</title>
      <link>https://Forest-L.github.io/post/k8s-storage-summary/</link>
      <pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s-storage-summary/</guid>
      <description>&lt;h2 id=&#34;1pvpvc和sc来源&#34;&gt;1、pv、pvc和sc来源&lt;/h2&gt;
&lt;p&gt;pv引入解耦了pod与底层存储；pvc引入分离声明与消费，分离开发与运维责任，存储由运维系统人员管理，开发人员只需要通过pvc声明需要存储的类型、大小和访问模式即可；sc引入使pv自动创建或删除，开发人员定义的pvc中声明stroageclassname以及大小等需求自动创建pv；运维人员只需要声明好sc以及quota配额即可，无需维护pv。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;早期pod使用volume方式，每次pod都需要配置存储，volume都需要配置存储插件的一堆配置，如果是第三方存储，配置非常复杂；强制开发人员需要了解底层存储类型和配置。从而引入了pv。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    volumeMounts:
    - mountPath: /data
      name: data
  volumes:
  - name: data
    capacity:
      storage: 10Gi
    cephfs:
      monitors:
      - 192.168.0.1:6789
      - 192.168.0.2:6789
      - 192.168.0.3:6789
      path: /opt/eshop_dir/eshop
      user: admin
      secretRef:
        name: ceph-secret

&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;pv的yaml文件，pv其实就是把volume的配置声明从pod中分离出来。存储系统由运维人员管理，开发人员不知道底层配置，所以引入了pvc。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: cephfs
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  cephfs:
    monitors:
    - 192.168.0.1:6789
    - 192.168.0.2:6789
    - 192.168.0.3:6789
    path: /opt/eshop_dir/eshop
    user: admin
    secretRef:
      name: ceph-secret
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;pvc的yaml文件，pvc会根据声明的大小、存储类型和accessMode等关键字查找pv，如果找到了匹配的pv，则会与之关联,而pod直接关联pvc。运维人员需要维护一堆pv，如果pv不够还需要手工创建新的pv，pv空闲还需要手动回收，所以引入了sc。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: cephfs
spec:
  accessModes:
      - ReadWriteMany
  resources:
      requests:
        storage: 8Gi
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;storageclass类似声明了一个非常大的存储池，其中一个最重要参数是provisioner，这个provisioner可以aws-ebs，ceph和nfs等。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: aws-gp2
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  fsType: ext4
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2存储发展过程&#34;&gt;2、存储发展过程&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;最初通过volume plugin实现，又称in-tree。&lt;/li&gt;
&lt;li&gt;1.8开始，新的插件形式支持外部存储系统，即FlexVolume,通过外部脚本集成外部存储接口。&lt;/li&gt;
&lt;li&gt;1.9开始，csi接入，存储厂商需要实现三个服务接口Identity Service、Controller Service、Node Service。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Identity service用于返回一些插件信息。
Controller Service实现Volume的curd操作。
Node Service运行在所有的Node节点，用于实现把volume挂载在当前Node节点的指定目录，该服务会监听一个socket，controller通过这个socket进行通信。
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;云原生分布式存储（Container Attached Storage）CAS&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;1、重新设计一个分布式存储，像openebs/longhorn/PortWorx/StorageOS。
2、已有的分布式存储包装管理，像Rook。
3、CAS：每个volume都由一个轻量级的controller来管理，这个controller可以是一个单独的pod；这个controller与使用该volume的应用pod在同一个node；不同的volume的数据使用多个独立的controller pod进行管理。
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3分布式存储分类&#34;&gt;3、分布式存储分类&lt;/h2&gt;
&lt;h4 id=&#34;31-块存储&#34;&gt;3.1 块存储&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;比如我们使用的ceph，ceph通过rbd实现块存储&lt;a href=&#34;https://kubesphereio.com/post/k8s-rook-install/&#34;&gt;rook搭建&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;32-共享存储&#34;&gt;3.2 共享存储&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;共享文件系统存储即提供文件系统存储接口，我们最常用的共享文件系统存储如NFS、CIFS、GlusterFS等，Ceph通过CephFS实现共享文件系统存储。&lt;a href=&#34;https://kubesphereio.com/post/linux-nfs-install/&#34;&gt;nfs搭建&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;33-对象存储&#34;&gt;3.3 对象存储&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Ceph通过RGW实现对象存储接口，RGW兼容AWS S3 API，因此Pod可以和使用S3一样使用Ceph RGW，比如Python可以使用boto3 SDK对桶和对象进行操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4各个存储fio性能测试供参考&#34;&gt;4、各个存储fio性能测试，供参考。&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://Forest-L.github.io/img/storage-fio.png&#34; alt=&#34;存储性能对比&#34;&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Pmem与块存储性能对比</title>
      <link>https://Forest-L.github.io/post/pmem-versus-block-storage-performance/</link>
      <pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/pmem-versus-block-storage-performance/</guid>
      <description>&lt;h2 id=&#34;1pmem介绍&#34;&gt;1、Pmem介绍&lt;/h2&gt;
&lt;p&gt;PMEM是硬件产品，Intel Optane DC持久存储模块，是一种具有大容量和数据持久性的创新存储技术。有2种运行模式。两级内存模式无需软件更改，DCPMM被视为更大的内存，并使用DRAM作为其缓存层。AppDirect模式将设备暴露为持久内存，支持软件栈，可用于加速不同的应用程序。在本文中，我们将使用AppDirect模式。&lt;/p&gt;
&lt;h2 id=&#34;2环境信息&#34;&gt;2、环境信息&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;一台master和一台node构成的K8s1.18.6集群&lt;/li&gt;
&lt;li&gt;redis镜像为根据源码编译为pmem-redis:4.0.0&lt;/li&gt;
&lt;li&gt;ceph/neonsan块存储&lt;/li&gt;
&lt;li&gt;centos7.7/内核5.8.7&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3测试两种模式&#34;&gt;3、测试两种模式&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;测试目的是体验PMEM对REDIS的加速效果。先在没有PMEM加速AOF模式，再有PMEM加速PBA模式。&lt;/li&gt;
&lt;li&gt;AOF模式是append only file的意思，通常REDIS是一种内存数据库，数据掉电就丢失了。AOF模式可以把数据库记录随时备份到分布式存储里，这样可以使得REDIS具有掉电恢复的功能。&lt;/li&gt;
&lt;li&gt;PBA模式是pointer based AOF模式，它是使用PMEM对AOF做了加速，原理是备份写盘时只把指针写到磁盘里，数据还在内存或PMEM里，使用PMEM作为缓存。这样既可以掉电恢复，又提升了性能。充分发挥了PMEM AD模式的优势。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4pmem-redis镜像构建&#34;&gt;4、pmem-redis镜像构建&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;git clone &lt;a href=&#34;https://github.com/clayding/opencloud_benchmark.git&#34;&gt;https://github.com/clayding/opencloud_benchmark.git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;cd opencloud_benchmark/k8s/redis/docker&lt;/li&gt;
&lt;li&gt;docker build -t pmem-redis:latest &amp;ndash;network host .&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;5pba模式的设置&#34;&gt;5、PBA模式的设置&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ipmctl安装&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;cd /etc/yum.repos.d/
wget https://copr.fedorainfracloud.org/coprs/jhli/ipmctl/repo/epel-7/jhli-ipmctl-epel-7.repo
wget https://copr.fedorainfracloud.org/coprs/jhli/safeclib/repo/epel-7/jhli-safeclib-epel-7.repo
yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
el-ha-for-rhel-*-server-rpms&amp;quot;
yum install ndctl ndctl-libs ndctl-devel libsafec rubygem-asciidoctor
yum install ipmctl
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;app direct模式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sudo ipmctl delete -goal
sudo ipmctl create -goal PersistentMemoryType=AppDirect

A reboot is required to process new memory allocation goals:
sudo reboot
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;检测Pmem能正常工作且为ad模式,ad是否有值&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;ipmctl show -memoryresources
 MemoryType   | DDR         | PMemModule  | Total
========================================================
 Volatile     | 191.000 GiB | 0.000 GiB   | 191.000 GiB
 AppDirect    | -           | 504.000 GiB | 504.000 GiB
 Cache        | 0.000 GiB   | -           | 0.000 GiB
 Inaccessible | 1.000 GiB   | 1.689 GiB   | 2.689 GiB
 Physical     | 192.000 GiB | 505.689 GiB | 697.689 GiB

 当ad没有值时，ipmctl start -diagnostic诊断是否有错误消息
 刚开始遇到这样的一个问题： 
 The platform configuration check detected that PMem module 0x0001 is not configured.
 分析为：新版Ipmctl有问题，用1.x的版本把pcd delete以后重新provision就可以了
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;6pmem-csi安装&#34;&gt;6、Pmem-csi安装&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;下载， https://github.com/intel/pmem-csi/blob/devel/docs/install.md#install-pmem-csi-driver
cd pmem-CSI

Setting up certificates for securities
# curl -L https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o _work/bin/cfssl --create-dirs
# curl -L https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o _work/bin/cfssljson --create-dirs
# chmod a+x _work/bin/cfssl _work/bin/cfssljson
# export PATH=$PATH:$PWD/_work/bin
# ./test/setup-ca-kubernetes.sh

Deploying the driver to K8s using LVM mode, please choose yaml files corresponding to your kubernetes version
# kubectl create -f deploy/kubernetes-1.18/pmem-csi-lvm.yaml
Applying a storage class
# kubectl apply -f deploy/kubernetes-1.18/pmem-storageclass-ext4.yaml
pod状态
kubectl get pod
NAME                    READY   STATUS        RESTARTS   AGE
pmem-csi-controller-0   2/2     Running       0          22s
pmem-csi-node-tw4mw     2/2     Running       2          33h
sc状态
kubectl get sc
NAME               PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
pmem-csi-sc-ext4   pmem-csi.intel.com         Delete          Immediate           false                  3d2h
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;7benchmark安装&#34;&gt;7、benchmark安装&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;yum install autoconf automake make gcc-c++&lt;/li&gt;
&lt;li&gt;yum install pcre-devel zlib-devel libmemcached-devel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Remove system libevent and install new version:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sudo yum remove libevent&lt;/li&gt;
&lt;li&gt;wget &lt;a href=&#34;https://github.com/downloads/libevent/libevent/libevent-2.0.21-stable.tar.gz&#34;&gt;https://github.com/downloads/libevent/libevent/libevent-2.0.21-stable.tar.gz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;tar xfz libevent-2.0.21-stable.tar.gz&lt;/li&gt;
&lt;li&gt;pushd libevent-2.0.21-stable&lt;/li&gt;
&lt;li&gt;./configure&lt;/li&gt;
&lt;li&gt;make&lt;/li&gt;
&lt;li&gt;sudo make install&lt;/li&gt;
&lt;li&gt;popd&lt;/li&gt;
&lt;li&gt;export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:${PKG_CONFIG_PATH}&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Build and install memtier:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;git clone &lt;a href=&#34;https://github.com/RedisLabs/memtier_benchmark&#34;&gt;https://github.com/RedisLabs/memtier_benchmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;cd memtier_benchmark&lt;/li&gt;
&lt;li&gt;autoreconf -ivf&lt;/li&gt;
&lt;li&gt;./configure &amp;ndash;disable-tls&lt;/li&gt;
&lt;li&gt;make&lt;/li&gt;
&lt;li&gt;sudo make install&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;8测试对比&#34;&gt;8、测试对比&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;aof的yanl文件&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language----&#34; data-lang=&#34;---&#34;&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 2Gi
  storageClassName: csi-neonsan
---
kind: Pod
apiVersion: v1
metadata:
  name: redis-aof
  labels:
    app: redis-aof
spec:
  containers:
    - name: redis-aof
      image: pmem-redis:latest
      imagePullPolicy: IfNotPresent
      args: [&amp;quot;no&amp;quot;]
      ports:
      - containerPort: 6379
      resources:
        limits:
          cpu: &amp;quot;1000m&amp;quot;
          memory: &amp;quot;16Gi&amp;quot;
        requests:
          cpu: &amp;quot;1000m&amp;quot;
          memory: &amp;quot;16Gi&amp;quot;
      volumeMounts:
      - mountPath: &amp;quot;/ceph&amp;quot;
        name: ceph-csi-volume
  volumes:
  - name: ceph-csi-volume
    persistentVolumeClaim:
      claimName: rbd-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: redis
  labels:
    app: redis-aof
spec:
  type: NodePort
  ports:
  - name: redis
    port: 6379
    nodePort: 30379
    targetPort: 6379
  selector:
    app: redis-aof
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;pba的yanl文件&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pmem-csi-pvc-ext4
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  storageClassName: pmem-csi-sc-ext4 # defined in pmem-storageclass-ext4.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 2Gi
  storageClassName: csi-neonsan
---
kind: Pod
apiVersion: v1
metadata:
  name: redis-with-pba
  labels:
    app: redis-with-pba
spec:
  containers:
    - name: redis-with-pba
      image: pmem-redis:latest
      imagePullPolicy: IfNotPresent
      args: [&amp;quot;yes&amp;quot;]
      ports:
      - containerPort: 6379
      resources:
        limits:
          cpu: &amp;quot;1000m&amp;quot;
          memory: &amp;quot;16Gi&amp;quot;
        requests:
          cpu: &amp;quot;1000m&amp;quot;
          memory: &amp;quot;16Gi&amp;quot;
      volumeMounts:
      - mountPath: &amp;quot;/data&amp;quot;
        name: my-csi-volume
      - mountPath: &amp;quot;/ceph&amp;quot;
        name: ceph-csi-volume
  volumes:
  - name: ceph-csi-volume
    persistentVolumeClaim:
      claimName: rbd-pvc
  - name: my-csi-volume
    persistentVolumeClaim:
      claimName: pmem-csi-pvc-ext4
---
apiVersion: v1
kind: Service
metadata:
  name: redis-pba
  labels:
    app: redis-with-pba
spec:
  type: NodePort
  ports:
  - name: redis
    port: 6379
    nodePort: 31379
    targetPort: 6379
  selector:
    app: redis-with-pba
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;先执行aof的yaml文件，kubectl apply -f aof.yaml，然后再执行memtier_benchmark&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;memtier_benchmark -s 172.30.35.9  -p 30379 -R -d 1024 --key-maximum=1000000 -n 10000  --ratio=1:9 | grep Totals&amp;gt;&amp;gt;aof_1024
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 100%,  37 secs]  0 threads:     2000000 ops,   53289 (avg:   53544) ops/sec, 7.23MB/sec (avg: 7.29MB/sec),  3.75 (avg:  3.73) msec latency



[root@neonsan-10 scripts]# cat aof_1024
Totals      53508.29        26.75     48130.71         3.73350         3.27900         8.31900        18.43100      7456.87
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;先执行pba的yaml文件，kubectl apply -f pba.yaml，然后再执行memtier_benchmark,注意yaml文件里面同时挂载不同存储。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;memtier_benchmark -s 172.30.35.9  -p 31379 -R -d 1024 --key-maximum=1000000 -n 10000  --ratio=1:9 | grep Totals&amp;gt;&amp;gt;pba_1024
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 100%,  24 secs]  0 threads:     2000000 ops,   81619 (avg:   81294) ops/sec, 11.07MB/sec (avg: 11.10MB/sec),  2.45 (avg:  2.46) msec latency


[root@neonsan-10 scripts]# cat pba_1024
Totals          0.00         0.00         0.00            -nan         0.00000         0.00000         0.00000         0.00
Totals      82856.35        82.86     74487.86         2.45929         2.38300         4.79900         6.30300     11588.37
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;通过速度和延迟性比较两种存储的性能。&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>CkA考试经验</title>
      <link>https://Forest-L.github.io/post/cka-test-experience/</link>
      <pubDate>Sat, 11 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/cka-test-experience/</guid>
      <description>&lt;h1 id=&#34;cka考试经验&#34;&gt;CKA考试经验&lt;/h1&gt;
&lt;p&gt;CKA: Kubernetes管理员认证（CKA）旨在确保认证持有者具备履行Kubernetes管理员职责的技能，知识和能力。如果企业想要申请 KCSP ，条件之一是：至少需要三名员工拥有CKA认证。
&lt;img src=&#34;https://Forest-L.github.io/img/cka.jpg&#34; alt=&#34;CKA图片.jpg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-考试报名&#34;&gt;1. 考试报名&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;%5Bhttps://www.cncf.io/certification/cka/%5D&#34;&gt;CKA报名地址&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;注意事项：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;1、报名成功之后，可在12个月之内进行考试，考试不过有一次补考机会。&lt;/p&gt;
&lt;p&gt;2、CKA：74分或以上可以获得证书。&lt;/p&gt;
&lt;p&gt;3、每年Cyber Monday（网络星期一，也就是黑五后的第一个星期一）有优惠或者某些辅导架构有优惠券。&lt;/p&gt;
&lt;h2 id=&#34;2-备考&#34;&gt;2. 备考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;考试时你可以打开两个浏览器Tab，一个是考试窗口，一个用来查阅官方文档（仅允许访问https://kubernetes.io/docs/、https://github.com/kubernetes/ 和https://kubernetes.io/blog/ ）&lt;/li&gt;
&lt;li&gt;查询文档的浏览器Tab可以弄成标签。&lt;/li&gt;
&lt;li&gt;考试前一周看下官网k8s版本，然后部署一个同样版本的K8s练习。&lt;/li&gt;
&lt;li&gt;可以参考考试大纲复习。&lt;/li&gt;
&lt;li&gt;怎么复习，可以在自己搭建的环境下，操作指令，生成对应的。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3-考前检查及考试环境&#34;&gt;3. 考前检查及考试环境&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;考试形式: 在线监控，需要共享桌面和摄像头&lt;/li&gt;
&lt;li&gt;考试环境: 在一个密闭空间，例如书房、卧室、会议室等，电脑屏幕不能对着窗户，房间里除了考生不能存在第二个人，考试的桌面不能放其它东西，水杯也不行&lt;/li&gt;
&lt;li&gt;考试时间及题目: CKA-3小时-24道题&lt;/li&gt;
&lt;li&gt;选择考试时间: 报名成功之后可以在12个月之内进行考试，考试之前需要选择考试时间，选择考试时间的时候记得先选择北京时区，默认是0时区时间。&lt;/li&gt;
&lt;li&gt;电脑要求: 可以在这里WebDelivery Compatibility Check检测自己的电脑环境和网络速度等&lt;/li&gt;
&lt;li&gt;选择的是Linux-Foundation&amp;mdash;&amp;gt;CKA-English&lt;/li&gt;
&lt;li&gt;考试前考官检查:
考试可以提前15分钟进入考试界面
考官会以发消息的方式和你交流（没有语音交流）
看不懂考官发的英文怎么办：可以在chrome浏览器右键翻译
考官会让你共享摄像头，共享桌面 考官会让你出示能确认你身份ID的证件，我当时用的是罗技C310摄像头，无法对焦，护照看上去模糊到不行，后来考官又叫我给护照打光还是不行，后面又叫我打开我的手机，用手机相机当作放大镜用，这样才能看清楚。（我考CKAD的时候，我护照还没举稳，考官就说可以了，应该是考过CKA，他们系统里面已经有我的信息了，就随便瞄了一眼而已）
考官会让你用摄像头环视房间一周，确认你的考试环境（当时我房间门开了一个小缝也要求我去把门关好，还是比较严格）
考官会让你用摄像头看你的整个桌面和桌子底下
考官会让你打开任务管理器，点击左下角简略信息，是否已关闭了其它后台服务。
考官会让你再次点一下桌面共享，然后你叫你点击取消，然后就开始进入考试了&lt;/li&gt;
&lt;li&gt;考试的界面:
左边是题目
右边是终端
终端上面是共享摄像头、共享屏幕、考试信息等按钮（可以唤出记事本）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-考试心得&#34;&gt;4. 考试心得&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;有Notepad记事本，可以记录下自己环境信息，哪道题还没做，题目中的信息等。&lt;/li&gt;
&lt;li&gt;一定要记得用鼠标，拷贝和粘贴特别方便。&lt;/li&gt;
&lt;li&gt;尽量在官网中拷贝yaml文件到答题环境中。&lt;/li&gt;
&lt;li&gt;很多指令记得不清楚，请使用-h，比如etcdctl,node不能调度等。&lt;/li&gt;
&lt;li&gt;特别重要，根据个人考试，然后在浏览器中收藏的记录为：
&lt;img src=&#34;https://Forest-L.github.io/img/content.png&#34; alt=&#34;CKA考试相关内容.png&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>netapp存储在kubesphere上的实践</title>
      <link>https://Forest-L.github.io/post/netapp-stored-on-kubesphere-practice/</link>
      <pubDate>Fri, 13 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/netapp-stored-on-kubesphere-practice/</guid>
      <description>&lt;p&gt;&lt;strong&gt;NetApp&lt;/strong&gt;是向目前的数据密集型企业提供统一存储解决方案的居世界最前列的公司，其 Data ONTAP是全球首屈一指的存储操作系统。NetApp 的存储解决方案涵盖了专业化的硬件、软件和服务，为开放网络环境提供了无缝的存储管理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ontap&lt;/strong&gt;数据管理软件支持高速闪存、低成本旋转介质和基于云的对象存储等存储配置，为通过块或文件访问协议读写数据的应用程序提供统一存储。
&lt;strong&gt;Trident&lt;/strong&gt;是一个由NetApp维护的完全支持的开源项目。以帮助您满足容器化应用程序的复杂&lt;strong&gt;持久性&lt;/strong&gt;需求。
&lt;strong&gt;KubeSphere&lt;/strong&gt; 是一款开源项目，在目前主流容器调度平台 Kubernetes 之上构建的企业级分布式多租户&lt;strong&gt;容器管理平台&lt;/strong&gt;，提供简单易用的操作界面以及向导式操作方式，在降低用户使用容器调度平台学习成本的同时，极大降低开发、测试、运维的日常工作的复杂度。&lt;/p&gt;
&lt;h3 id=&#34;1整体方案&#34;&gt;1、整体方案&lt;/h3&gt;
&lt;p&gt;在VMware Workstation环境下安装ONTAP;ONTAP系统上创建SVM(Storage Virtual Machine)且对接nfs协议；在已有k8s环境下部署Trident,Trident将使用ONTAP系统上提供的信息（svm、managementLIF和dataLIF）作为后端来提供卷；在已创建的k8s和StorageClass卷下部署kubesphere。&lt;/p&gt;
&lt;h3 id=&#34;2版本信息&#34;&gt;2、版本信息&lt;/h3&gt;
&lt;p&gt;Ontap: 9.5
Trident: v19.07
k8s: 1.15
kubesphere: 2.0.2&lt;/p&gt;
&lt;h3 id=&#34;3步骤&#34;&gt;3、步骤&lt;/h3&gt;
&lt;p&gt;主要描述ontap搭建及配置、Trident搭建和配置和kubesphere搭建及配置等方面。参考&lt;a href=&#34;https://kubesphereio.com/post/netapp-works-with-k8s-in-kubesphere/&#34;&gt;ontap搭建&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;31-ontap搭建及配置&#34;&gt;3.1 ontap搭建及配置&lt;/h4&gt;
&lt;p&gt;在VMware Workstation上Simulate_ONTAP_9.5P6_Installation_and_Setup_Guide运行，ontap启动之后，按下面操作配置，其中以cluster base license、feature licenses for the non-ESX build配置证书、e0c、ip address：192.168.*.20、netmask:255.255.255.0、集群名:cluster1、密码等信息。
https://IP address,以上设置的iP地址，用户名和密码：
&lt;img src=&#34;https://Forest-L.github.io/img/netapp.png&#34; alt=&#34;netapp&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;32-trident搭建及配置&#34;&gt;3.2 Trident搭建及配置&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;下载安装包trident-installer-19.07.0.tar.gz，解压进入trident-installer目录，执行trident安装指令:
&lt;code&gt;./tridentctl install -n trident&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;结合ontap的提供的参数创建第一个后端vi backend.json。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;{
    &amp;quot;version&amp;quot;: 1,
    &amp;quot;storageDriverName&amp;quot;: &amp;quot;ontap-nas&amp;quot;,
    &amp;quot;backendName&amp;quot;: &amp;quot;customBackendName&amp;quot;,
    &amp;quot;managementLIF&amp;quot;: &amp;quot;10.0.0.1&amp;quot;,
    &amp;quot;dataLIF&amp;quot;: &amp;quot;10.0.0.2&amp;quot;,
    &amp;quot;svm&amp;quot;: &amp;quot;trident_svm&amp;quot;,
    &amp;quot;username&amp;quot;: &amp;quot;cluster-admin&amp;quot;,
    &amp;quot;password&amp;quot;: &amp;quot;password&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;生成后端卷&lt;code&gt;./tridentctl -n trident create backend -f backend.json&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;创建StorageClass,vi storage-class-ontapnas.yaml&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ontapnasudp
provisioner: netapp.io/trident
mountOptions: [&amp;quot;rw&amp;quot;, &amp;quot;nfsvers=3&amp;quot;, &amp;quot;proto=udp&amp;quot;]
parameters:
  backendType: &amp;quot;ontap-nas&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建StorageClass指令&lt;code&gt;kubectl create -f storage-class-ontapnas.yaml&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;33-kubesphere的安装及配置&#34;&gt;3.3 kubesphere的安装及配置&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;在 Kubernetes 集群中创建名为 kubesphere-system 和 kubesphere-monitoring-system 的 namespace。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;$ cat &amp;lt;&amp;lt;EOF | kubectl create -f -
---
apiVersion: v1
kind: Namespace
metadata:
    name: kubesphere-system
---
apiVersion: v1
kind: Namespace
metadata:
    name: kubesphere-monitoring-system
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;创建 Kubernetes 集群 CA 证书的 Secret。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl -n kubesphere-system create secret generic kubesphere-ca  \
--from-file=ca.crt=/etc/kubernetes/pki/ca.crt  \
--from-file=ca.key=/etc/kubernetes/pki/ca.key
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;若 etcd 已经配置过证书，则参考如下创建（以下命令适用于 Kubeadm 创建的 Kubernetes 集群环境）：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl -n kubesphere-monitoring-system create secret generic kube-etcd-client-certs  \
--from-file=etcd-client-ca.crt=/etc/kubernetes/pki/etcd/ca.crt  \
--from-file=etcd-client.crt=/etc/kubernetes/pki/etcd/healthcheck-client.crt  \
--from-file=etcd-client.key=/etc/kubernetes/pki/etcd/healthcheck-client.key
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;修改kubesphere.yaml中存储的设置参数和对应的参数即可
&lt;code&gt;kubectl apply -f kubesphere.yaml&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;访问 KubeSphere UI 界面。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://Forest-L.github.io/img/kubesphere.png&#34; alt=&#34;kubesphere&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;参考文档&#34;&gt;参考文档&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://docs.netapp.com/ontap-9/index.jsp?lang=zh_CN&#34;&gt;http://docs.netapp.com/ontap-9/index.jsp?lang=zh_CN&lt;/a&gt;
&lt;a href=&#34;https://netapp-trident.readthedocs.io/en/stable-v19.07/introduction.html&#34;&gt;https://netapp-trident.readthedocs.io/en/stable-v19.07/introduction.html&lt;/a&gt;
&lt;a href=&#34;https://github.com/kubesphere/ks-installer/blob/master/README_zh.md&#34;&gt;https://github.com/kubesphere/ks-installer/blob/master/README_zh.md&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kubesphere基于Velero做集群的迁移</title>
      <link>https://Forest-L.github.io/post/kubesphere-does-cluster-migration-based-on-velero/</link>
      <pubDate>Wed, 13 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/kubesphere-does-cluster-migration-based-on-velero/</guid>
      <description>&lt;p&gt;#Kubesphere基于Velero做集群的迁移
使用Velero 快速完成云原生应用迁移至备份集群中。&lt;/p&gt;
&lt;h3 id=&#34;环境信息&#34;&gt;环境信息&lt;/h3&gt;
&lt;p&gt;集群A（生产）：
master：192.168.11.6、192.168.11.13、192.168.11.16
lb：192.168.11.252
node：192.168.11.22
nfs：192.168.11.14
集群B（容灾）：
master：192.168.11.8、192.168.11.10、192.168.11.17
lb：192.168.11.253
node：192.168.11.18
nfs：192.168.11.14&lt;/p&gt;
&lt;h3 id=&#34;velero安装部署&#34;&gt;Velero安装部署&lt;/h3&gt;
&lt;p&gt;集群A和集群B都需要安装velero，安装过程参考官方文档&lt;a href=&#34;https://velero.io/docs/v1.2.0/contributions/minio/&#34;&gt;velero安装&lt;/a&gt;,大致流程为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、安装velero客户端安装包，A和B集群都需要。
1.1、wget https://github.com/vmware-tanzu/velero/releases/download/v1.0.0/velero-v1.0.0-linux-amd64.tar.gz
1.2、解压安装包，且将velero拷贝至/usr/local/bin目录下。
2、安装velero服务端，A和B集群都需要。
2.1、本地创建密钥文件，vi credentials-velero
[default]
aws_access_key_id = minio
aws_secret_access_key = minio123
2.2、集群B环境，运下载和运行00-minio-deployment.yaml文件，且需要将其中的ClusterIP改成NodePort，添加nodePort: 31860，集群A环境不需要执行这步。
kubectl apply -f examples/minio/00-minio-deployment.yaml
2.3、集群B环境，启动服务端,需要在密钥文件同级目录下执行：
velero install \
    --provider aws \
    --bucket velero \
    --secret-file ./credentials-velero \
    --use-volume-snapshots=false \
    --backup-location-config region=minio,s3ForcePathStyle=&amp;quot;true&amp;quot;,s3Url=http://minio.velero.svc:9000 \
	--plugins velero/velero-plugin-for-aws:v1.0.0
	
2.4、集群A环境，启动服务端，注意：需要在集群A中获取velero的外部curl：
2.4.1、集群A中，kubectl get svc -n velero,获取9000映射的端口，如：9000:31860，根据情况而定
2.4.2、启动指令：
velero install \
    --provider aws \
    --bucket velero \
    --secret-file ./credentials-velero \
    --use-volume-snapshots=false \
    --backup-location-config region=minio,s3ForcePathStyle=&amp;quot;true&amp;quot;,s3Url=http://192.168.11.8:31860 \
	--plugins velero/velero-plugin-for-aws:v1.0.0

&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;集群a数据的备份及集群b恢复&#34;&gt;集群A数据的备份及集群B恢复&lt;/h3&gt;
&lt;p&gt;具体备份指令，定时备份，参考官方文档&lt;a href=&#34;https://velero.io/docs/v1.2.0/contributions/minio/&#34;&gt;备份指令&lt;/a&gt;
在集群A中模拟了带有持久化的有状态和无状态的应用，备份维度以namespace为基准，为test,将pv的模式改成retain形式。
备份指令为：velero backup create test-backup &amp;ndash;include-namespaces test
集群A所有的机器关机且在机器B恢复验证：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@master1 velero-v1.2.0-linux-amd64]# velero restore create --from-backup test-backup
Restore request &amp;quot;test-backup-20191121141336&amp;quot; submitted successfully.
Run `velero restore describe test-backup-20191121141336` or `velero restore logs test-backup-20191121141336` for more details.
[root@master1 velero-v1.2.0-linux-amd64]# kubectl get ns
NAME                           STATUS   AGE
default                        Active   43h
demo                           Active   42h
kube-node-lease                Active   43h
kube-public                    Active   43h
kube-system                    Active   43h
kubesphere-controls-system     Active   43h
kubesphere-monitoring-system   Active   43h
kubesphere-system              Active   43h
openpitrix-system              Active   23h
test                           Active   12s
velero                         Active   24m
[root@master1 velero-v1.2.0-linux-amd64]# kubectl get pod -n test
NAME                             READY   STATUS    RESTARTS   AGE
mysql-v1-0                       1/1     Running   0          22s
tomcattest-v1-554c8875cd-26fz4   1/1     Running   0          22s
tomcattest-v1-554c8875cd-cmm2z   1/1     Running   0          22s
tomcattest-v1-554c8875cd-dc7mr   1/1     Running   0          22s
tomcattest-v1-554c8875cd-fcgn4   1/1     Running   0          22s
tomcattest-v1-554c8875cd-wqb4t   1/1     Running   0          22s
wordpress-v1-65d58448f8-g5bh8    1/1     Running   0          22s
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>k8s1.15安装</title>
      <link>https://Forest-L.github.io/post/k8s1-15-install/</link>
      <pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s1-15-install/</guid>
      <description>&lt;h1 id=&#34;使用kubeadm安装k8s-115版本&#34;&gt;使用kubeadm安装k8s 1.15版本&lt;/h1&gt;
&lt;p&gt;k8s 1.15版本中，kubeadm对HA集群的配置已经达到了beta可用，这一版本更新主要是针对稳定性的持续改善和可扩展性。其中用到的镜像和rpm包在百度云上，链接如下。
&lt;a href=&#34;https://pan.baidu.com/s/1LoKvv86Fs5ilZ-TYQdN35A&#34;&gt;https://pan.baidu.com/s/1LoKvv86Fs5ilZ-TYQdN35A&lt;/a&gt;
cos3&lt;/p&gt;
&lt;h2 id=&#34;1准备&#34;&gt;1.准备&lt;/h2&gt;
&lt;h3 id=&#34;11系统准备&#34;&gt;1.1系统准备&lt;/h3&gt;
&lt;p&gt;需要将主机ip和主机名放在每台机器的&lt;code&gt;vi /etc/hosts&lt;/code&gt;下
&lt;code&gt;192.168.11.21 i-fahx5c7k&lt;/code&gt;
&lt;code&gt;192.168.11.22 i-ouaaujhz&lt;/code&gt;
如果各个主机启用了防火墙，需要开启各个组件所需要的端口，可以查看https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
这里各个节点禁用防火墙
&lt;code&gt;systemctl stop firewalld&lt;/code&gt;
&lt;code&gt;systemctl disable firewalld&lt;/code&gt;
禁用selinux
&lt;code&gt;setenforce 0&lt;/code&gt;
vi /etc/selinux/config
SELINUX=disabled
创建vi /etc/sysctl.d/k8s.conf文件，添加如下内容：
&lt;code&gt;net.bridge.bridge-nf-call-ip6tables = 1&lt;/code&gt;
&lt;code&gt;net.bridge.bridge-nf-call-iptables = 1&lt;/code&gt;
&lt;code&gt;net.ipv4.ip_forward = 1&lt;/code&gt;
执行命令使修改生效
&lt;code&gt;modprobe br_netfilter&lt;/code&gt;
&lt;code&gt;sysctl -p /etc/sysctl.d/k8s.conf&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;12-kube-proxy开启ipvs的前置条件&#34;&gt;1.2 kube-proxy开启ipvs的前置条件&lt;/h3&gt;
&lt;p&gt;在所有的节点上执行如下脚本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt; /etc/sysconfig/modules/ipvs.modules &amp;lt;&amp;lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; bash /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4

各个节点需要安装 ipset ipvsadm
yum install ipset ipvsadm -y
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;13-docker安装&#34;&gt;1.3 docker安装&lt;/h3&gt;
&lt;p&gt;安装docker的yum源,国内寻找清华源
&lt;code&gt;yum install wget -y&lt;/code&gt;
&lt;code&gt;yum install -y yum-utils device-mapper-persistent-data lvm2&lt;/code&gt;
&lt;code&gt;wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo&lt;/code&gt;
&lt;code&gt;sed -i &#39;s+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+&#39; /etc/yum.repos.d/docker-ce.repo&lt;/code&gt;
&lt;code&gt;yum makecache fast&lt;/code&gt;
&lt;code&gt;yum install docker-ce -y&lt;/code&gt;
重启docker：&lt;code&gt;systemctl enable docker;systemctl restart docker&lt;/code&gt;
修改docker cgroup driver为systemd
创建或修改&lt;code&gt;vi /etc/docker/daemon.json&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启docker:&lt;code&gt;systemctl restart docker&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-使用kubeadm部署kubernetes&#34;&gt;2. 使用kubeadm部署kubernetes&lt;/h2&gt;
&lt;h3 id=&#34;21-安装kubeadm和kubelet&#34;&gt;2.1 安装kubeadm和kubelet&lt;/h3&gt;
&lt;p&gt;下面在各节点安装kubeadm和kubelet和kubectl，请在百度云上面下载rpm包，在rmp目录下执行如下指令：
&lt;code&gt;yum install -y cri-tools-1.13.0-0.x86_64.rpm kubernetes-cni-0.7.5-0.x86_64.rpm kubelet-1.15.1-0.x86_64.rpm kubectl-1.15.1-0.x86_64.rpm kubeadm-1.15.1-0.x86_64.rpm &lt;/code&gt;
k8s 1.8开始要求关闭系统的swap,不关闭，kubelet将无法启动，执行指令方法：
&lt;code&gt;swapoff -a&lt;/code&gt;
修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载，使用free -m确认swap已经关闭。 swappiness参数调整，修改&lt;code&gt;vi /etc/sysctl.d/k8s.conf&lt;/code&gt;添加下面一行：
&lt;code&gt;vm.swappiness=0&lt;/code&gt;
执行&lt;code&gt;sysctl -p /etc/sysctl.d/k8s.conf&lt;/code&gt;使修改生效。
因为这里本次测试主机上还运行其他服务，关闭swap可能会对其他服务产生影响，所以这里修改kubelet的配置去掉这个限制。
修改&lt;code&gt;vi /etc/sysconfig/kubelet&lt;/code&gt;，加入：&lt;code&gt;KUBELET_EXTRA_ARGS=--fail-swap-on=false&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;22-使用kubeadm-init初始化集群&#34;&gt;2.2 使用kubeadm init初始化集群&lt;/h3&gt;
&lt;p&gt;在各节点开机启动kubelet服务：&lt;code&gt;systemctl enable kubelet&lt;/code&gt;
使用&lt;code&gt;kubeadm config print init-defaults&lt;/code&gt;可以打印集群初始化默认的使用的配置:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: node1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: v1.14.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从默认的配置中可以看到，可以使用imageRepository定制在集群初始化时拉取k8s所需镜像的地址。advertiseAddress的ip替换本机，基于默认配置定制出本次使用kubeadm初始化集群所需的配置文件且在11.21机器上&lt;code&gt;vi kubeadm.yaml&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.11.21
  bindPort: 6443
nodeRegistration:
  taints:
  - effect: PreferNoSchedule
    key: node-role.kubernetes.io/master
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.15.0
networking:
  podSubnet: 10.244.0.0/16
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;font color=#DC143C &gt;说 明 : &lt;/font&gt;
使用kubeadm默认配置初始化的集群，会在master节点打上node-role.kubernetes.io/master:NoSchedule的污点，阻止master节点接受调度运行工作负载。这里测试环境只有两个节点，所以将这个taint修改为node-role.kubernetes.io/master:PreferNoSchedule。&lt;/p&gt;
&lt;p&gt;在开始初始化集群之前，需要从百度云上面下载tar包镜像下来，tar通过scp分别传到各个节点执行docker load -i解压，
镜像列表:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;k8s.gcr.io/kube-proxy                v1.15.0             d235b23c3570        5 weeks ago         82.4MB
k8s.gcr.io/kube-apiserver            v1.15.0             201c7a840312        5 weeks ago         207MB
k8s.gcr.io/kube-scheduler            v1.15.0             2d3813851e87        5 weeks ago         81.1MB
k8s.gcr.io/kube-controller-manager   v1.15.0             8328bb49b652        5 weeks ago         159MB
gcr.io/kubernetes-helm/tiller        v2.14.1             ac22eb1f780e        7 weeks ago         94.2MB
quay.io/coreos/flannel               v0.11.0-amd64       ff281650a721        6 months ago        52.6MB
k8s.gcr.io/coredns                   1.3.1               eb516548c180        6 months ago        40.3MB
k8s.gcr.io/etcd                      3.3.10              2c4adeb21b4f        8 months ago        258MB
k8s.gcr.io/pause                     3.1                 da86e6ba6ca1        19 months ago       742kB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;接下来使用kubeadm初始化集群，选择node1作为Master Node，在node1上执行下面的命令：&lt;code&gt;kubeadm init --config kubeadm.yaml --ignore-preflight-errors=Swap&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubeadm join 192.168.11.21:6443 --token k6s0kn.nyao1ulwwmlm2org \ --discovery-token-ca-cert-hash sha256:b721d25356668f921c90bf5f554836cb346b827a62b78797d0bcbcba24214725 &lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;其中关键步骤：
* [kubelet-start] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml”
* [certs]生成相关的各种证书
* [kubeconfig]生成相关的kubeconfig文件
* [control-plane]使用/etc/kubernetes/manifests目录中的yaml文件创建apiserver、controller-manager、scheduler的静态pod
* [bootstraptoken]生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到
* 下面的命令是配置常规用户如何使用kubectl访问集群：
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
* 最后给出了将节点加入集群的命令kubeadm join 192.168.99.11:6443 –token 4qcl2f.gtl3h8e5kjltuo0r \ –discovery-token-ca-cert-hash sha256:7ed5404175cc0bf18dbfe53f19d4a35b1e3d40c19b10924275868ebf2a3bbe6e
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;需要在11.21机器上执行：
&lt;code&gt;mkdir -p $HOME/.kube&lt;/code&gt;
&lt;code&gt;sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config&lt;/code&gt;
&lt;code&gt;sudo chown $(id -u):$(id -g) $HOME/.kube/config&lt;/code&gt;
查看集群状态，确认组件都处于healthy状态：
&lt;code&gt;kubectl get cs&lt;/code&gt;
集群初始化如果遇到问题，可以使用下面的命令进行清理：
&lt;code&gt;kubeadm reset&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;23-安装pod-network&#34;&gt;2.3 安装Pod Network&lt;/h3&gt;
&lt;p&gt;接下来安装flannel network add-on：
&lt;code&gt;mkdir -p ~/k8s/&lt;/code&gt;
&lt;code&gt;cd ~/k8s&lt;/code&gt;
&lt;code&gt;curl -O https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml&lt;/code&gt;
&lt;code&gt;kubectl apply -f  kube-flannel.yml&lt;/code&gt;
这里注意kube-flannel.yml这个文件里的flannel的镜像是0.11.0，quay.io/coreos/flannel:v0.11.0-amd64
如果Node有多个网卡的话，参考https://github.com/kubernetes/kubernetes/issues/39701，
目前需要在kube-flannel.yml中使用–iface参数指定集群主机内网网卡的名称，否则可能会出现dns无法解析。需要将kube-flannel.yml下载到本地，flanneld启动参数加上–iface=&lt;iface-name&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        - --iface=eth1
......
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;使用&lt;code&gt;kubectl get pod –-all-namespaces -o wide&lt;/code&gt;确保所有的Pod都处于Running状态。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kube-flannel.yml
[root@i-fahx5c7k k8s]# kubectl get pod --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP              NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5c98db65d4-nbb4w             1/1     Running   0          6m29s   10.244.0.2      i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-5c98db65d4-wtm58             1/1     Running   0          6m29s   10.244.0.3      i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   etcd-i-fahx5c7k                      1/1     Running   0          5m26s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-apiserver-i-fahx5c7k            1/1     Running   0          5m37s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-controller-manager-i-fahx5c7k   1/1     Running   0          5m45s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-flannel-ds-amd64-bqswg          1/1     Running   0          58s     192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-zhzxj                     1/1     Running   0          6m29s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-scheduler-i-fahx5c7k            1/1     Running   0          5m20s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;24-测试集群dns是否可用&#34;&gt;2.4 测试集群DNS是否可用&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl run curl --image=radial/busyboxplus:curl -it&lt;/code&gt;
进入后执行&lt;code&gt;nslookup kubernetes.default&lt;/code&gt;确认解析正常:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[ root@curl-6bf6db5c4f-f8jjn:/ ]$ nslookup kubernetes.default
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;25-向kubernetes集群中添加node节点&#34;&gt;2.5 向Kubernetes集群中添加Node节点&lt;/h3&gt;
&lt;p&gt;在master上查看添加节点指令：&lt;code&gt;kubeadm token create --print-join-command&lt;/code&gt;
下面将11.22这个主机添加到Kubernetes集群中，在11.22机器上执行:
&lt;code&gt;kubeadm join 192.168.11.21:6443 --token k6s0kn.nyao1ulwwmlm2org \ --discovery-token-ca-cert-hash sha256:b721d25356668f921c90bf5f554836cb346b827a62b78797d0bcbcba24214725&lt;/code&gt;
11.22加入集群很是顺利，下面在master节点上执行命令查看集群中的节点：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@i-fahx5c7k k8s]# kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
i-fahx5c7k   Ready    master   13m   v1.15.1
i-ouaaujhz   Ready    &amp;lt;none&amp;gt;   50s   v1.15.1
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;251-如何从集群中移除node&#34;&gt;2.5.1 如何从集群中移除Node&lt;/h4&gt;
&lt;p&gt;如果需要从集群中移除11.22这个Node执行下面的命令：
在master节点上执行：
&lt;code&gt;kubectl drain i-ouaaujhz --delete-local-data --force --ignore-daemonsets&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete node i-ouaaujhz&lt;/code&gt;
在11.22上执行：
&lt;code&gt;kubeadm reset&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;kube-proxy开启ipvs&#34;&gt;kube-proxy开启ipvs&lt;/h3&gt;
&lt;p&gt;修改ConfigMap的kube-system/kube-proxy中的config.conf，mode: “ipvs”
&lt;code&gt;kubectl edit cm kube-proxy -n kube-system&lt;/code&gt;
之后重启各个节点上的kube-proxy pod：
&lt;code&gt;kubectl get pod -n kube-system | grep kube-proxy | awk &#39;{system(&amp;quot;kubectl delete pod &amp;quot;$1&amp;quot; -n kube-system&amp;quot;)}&#39;&lt;/code&gt;
日志查看：&lt;code&gt;kubectl logs kube-proxy-62ntf  -n kube-system&lt;/code&gt;出现ipvs即开启。&lt;/p&gt;
&lt;h2 id=&#34;3kubernetes常用组件部署&#34;&gt;3.Kubernetes常用组件部署&lt;/h2&gt;
&lt;p&gt;使用Helm这个Kubernetes的包管理器，这里也将使用Helm安装Kubernetes的常用组件。&lt;/p&gt;
&lt;h3 id=&#34;31-helm的安装&#34;&gt;3.1 Helm的安装&lt;/h3&gt;
&lt;p&gt;Helm由客户端命helm令行工具和服务端tiller组成，Helm的安装十分简单。 下载helm命令行工具到master节点node1的/usr/local/bin下，这里下载的2.14.1版本：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;curl -O https://get.helm.sh/helm-v2.14.1-linux-amd64.tar.gz&lt;/code&gt;
&lt;code&gt;tar -zxvf helm-v2.14.1-linux-amd64.tar.gz&lt;/code&gt;
&lt;code&gt;cd linux-amd64/&lt;/code&gt;
&lt;code&gt;cp helm /usr/local/bin/&lt;/code&gt;
为了安装服务端tiller，还需要在这台机器上配置好kubectl工具和kubeconfig文件，确保kubectl工具可以在这台机器上访问apiserver且正常使用。 这里的11.21节点已经配置好了kubectl。
因为Kubernetes APIServer开启了RBAC访问控制，所以需要创建tiller使用的service account: tiller并分配合适的角色给它。这里简单起见直接分配cluster-admin这个集群内置的ClusterRole给它。创建&lt;code&gt;vi helm-rbac.yaml&lt;/code&gt;文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f helm-rbac.yaml&lt;/code&gt;
接下来使用helm部署tiller:
&lt;code&gt;helm init --service-account tiller --skip-refresh&lt;/code&gt;
tiller默认被部署在k8s集群中的kube-system这个namespace下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@i-fahx5c7k centosrepo]# kubectl get pod -n kube-system -l app=helm
NAME                             READY   STATUS    RESTARTS   AGE
tiller-deploy-7bf78cdbf7-46bv5   1/1     Running   0          22s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;helm version&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>K8s1.16在centos安装</title>
      <link>https://Forest-L.github.io/post/k8s1.16-installed-on-centos-system/</link>
      <pubDate>Sun, 22 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s1.16-installed-on-centos-system/</guid>
      <description>&lt;h1 id=&#34;centos系统k8s-116版本安装&#34;&gt;centos系统k8s-1.16版本安装&lt;/h1&gt;
&lt;p&gt;k8s1.16版本相对之前版本变化不小，亮点和升级参看&lt;a href=&#34;http://k8smeetup.com/article/N1lqGc0i8v&#34;&gt;v1.16说明&lt;/a&gt;。相关联的镜像和v1.16二进制包上传至百度云上，链接如下&lt;a href=&#34;https://pan.baidu.com/s/19khl0Hn5ZnZ8TvbO5HZVww&#34;&gt;k8s1.16介质，ftq5&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;1准备&#34;&gt;1.准备&lt;/h2&gt;
&lt;h3 id=&#34;11系统准备&#34;&gt;1.1系统准备&lt;/h3&gt;
&lt;p&gt;需要将主机ip和主机名放在每台机器的&lt;code&gt;vi /etc/hosts&lt;/code&gt;下&lt;/p&gt;
&lt;p&gt;&lt;code&gt;192.168.11.21 i-fahx5c7k&lt;/code&gt;
&lt;code&gt;192.168.11.22 i-ouaaujhz&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;如果各个主机启用了防火墙，需要开启各个组件所需要的端口，可以查看https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
这里各个节点禁用防火墙&lt;/p&gt;
&lt;p&gt;&lt;code&gt;systemctl stop firewalld&lt;/code&gt;
&lt;code&gt;systemctl disable firewalld&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;禁用selinux&lt;/p&gt;
&lt;p&gt;&lt;code&gt;setenforce 0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;vi /etc/selinux/config
SELINUX=disabled
创建vi /etc/sysctl.d/k8s.conf文件，添加如下内容：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;net.bridge.bridge-nf-call-ip6tables = 1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;net.bridge.bridge-nf-call-iptables = 1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;net.ipv4.ip_forward = 1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;执行命令使修改生效&lt;/p&gt;
&lt;p&gt;&lt;code&gt;modprobe br_netfilter&lt;/code&gt;
&lt;code&gt;sysctl -p /etc/sysctl.d/k8s.conf&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;12-kube-proxy开启ipvs的前置条件&#34;&gt;1.2 kube-proxy开启ipvs的前置条件&lt;/h3&gt;
&lt;p&gt;在所有的节点上执行如下脚本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt; /etc/sysconfig/modules/ipvs.modules &amp;lt;&amp;lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; bash /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4

各个节点需要安装 ipset ipvsadm
yum install ipset ipvsadm -y
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;13-docker安装&#34;&gt;1.3 docker安装&lt;/h3&gt;
&lt;p&gt;安装docker的yum源,国内寻找清华源&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget -y
yum install -y yum-utils device-mapper-persistent-data lvm2
wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo
sed -i &#39;s+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+&#39; /etc/yum.repos.d/docker-ce.repo
yum makecache fast
yum install docker-ce -y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启docker：systemctl enable docker;systemctl restart docker
修改docker cgroup driver为systemd
创建或修改&lt;code&gt;vi /etc/docker/daemon.json&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启docker:&lt;code&gt;systemctl restart docker&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-使用kubeadm部署kubernetes&#34;&gt;2. 使用kubeadm部署kubernetes&lt;/h2&gt;
&lt;h3 id=&#34;21-安装kubeadm和kubelet&#34;&gt;2.1 安装kubeadm和kubelet&lt;/h3&gt;
&lt;p&gt;下面在各节点安装kubeadm和kubelet和kubectl，请在百度云上面下载rpm包，在k8s116目录下执行如下指令：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;yum install -y cri-tools-1.13.0-0.x86_64.rpm cni-0.7.5-0.x86_64.rpm kubelet-1.16.0-0.x86_64.rpm kubectl-1.16.0-0.x86_64.rpm kubeadm-1.16.0-0.x86_64.rpm &lt;/code&gt;&lt;/p&gt;
&lt;p&gt;k8s 1.8开始要求关闭系统的swap,不关闭，kubelet将无法启动，执行指令方法：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;swapoff -a&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载，使用free -m确认swap已经关闭。 swappiness参数调整，修改&lt;code&gt;vi /etc/sysctl.d/k8s.conf&lt;/code&gt;添加下面一行：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;vm.swappiness=0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;执行&lt;code&gt;sysctl -p /etc/sysctl.d/k8s.conf&lt;/code&gt;使修改生效。
因为这里本次测试主机上还运行其他服务，关闭swap可能会对其他服务产生影响，所以这里修改kubelet的配置去掉这个限制。
修改&lt;code&gt;vi /etc/sysconfig/kubelet&lt;/code&gt;，加入：&lt;code&gt;KUBELET_EXTRA_ARGS=--fail-swap-on=false&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;22-使用kubeadm-init初始化集群&#34;&gt;2.2 使用kubeadm init初始化集群&lt;/h3&gt;
&lt;p&gt;在各节点开机启动kubelet服务：&lt;code&gt;systemctl enable kubelet&lt;/code&gt;
使用&lt;code&gt;kubeadm config print init-defaults&lt;/code&gt;可以打印集群初始化默认的使用的配置:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: node1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: v1.14.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从默认的配置中可以看到，可以使用imageRepository定制在集群初始化时拉取k8s所需镜像的地址。advertiseAddress的ip替换本机，基于默认配置定制出本次使用kubeadm初始化集群所需的配置文件且在11.21机器上&lt;code&gt;vi kubeadm.yaml&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.11.21
  bindPort: 6443
nodeRegistration:
  taints:
  - effect: PreferNoSchedule
    key: node-role.kubernetes.io/master
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.16.0
networking:
  podSubnet: 10.244.0.0/16
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;font color=#DC143C &gt;说 明 : &lt;/font&gt;
使用kubeadm默认配置初始化的集群，会在master节点打上node-role.kubernetes.io/master:NoSchedule的污点，阻止master节点接受调度运行工作负载。这里测试环境只有两个节点，所以将这个taint修改为node-role.kubernetes.io/master:PreferNoSchedule。&lt;/p&gt;
&lt;p&gt;在开始初始化集群之前，kubeadm config images pull查看需要哪些镜像,需要从百度云上面下载tar包镜像下来，tar通过scp分别传到各个节点执行docker load -i解压，
镜像列表:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;k8s.gcr.io/kube-apiserver            v1.16.0             b305571ca60a        42 hours ago        217MB
k8s.gcr.io/kube-proxy                v1.16.0             c21b0c7400f9        42 hours ago        86.1MB
k8s.gcr.io/kube-controller-manager   v1.16.0             06a629a7e51c        42 hours ago        163MB
k8s.gcr.io/kube-scheduler            v1.16.0             301ddc62b80b        42 hours ago        87.3MB
k8s.gcr.io/etcd                      3.3.15-0            b2756210eeab        2 weeks ago         247MB
k8s.gcr.io/coredns                   1.6.2               bf261d157914        5 weeks ago         44.1MB
gcr.io/kubernetes-helm/tiller        v2.14.1             ac22eb1f780e        3 months ago        94.2MB
quay.io/coreos/flannel               v0.11.0-amd64       ff281650a721        7 months ago        52.6MB
k8s.gcr.io/pause                     3.1                 da86e6ba6ca1        21 months ago       742kB
radial/busyboxplus                   curl                71fa7369f437        5 years ago         4.23MB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;接下来使用kubeadm初始化集群，选择node1作为Master Node，在node1上执行下面的命令：
&lt;code&gt;kubeadm init --config kubeadm.yaml --ignore-preflight-errors=Swap&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubeadm join 192.168.11.21:6443 --token k6s0kn.nyao1ulwwmlm2org \ --discovery-token-ca-cert-hash sha256:b721d25356668f921c90bf5f554836cb346b827a62b78797d0bcbcba24214725 &lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;其中关键步骤：
* [kubelet-start] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml”
* [certs]生成相关的各种证书
* [kubeconfig]生成相关的kubeconfig文件
* [control-plane]使用/etc/kubernetes/manifests目录中的yaml文件创建apiserver、controller-manager、scheduler的静态pod
* [bootstraptoken]生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到
* 下面的命令是配置常规用户如何使用kubectl访问集群：
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
* 最后给出了将节点加入集群的命令kubeadm join 192.168.99.11:6443 –token 4qcl2f.gtl3h8e5kjltuo0r \ –discovery-token-ca-cert-hash sha256:7ed5404175cc0bf18dbfe53f19d4a35b1e3d40c19b10924275868ebf2a3bbe6e
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;需要在11.21机器上执行：
&lt;code&gt;mkdir -p $HOME/.kube&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo chown $(id -u):$(id -g) $HOME/.kube/config&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;查看集群状态，确认组件都处于healthy状态：
&lt;code&gt;kubectl get cs&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;集群初始化如果遇到问题，可以使用下面的命令进行清理：
&lt;code&gt;kubeadm reset&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;23-安装pod-network&#34;&gt;2.3 安装Pod Network&lt;/h3&gt;
&lt;p&gt;接下来安装flannel network add-on：
&lt;code&gt;mkdir -p ~/k8s/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cd ~/k8s&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;curl -O https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl apply -f  kube-flannel.yml&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这里注意kube-flannel.yml这个文件里的flannel的镜像是0.11.0，quay.io/coreos/flannel:v0.11.0-amd64
注意需要在vi /var/lib/kubelet/kubeadm-flags.env文件配置中去掉&amp;ndash;network-plugin=cni,然后重启kubelet,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl restart kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果Node有多个网卡的话，参考https://github.com/kubernetes/kubernetes/issues/39701，
目前需要在kube-flannel.yml中使用–iface参数指定集群主机内网网卡的名称，否则可能会出现dns无法解析。需要将kube-flannel.yml下载到本地，flanneld启动参数加上–iface=&lt;iface-name&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        - --iface=eth1
......
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;使用&lt;code&gt;kubectl get pods –-all-namespaces -o wide&lt;/code&gt;确保所有的Pod都处于Running状态。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kube-flannel.yml
[root@i-fahx5c7k k8s]# kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP              NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5c98db65d4-nbb4w             1/1     Running   0          6m29s   10.244.0.2      i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-5c98db65d4-wtm58             1/1     Running   0          6m29s   10.244.0.3      i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   etcd-i-fahx5c7k                      1/1     Running   0          5m26s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-apiserver-i-fahx5c7k            1/1     Running   0          5m37s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-controller-manager-i-fahx5c7k   1/1     Running   0          5m45s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-flannel-ds-amd64-bqswg          1/1     Running   0          58s     192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-zhzxj                     1/1     Running   0          6m29s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-scheduler-i-fahx5c7k            1/1     Running   0          5m20s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;24-测试集群dns是否可用&#34;&gt;2.4 测试集群DNS是否可用&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl run curl --image=radial/busyboxplus:curl -it&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;进入后执行&lt;code&gt;nslookup kubernetes.default&lt;/code&gt;确认解析正常:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[ root@curl-6bf6db5c4f-f8jjn:/ ]$ nslookup kubernetes.default
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;25-向kubernetes集群中添加node节点&#34;&gt;2.5 向Kubernetes集群中添加Node节点&lt;/h3&gt;
&lt;p&gt;下面将11.22这个主机添加到Kubernetes集群中，在11.22机器上执行:
&lt;code&gt;kubeadm join 192.168.11.21:6443 --token k6s0kn.nyao1ulwwmlm2org \ --discovery-token-ca-cert-hash sha256:b721d25356668f921c90bf5f554836cb346b827a62b78797d0bcbcba24214725&lt;/code&gt;
11.22加入集群很是顺利，下面在master节点上执行命令查看集群中的节点：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@i-fahx5c7k k8s]# kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
i-fahx5c7k   Ready    master   13m   v1.15.1
i-ouaaujhz   Ready    &amp;lt;none&amp;gt;   50s   v1.15.1
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;251-如何从集群中移除node&#34;&gt;2.5.1 如何从集群中移除Node&lt;/h4&gt;
&lt;p&gt;如果需要从集群中移除11.22这个Node执行下面的命令：
在master节点上执行：
&lt;code&gt;kubectl drain i-ouaaujhz --delete-local-data --force --ignore-daemonsets&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete node i-ouaaujhz&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;在11.22上执行：
&lt;code&gt;kubeadm reset&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;kube-proxy开启ipvs&#34;&gt;kube-proxy开启ipvs&lt;/h3&gt;
&lt;p&gt;修改ConfigMap的kube-system/kube-proxy中的config.conf，mode: “ipvs”&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl edit cm kube-proxy -n kube-system&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;之后重启各个节点上的kube-proxy pod：
&lt;code&gt;kubectl get pod -n kube-system | grep kube-proxy | awk &#39;{system(&amp;quot;kubectl delete pod &amp;quot;$1&amp;quot; -n kube-system&amp;quot;)}&#39;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;日志查看：&lt;code&gt;kubectl logs kube-proxy-62ntf  -n kube-system&lt;/code&gt;出现ipvs即开启。&lt;/p&gt;
&lt;h2 id=&#34;3kubernetes常用组件部署&#34;&gt;3.Kubernetes常用组件部署&lt;/h2&gt;
&lt;p&gt;使用Helm这个Kubernetes的包管理器，这里也将使用Helm安装Kubernetes的常用组件。&lt;/p&gt;
&lt;h3 id=&#34;31-helm的安装&#34;&gt;3.1 Helm的安装&lt;/h3&gt;
&lt;p&gt;Helm由客户端命helm令行工具和服务端tiller组成，Helm的安装十分简单。 下载helm命令行工具到master节点node1的/usr/local/bin下，这里下载的2.14.1版本：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;curl -O https://get.helm.sh/helm-v2.14.1-linux-amd64.tar.gz&lt;/code&gt;
&lt;code&gt;tar -zxvf helm-v2.14.1-linux-amd64.tar.gz&lt;/code&gt;
&lt;code&gt;cd linux-amd64/&lt;/code&gt;
&lt;code&gt;cp helm /usr/local/bin/&lt;/code&gt;
为了安装服务端tiller，还需要在这台机器上配置好kubectl工具和kubeconfig文件，确保kubectl工具可以在这台机器上访问apiserver且正常使用。 这里的11.21节点已经配置好了kubectl。
&lt;code&gt;helm init --output yaml &amp;gt; tiller.yaml&lt;/code&gt;
更新 tiller.yaml 两处：apiVersion 版本;增加选择器&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
...
spec:
  replicas: 1
  strategy: {}
  selector:
    matchLabels:
      app: helm
      name: tiller
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建：&lt;code&gt;kubectl create -f tiller.yaml&lt;/code&gt;
因为Kubernetes APIServer开启了RBAC访问控制，所以需要创建tiller使用的service account: tiller并分配合适的角色给它。这里简单起见直接分配cluster-admin这个集群内置的ClusterRole给它。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create serviceaccount --namespace kube-system tiller
kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
kubectl patch deploy --namespace kube-system tiller-deploy -p &#39;{&amp;quot;spec&amp;quot;:{&amp;quot;template&amp;quot;:{&amp;quot;spec&amp;quot;:{&amp;quot;serviceAccount&amp;quot;:&amp;quot;tiller&amp;quot;}}}}&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;检查helm是否安装成功&lt;code&gt;helm list&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;安装k8s116脚本的安装&#34;&gt;安装k8s1.16脚本的安装&lt;/h3&gt;
&lt;p&gt;解压包，然后执行脚本install-k8s.sh
&lt;code&gt;tar -xzvf k8s116.tar.gz&lt;/code&gt;
&lt;code&gt;./install-k8s.sh&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;参考文档&#34;&gt;参考文档&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/zh/docs/&#34;&gt;kubeadm安装&lt;/a&gt;
&lt;a href=&#34;https://kubernetes.io/zh/docs/setup/independent/create-cluster-kubeadm/&#34;&gt;kubeadm创建集群&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>kubesphere2-1-HA环境，一个master或者两个master宕机恢复</title>
      <link>https://Forest-L.github.io/post/kubesphere2-1-ha-environment-one-master-or-two-masters-down-to-recover/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/kubesphere2-1-ha-environment-one-master-or-two-masters-down-to-recover/</guid>
      <description>&lt;h1 id=&#34;kubesphere21-ha环境一个master或者两个master宕机恢复&#34;&gt;kubesphere2.1-HA环境，一个master或者两个master宕机恢复&lt;/h1&gt;
&lt;p&gt;kubesphere2.1版本提供了master节点的高可用功能，为生产环境保驾护航。然而很多事情都有意外，当其中一个master或者两个master卡住了，或者重启都不能自动恢复的情况下，那么怎么恢复呢，以下分一个master宕机和两个master宕机的恢复方法。&lt;/p&gt;
&lt;h3 id=&#34;验证环境信息&#34;&gt;验证环境信息&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;os: centos7.5
master1: 192.168.11.6
master2: 192.168.11.8
master3: 192.168.11.13
node1: 192.168.11.14
lb: 192.168.11.253
nfs服务端: 192.168.11.14
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;一个master宕机的模拟及恢复方法&#34;&gt;一个master宕机的模拟及恢复方法&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;正常的环境：nodes都running，etcd服务都正常。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes
NAME      STATUS   ROLES    AGE   VERSION
master1   Ready    master   19m   v1.15.5
master2   Ready    master   16m   v1.15.5
master3   Ready    master   16m   v1.15.5
node1     Ready    worker   14m   v1.15.5

export ETCDCTL_API=3

etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
192.168.11.6:2379, b3487da7c562b17, 3.2.18, 3.8 MB, true, 5, 4434
192.168.11.8:2379, 3d04d74d80b7f07b, 3.2.18, 3.8 MB, false, 5, 4434
192.168.11.13:2379, 3ad323c042cc4c05, 3.2.18, 3.8 MB, false, 5, 4434
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;一个master宕机情况，把master2重置，看nodes和etcd情况。还需在界面创建一些带有存储的pod用例。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes   
NAME      STATUS     ROLES    AGE   VERSION
master1   Ready      master   57m   v1.15.5
master2   NotReady   master   55m   v1.15.5
master3   Ready      master   55m   v1.15.5
node1     Ready      worker   52m   v1.15.5

etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
Failed to get the status of endpoint 192.168.11.8:2379 (context deadline exceeded)
192.168.11.6:2379, b3487da7c562b17, 3.2.18, 11 MB, true, 5, 14953
192.168.11.13:2379, 3ad323c042cc4c05, 3.2.18, 11 MB, false, 5, 14972
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;恢复方法：修改脚本中hosts.ini文件，需要注意顺序
在[all]组里面，master2行整体放在master3下面，[kube-master]和[etcd]组中master2放在master3下面,然后在另一个终端机器上重新执行安装脚本。以下恢复情况，etcd正常，nodes也正常，业务数据存在且业务pod没有中断正常使用。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes
NAME      STATUS     ROLES    AGE   VERSION
master1   Ready      master   83m   v1.15.5
master2   Ready      master   80m   v1.15.5
master3   Ready      master   80m   v1.15.5
node1     Ready      worker   78m   v1.15.5

etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
192.168.11.6:2379, b3487da7c562b17, 3.2.18, 11 MB, true, 11, 20292
192.168.11.8:2379, 3d04d74d80b7f07b, 3.2.18, 11 MB, false, 11, 20297
192.168.11.13:2379, 3ad323c042cc4c05, 3.2.18, 11 MB, false, 11, 20298

docker ps | grep tomcat
6863620b07cf        882487b8be1d                                     &amp;quot;catalina.sh run&amp;quot;        17 minutes ago       Up 17 minutes                           k8s_tomcat-2jl160_tomcat-v1-6c7745494-k64w5_demo_5b406841-df9b-4d5c-b018-998c400a2c3e_1
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;两个master宕机的模拟及恢复方法&#34;&gt;两个master宕机的模拟及恢复方法&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;两个master宕机情况，把master2和master3重置，看nodes和etcd情况，nodes不正常，etcd两个不正常。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes
Unable to connect to the server: EOF
[root@master1 ~]# 
[root@master1 ~]# 
[root@master1 ~]# etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
Failed to get the status of endpoint 192.168.11.8:2379 (context deadline exceeded)
Failed to get the status of endpoint 192.168.11.13:2379 (context deadline exceeded)
192.168.11.6:2379, b3487da7c562b17, 3.2.18, 11 MB, false, 12, 27944
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;恢复方法：
1、同样需要在host.ini文件修改master的顺序，由于我们重置2和3，所有此处不用修改顺序；
2、在已解压的安装介质目录下，进入k8s/roles/kubernetes/preinstall/tasks/main.yml文件，用#注释如下内容，重跑安装脚本，作用说明：在重置的master2和master3机器上安装docker和etcd，但整个集群还需修复。。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;#- import_tasks: 0020-verify-settings.yml
#  when:
#    - not dns_late
#  tags:
#    - asserts
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3、在master2机器上，临时修复master2节点的etcd服务，执行如下指令：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;停止etcd：systemctl stop etcd
备份etcd数据：mv /var/lib/etcd /var/lib/etcd-bak
从master1的/var/backups/kube_etcd/备份目录下拷贝最近时间的snapshot.db至master2机器上
在master2先转为版本3指令令：export ETCDCTL_API=3
在master2恢复指令：etcdctl snapshot restore /root/snapshot.db    --endpoints=192.168.11.8:2379    --cert=/etc/ssl/etcd/ssl/node-master2.pem    --key=/etc/ssl/etcd/ssl/node-master2-key.pem    --cacert=/etc/ssl/etcd/ssl/ca.pem --data-dir=/var/lib/etcd
重启etcd: systemctl restart etcd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4、修改host.ini文件master2和master3顺序，把[all][kube-master][etcd]三个组的master3放在最后面，再次跑安装脚本。其中的host.ini文件为&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[all]
master1 ansible_connection=local  ip=192.168.11.6
master2  ansible_host=192.168.11.8  ip=192.168.11.8  ansible_ssh_pass=
master3  ansible_host=192.168.11.13  ip=192.168.11.13  ansible_ssh_pass=
node1    ansible_host=192.168.11.14  ip=192.168.11.14  ansible_ssh_pass=

[kube-master]
master1
master2
master3

[kube-node]
node1

[etcd]
master1
master2
master3

[k8s-cluster:children]
kube-node
kube-master 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;5、由第四步只是临时修复master2的etcd，并不完全修复，先全部修复作如下处理：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;第四步执行过程中，z在“wait for etcd up”会报错，先ctrl +c 终止脚本；
在master2机器上，停止etcd服务：systemctl stop etcd
在master2机器上，删除etcd数据：rm -rf /var/lib/etcd
再次修改host.ini文件，master2和master3顺序，把[all][kube-master][etcd]三个组的master2放在最后面，再次跑安装脚本即可。
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;验证结果&#34;&gt;验证结果&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;集群中的nodes、etcd和带有存储数据的业务pod情况：&lt;/li&gt;
&lt;li&gt;集群中的nodes、etcd和带有存储数据的业务pod情况,nodes恢复正常，etcd服务也回复正常，带有存储的pod一直运行，集群恢复成功：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[root@master1 ~]# kubectl get nodes
NAME      STATUS   ROLES    AGE     VERSION
master1   Ready    master   4h56m   v1.15.5
master2   Ready    master   4h53m   v1.15.5
master3   Ready    master   4h53m   v1.15.5
node1     Ready    worker   4h51m   v1.15.5
[root@master1 ~]# 
[root@master1 ~]# 
[root@master1 ~]# 
[root@master1 ~]# etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
192.168.11.6:2379, b3487da7c562b17, 3.2.18, 12 MB, false, 1372, 32116
192.168.11.8:2379, 3d04d74d80b7f07b, 3.2.18, 12 MB, true, 1372, 32116
192.168.11.13:2379, 3ad323c042cc4c05, 3.2.18, 12 MB, false, 1372, 32116

docker ps | grep tomcat
6863620b07cf        882487b8be1d                                     &amp;quot;catalina.sh run&amp;quot;        4 hours ago         Up 4 hours                              k8s_tomcat-2jl160_tomcat-v1-6c7745494-k64w5_demo_5b406841-df9b-4d5c-b018-998c400a2c3e_1
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>kubesphere2-1-HA环境，某台master或者master的etcd宕机，新加机器恢复方法</title>
      <link>https://Forest-L.github.io/post/kubesphere2-1-ha-environment-a-master-or-master-etcd-down-a-new-machine-recovery-method/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/kubesphere2-1-ha-environment-a-master-or-master-etcd-down-a-new-machine-recovery-method/</guid>
      <description>&lt;p&gt;kubesphere2.1版本提供了master节点的高可用功能，为生产环境保驾护航。然而在生产环境中，为了业务更正常运行，当以下两种情形发生时，告诉大家怎么恢复。第一种情形是：其中某台master机器的etcd服务不能正常提供服务，而需要在另外一台机器上部署一个etcd服务加入到现etcd集群中；第二种情形是：其中某台master宕机，需要在另外一台机器上部署master，并加入到现master集群中。&lt;/p&gt;
&lt;h3 id=&#34;环境信息&#34;&gt;环境信息&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;os: centos7.5
master1: 192.168.11.6
master2: 192.168.11.16
master3: 192.168.11.13
node1: 192.168.11.14
lb: 192.168.11.253
nfs服务端: 192.168.11.14
新加机器master2: 192.168.11.8
安装介质机器：192.168.11.6
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;一个master宕机在另外一台服务器上恢复方法&#34;&gt;一个master宕机，在另外一台服务器上恢复方法&lt;/h2&gt;
&lt;p&gt;假设为master2（192.168.11.16）宕机了，现以新机器192.168.11.8作为恢复master2的机器。以下操作都在master1机器上执行。
1、在安装介质机器即master1机器上面，进入/etc/ssl/etcd/ssl目录下，将含有master2相关联的文件证书删掉（那个master有问题就出来那个）。
&lt;code&gt;rm -rf admin-master2-key.pem admin-master2.pem member-master2-key.pem member-master2.pem node-master2-key.pem node-master2.pem&lt;/code&gt;
2、将etcd集群中master2的节点移除。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;第一步先转为etcd3版本：export ETCDCTL_API=3
查看etcd集群的成员：
etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem member list
b3487da7c562b17, started, etcd1, https://192.168.11.6:2380, https://192.168.11.6:2379
3ad323c042cc4c05, started, etcd2, https://192.168.11.13:2380, https://192.168.11.13:2379
52a4779150442b41, started, etcd3, https://192.168.11.16:2380, https://192.168.11.16:2379
第三步：移除master2节点，如192.168.11.16
etcdctl member remove 52a4779150442b41 --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem  --key=/etc/ssl/etcd/ssl/node-master1-key.pem
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3、新打开master1机器窗口进入解压包目录下修改conf/hosts.ini,一定要新打开窗口（由于上一步操作使master1机器为etcd3版本指令）。
master2的IP由原来的192.168.11.16改成192.168.11.8；在[all]组里面，master2行整体放在master3下面，[kube-master]和[etcd]组中master2放在master3下面,然后执行安装脚本即可。
4、验证结果：
用&lt;code&gt;kubectl get nodes -o wide&lt;/code&gt;指令看master2IP是否替换；
看etcd集群中是否包含新的master2IP，指令为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、开启etcd3版本：export ETCDCTL_API=3
2、etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;一个master中etcd服务不正常在另外一台服务器上恢复etcd方法&#34;&gt;一个master中etcd服务不正常，在另外一台服务器上恢复etcd方法&lt;/h2&gt;
&lt;p&gt;假设为master2（192.168.11.16）宕机了，现以新机器192.168.11.8作为恢复master2的机器。以下操作都在master1机器上执行。
1、在安装介质机器即master1机器上面，进入/etc/ssl/etcd/ssl目录下，将含有master2相关联的文件证书删掉（那个master有问题就出来那个）。
&lt;code&gt;rm -rf admin-master2-key.pem admin-master2.pem member-master2-key.pem member-master2.pem node-master2-key.pem node-master2.pem&lt;/code&gt;
2、将etcd集群中master2的节点移除。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;第一步先转为etcd3版本：export ETCDCTL_API=3
查看etcd集群的成员：
etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem member list
b3487da7c562b17, started, etcd1, https://192.168.11.6:2380, https://192.168.11.6:2379
3ad323c042cc4c05, started, etcd2, https://192.168.11.13:2380, https://192.168.11.13:2379
52a4779150442b41, started, etcd3, https://192.168.11.16:2380, https://192.168.11.16:2379
第三步：移除master2节点，如192.168.11.16
etcdctl member remove 52a4779150442b41 --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem  --key=/etc/ssl/etcd/ssl/node-master1-key.pem
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3、新打开master1机器窗口进入解压包目录下修改conf/hosts.ini,一定要新打开窗口（由于上一步操作使master1机器为etcd3版本指令）。
master2的IP由原来的192.168.11.16改成192.168.11.8；在[all]组里面，master2行整体放在master3下面，[kube-master]和[etcd]组中master2放在master3下面。
4、进入解压包，scripts目录下，编辑install.sh脚本，用#将如下内容注释掉：
&lt;code&gt;ansible-playbook -i $BASE_FOLDER/../k8s/inventory/my_cluster/hosts.ini $BASE_FOLDER/../kubesphere/kubesphere.yml -b&lt;/code&gt;
5、进入解压包，k8s目录下，编辑cluster.yml文件，用#将以下开头的内容至结尾都注释掉&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- hosts: k8s-cluster
  any_errors_fatal: &amp;quot;{{ any_errors_fatal | default(true) }}&amp;quot;
  roles:
    - { role: kubespray-defaults}
    - { role: kubernetes/node, tags: node }
  environment: &amp;quot;{{ proxy_env }}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;6、重新到脚本目录，执行install.sh脚本即可。
7、验证结果：
看etcd集群中是否包含新的master2IP，指令为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、开启etcd3版本：export ETCDCTL_API=3
2、etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem member list
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>添加公网ip到kubernetes的apiserver操作指南</title>
      <link>https://Forest-L.github.io/post/add-public-ip-to-kubernetes-apiserver-operation-guide/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/add-public-ip-to-kubernetes-apiserver-operation-guide/</guid>
      <description>&lt;p&gt;通常情况下，我们的kubernetes集群是内网环境，如果希望通过本地访问这个集群，怎么办呢？大家想到的是Kubeadm在初始化的时候会为管理员生成一个 Kubeconfig文件，把它下载下来 是不是就可以？事实证明这样不行， 因为这个集群是内网集群，Kubeconfig文件 中APIServer的地址是内网ip。解决方案很简单，把公网ip签到证书里面就可以，其中有apiServerCertSANs这个选项，只要把公网IP写到这里，再启动这个集群的时候，这个公网ip就会签到证书里。&lt;/p&gt;
&lt;h2 id=&#34;1-环境信息&#34;&gt;1. 环境信息&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;安装方式：kubeadm&lt;/li&gt;
&lt;li&gt;内网IP：192.168.0.8&lt;/li&gt;
&lt;li&gt;外网IP：139.198.19.37&lt;/li&gt;
&lt;li&gt;证书目录：/etc/kubernetes/pki&lt;/li&gt;
&lt;li&gt;kubeadm配置文件目录：/etc/kubernetes/kubeadm-config.yaml&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-查看apiserver的证书包含的ip进入到证书目录执行&#34;&gt;2. 查看apiserver的证书包含的ip,进入到证书目录执行&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;cd /etc/kubernetes/pki&lt;/code&gt;
&lt;code&gt;openssl x509 -in apiserver.crt -noout -text&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;openssl x509 -in apiserver.crt -noout -text
Certificate:
    Data:
        ................
        Validity
            Not Before: Jun  5 02:26:44 2020 GMT
            Not After : Jun  5 02:26:44 2021 GMT
        ..................
        X509v3 extensions:
            ..........
            X509v3 Subject Alternative Name:
                IP Address:192.168.0.8
    .......
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3-添加公网ip到apiserver&#34;&gt;3. 添加公网IP到apiserver&lt;/h2&gt;
&lt;p&gt;绑定的公网ip为 139.198.19.37 ，确保公网ip的防火墙已经打开6443端口&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3.1 登录到主节点，进入 /etc/kubernetes/目录下&lt;/li&gt;
&lt;li&gt;3.2 修改kubeadm-config.yaml，找到 ClusterConfiguration 中的 	certSANs (如无，在 apiServer 下添加这一配置)，如下。添加刚才绑定的 139.198.19.37 到 certSANs 下，保存文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
etcd:
  external:
    endpoints:
    - https://192.168.0.8:2379
apiServer:
  extraArgs:
    authorization-mode: Node,RBAC
  timeoutForControlPlane: 4m0s
  certSANs:
    - 192.168.0.8
    - 139.198.19.37
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;3.3 执行如下命令更新 apiserver.crt apiserver.key
注意需要把之前apiserver.crt apiserver.key做备份,进入到pki目录下，执行如下指令做备份：
&lt;code&gt;mv apiserver.crt apiserver.crt-bak&lt;/code&gt;
&lt;code&gt;mv apiserver.key apiserver.key-bak&lt;/code&gt;
备份完之后，回到/etc/kubernetes目录下，执行公网ip添加到apiserver操作指令为：
kubeadm init phase certs apiserver &amp;ndash;config kubeadm-config.yaml&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubeadm init phase certs apiserver --config kubeadm-config.yaml
[certs] Generating &amp;quot;apiserver&amp;quot; certificate and key
[certs] apiserver serving cert is signed for DNS names [192.168.0.8  139.198.19.37]
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;3.4 再次查看apiserver中证书包含的ip，指令如下,看的公网ip则操作成功。
openssl x509 -in pki/apiserver.crt -noout -text | grep 139.198.19.37&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;openssl x509 -in pki/apiserver.crt -noout -text | grep 139.198.19.37
                IP Address:192.168.0.8, IP Address:139.198.19.37
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;3.5 重启kube-apiserver
如果是高可用集群，直接杀死当前节点的kube-apiserver进程，等待kubelet拉起kube-apiserver即可。需要在三个节点执行步骤1到步骤4，逐一更新。
如果是非高可用集群，杀死kube-apiserver可能会导致服务有中断，需要在业务低峰的时候操作。
进入/etc/kubernetes/manifests目录下，mv kube-apiserver.yaml文件至别的位置，然后又移回来即可&lt;/li&gt;
&lt;li&gt;3.6 修改kubeconfig中的server ip地址为 139.198.19.37，保存之后就可以直接通过公网访问kubernetes集群
&lt;code&gt;kubectl --kubeconfig config config view&lt;/code&gt;
&lt;code&gt;kubectl --kubeconfig config get node&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-附录-证书过期处理方式&#34;&gt;4. 附录-证书过期处理方式&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;kubeadm部署的k8s集群，默认证书目录为：/etc/kubernetes/pki,如果非pki，以ssl为例，需要创建软链接。证书过期包含核心组件apiserver和node上的token。&lt;/li&gt;
&lt;li&gt;4.1 master节点-apiserver处理方式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;1、查看证书有效期
cd /etc/kubernetes
openssl x509 -in ssl/apiserver.crt -noout -enddate 
2. 更新过期证书（/etc/kubernetes） (先在master1 节点执行)
创建软连接pki -&amp;gt; ssl ： ln -s ssl/ pki    (如pki存在，可略过)

kubeadm alpha certs renew apiserver 
kubeadm alpha certs renew apiserver-kubelet-client 
kubeadm alpha certs renew front-proxy-client 
3. 更新kubeconfig（/etc/kubernetes）(master1 节点)
需更新admin.conf / scheduler.conf / controller-manager.conf / kubelet.conf

kubeadm alpha certs renew admin.conf
kubeadm alpha certs renew controller-manager.conf
kubeadm alpha certs renew scheduler.conf

特别注意：以master1为例，将如下master1替换实际的节点名称。
kubeadm alpha kubeconfig user --client-name=system:node:master1 --org=system:nodes &amp;gt; kubelet.conf

4. 如上述kubeconfig中apiserver地址非lb地址，则修改为lb地址：(master1 节点)
https://192.168.0.13:6443 -&amp;gt; https://{ lb domain or ip }:6443

5. 重启k8s master组件：(master1 节点)
docker ps -af name=k8s_kube-apiserver* -q | xargs --no-run-if-empty docker rm -f
docker ps -af name=k8s_kube-scheduler* -q | xargs --no-run-if-empty docker rm -f
docker ps -af name=k8s_kube-controller-manager* -q | xargs --no-run-if-empty docker rm -f
systemctl restart kubelet

6. 验证kubeconfig有效性及查看节点状态 (master1 节点)

kubectl get node –kubeconfig admin.conf
kubectl get node –kubeconfig scheduler.conf
kubectl get node –kubeconfig controller-manager.conf
kubectl get node –kubeconfig kubelet.conf

7. 特别注意：同步master1证书/etc/kubernetes/ssl至master2、master3的对应路径中/etc/kubernetes/ssl（同步前建议备份旧证书）
证书路径：/etc/kubernetes/ssl

8. 更新kubeconfig（/etc/kubernetes）(master2, master3)

kubeadm alpha certs renew admin.conf
kubeadm alpha certs renew controller-manager.conf
kubeadm alpha certs renew scheduler.conf

特别注意：以下命令中以master2、master3为例，将如下master2/master3替换实际的节点名称。
kubeadm alpha kubeconfig user --client-name=system:node:master2 --org=system:nodes &amp;gt; kubelet.conf （master2）
kubeadm alpha kubeconfig user --client-name=system:node:master3 --org=system:nodes &amp;gt; kubelet.conf （master3）

9. 如上述kubeconfig中apiserver地址非lb地址，则修改为lb地址：(master2、master3)

https://192.168.0.13:6443 -&amp;gt; https://{ lb domain or ip }:6443

注：涉及文件：admin.conf、controller-manager.conf、scheduler.conf、kubelet.conf

10. 重启master2、master3中对应master组件

docker ps -af name=k8s_kube-apiserver* -q | xargs --no-run-if-empty docker rm -f
docker ps -af name=k8s_kube-scheduler* -q | xargs --no-run-if-empty docker rm -f
docker ps -af name=k8s_kube-controller-manager* -q | xargs --no-run-if-empty docker rm -f
systemctl restart kubelet

11. 验证kubeconfig有效性 （master2、master3）

kubectl get node –kubeconfig admin.conf
kubectl get node –kubeconfig scheduler.conf
kubectl get node –kubeconfig controller-manager.conf
kubectl get node –kubeconfig kubelet.conf
12. 更新~/.kube/config （master1、master2、master3）

cp admin.conf ~/.kube/config
注：如node节点也需使用kubectl，将master1上的~/.kube/config拷贝至对应node节点~/.kube/config

13. 验证~/.kube/config有效性：

 kubectl get node  查看集群状态
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;4.2 node节点token证书处理方式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;1. kubeadm token list 查看输出若为空或显示日期过期，则需重新生成。

2. kubeadm token create 重新生成token

3. 记录token值,保存下来。

4. 替换node节点/etc/kubernetes/ bootstrap-kubelet.conf中token （所有node节点）

5. 删除/etc/kubernetes/kubelet.conf （所有node节点）

rm -rf /etc/kubernetes/kubelet.conf
6. 重启kubelet （所有node节点）

systemctl restart kubelet
7. 查看节点状态：

kubectl get node 验证集群状态
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>openebs基于k8s安装</title>
      <link>https://Forest-L.github.io/post/k8s-openebs-install/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s-openebs-install/</guid>
      <description>&lt;h2 id=&#34;ceph介绍&#34;&gt;ceph介绍&lt;/h2&gt;
&lt;p&gt;ceph为块存储，以下通过cephadm安装ceph集群，ceph主要有三个基本角色：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OSD 用于集群中所有数据与对象的存储，处理集群数据的复制、恢复、回填、再均衡，并向其他osd守护进程发送心跳，然后向 Monitor 提供一些监控信息。&lt;/li&gt;
&lt;li&gt;Monitor 监控整个集群的状态，维护集群的 cluster MAP 二进制表，保证集群数据的一致性。&lt;/li&gt;
&lt;li&gt;MDS (可选) 为 Ceph 文件系统提供元数据计算、缓存与同步。MDS 进程并不是必须的进程，只有需要使用 CephFS 时，才需要配置 MDS 节点。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1环境准备&#34;&gt;1、环境准备&lt;/h2&gt;
&lt;h3 id=&#34;11节点规划&#34;&gt;1.1、节点规划&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;节点  &lt;/th&gt;
&lt;th&gt;os  &lt;/th&gt;
&lt;th&gt;IP   &lt;/th&gt;
&lt;th&gt;磁盘&lt;/th&gt;
&lt;th&gt;角色&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ceph1&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.13&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;cephadm，mgr, mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph2&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.14&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph3&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.16&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;12修改hosts文件&#34;&gt;1.2、修改hosts文件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hostnamectl set-hostname ceph1  #节点1上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph2  #节点2上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph3  #节点3上执行&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;13配置hosts&#34;&gt;1.3、配置hosts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt; /etc/hosts &amp;lt;&amp;lt;EOF  
192.168.0.13 ceph1
192.168.0.14 ceph2
192.168.0.16 ceph3
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;14关闭防火墙&#34;&gt;1.4、关闭防火墙&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都要执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld
setenforce 0
sed -i &#39;s/^SELINUX=.*/SELINUX=disabled/&#39; /etc/selinux/config
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;15时间同步&#34;&gt;1.5、时间同步&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;yum install -y chrony
systemctl enable --now chronyd

以ceph1作为时间参考，以下在三个节点都操作，添加server ceph1 prefer
vi /etc/chrony.conf
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
server ceph1 prefer

重启chrony
systemctl restart chronyd
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;16dockerlvm2和python3安装&#34;&gt;1.6、docker、lvm2和python3安装&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;三个节点都需要执行：
curl -fsSL get.docker.com -o get-docker.sh 
sh get-docker.sh
systemctl enable docker
systemctl restart docker

yum install -y python3

yum -y install lvm2
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2cephadm安装ceph1&#34;&gt;2、cephadm安装(ceph1)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;官方下载方法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;git下载&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;git clone --single-branch --branch octopus https://github.com/ceph/ceph.git&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cp ceph/src/cephadm/cephadm ./&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cephadm安装(ceph1)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;./cephadm add-repo --release octopus
./cephadm install

mkdir -p /etc/ceph
cephadm bootstrap --mon-ip 192.168.0.13

安装完成提示：

             URL: https://ceph1:8443/
            User: admin
        Password: 86yvswzdzd

INFO:cephadm:You can access the Ceph CLI with:

        sudo /usr/sbin/cephadm shell --fsid 3861dbf4-fd47-11ea-a373-5254228af870 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

INFO:cephadm:Please consider enabling telemetry to help improve Ceph:

        ceph telemetry on

For more information see:

        https://docs.ceph.com/docs/master/mgr/telemetry/

INFO:cephadm:Bootstrap complete.
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;配置ceph.pub
将ceph1的/etc/ceph/ceph.pub放入到ceph2和ceph3的.ssh/authorized_keys,ceph2和ceph3下root目录下没有.ssh，得先创建，最低权限为700，authorized_keys最低权限600。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ceph指令生效&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;cephadm shell&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;添加节点&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph3&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3osd部署&#34;&gt;3、OSD部署&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cephadm shell下执行&lt;/li&gt;
&lt;li&gt;若块设备没显示，可加&amp;ndash;refresh参数&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[ceph: root@ceph1 /]# ceph orch device ls
HOST   PATH      TYPE   SIZE  DEVICE_ID     MODEL  VENDOR  ROTATIONAL  AVAIL  REJECT REASONS
ceph1  /dev/vdc  hdd   50.0G  vol-e801hi3v  n/a    0x1af4  1           True
ceph1  /dev/vda  hdd   20.0G  i-2iwis9yr    n/a    0x1af4  1           False  locked
ceph1  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
ceph2  /dev/vdc  hdd   50.0G  vol-lqcchpox  n/a    0x1af4  1           True
ceph2  /dev/vda  hdd   20.0G  i-r53b2flp    n/a    0x1af4  1           False  locked
ceph2  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  locked, Insufficient space (&amp;lt;5GB)
ceph3  /dev/vdc  hdd    100G  vol-p0eksa5m  n/a    0x1af4  1           True
ceph3  /dev/vda  hdd   20.0G  i-f6njdmtq    n/a    0x1af4  1           False  locked
ceph3  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
[ceph: root@ceph1 /]#

devcice确认true后，执行
[ceph: root@ceph1 /]# ceph orch apply osd --all-available-devices
Scheduled osd.all-available-devices update...
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;查看集群状态,ceph方式及运行容器方式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;ceph方式：
[ceph: root@ceph1 /]# ceph -s
  cluster:
    id:     af1f33f0-fd69-11ea-8530-5254228af870
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 66s)
    mgr: ceph1.rbarkj(active, since 12m), standbys: ceph2.uzdnki
    osd: 3 osds: 3 up (since 14s), 3 in (since 14s)

  data:
    pools:   1 pools, 1 pgs
    objects: 1 objects, 0 B
    usage:   3.0 GiB used, 197 GiB / 200 GiB avail
    pgs:     1 active+clean

容器方式：
[root@ceph1 ceph]# docker ps -a
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES
b417841153a9        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-osd -…&amp;quot;   5 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-osd.2
9dbd720f4a6f        prom/prometheus:v2.18.1      &amp;quot;/bin/prometheus --c…&amp;quot;   6 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-prometheus.ceph1
f860282adf85        prom/alertmanager:v0.20.0    &amp;quot;/bin/alertmanager -…&amp;quot;   6 minutes ago       Up 6 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-alertmanager.ceph1
a6cb4b354f46        ceph/ceph-grafana:6.6.2      &amp;quot;/bin/sh -c &#39;grafana…&amp;quot;   16 minutes ago      Up 16 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-grafana.ceph1
b745f57895f3        prom/node-exporter:v0.18.1   &amp;quot;/bin/node_exporter …&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-node-exporter.ceph1
b00c1a2c7ef6        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-crash…&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-crash.ceph1
99c703f18f67        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mgr -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mgr.ceph1.rbarkj
e972d3d3f13a        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mon -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mon.ceph1

&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.ceph.com/en/latest/cephadm/install/&#34;&gt;https://docs.ceph.com/en/latest/cephadm/install/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>rook基于k8s安装</title>
      <link>https://Forest-L.github.io/post/k8s-rook-install/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s-rook-install/</guid>
      <description>&lt;h2 id=&#34;rook介绍&#34;&gt;rook介绍&lt;/h2&gt;
&lt;p&gt;Rook是目前开源中比较流行的云原生的存储编排系统，专注于如何实现把ceph运行在Kubernetes平台上。将之前手工执行部署、初始化、配置、扩展、升级、迁移、灾难恢复、监控以及资源管理等转变为自动触发。比如集群增加一块磁盘，rook能自动初始化为一个OSD，并自动加入到合适的故障中，这个osd在kubernetes中是以pod形式运行的。&lt;/p&gt;
&lt;h2 id=&#34;1部署&#34;&gt;1、部署&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Rook-Ceph底层存储可以是：卷、分区、block模式的pv。主要包括三部分：CRD、Operator、Cluster。&lt;/li&gt;
&lt;li&gt;下载rook安装文件并部署&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;git clone --single-branch --branch release-1.3 https://github.com/rook/rook.git

cd cluster/examples/kubernetes/ceph

注意：若是重装需清空node节点的/var/lib/rook下的文件， 并且要保证挂载的卷或者分区是没有文件系统的

部署：
(1) CRD:    kubectl create -f common.yaml
(2) Operator:   kubectl create -f operator.yaml
(3) Cluster:    kubectl create -f cluster.yaml

[root@node1 ~]# kubectl get pod -n rook-ceph
NAME                                              READY   STATUS      RESTARTS   AGE
csi-cephfsplugin-l5rw5                            3/3     Running     0          8m33s
csi-cephfsplugin-lrzfj                            3/3     Running     0          22m
csi-cephfsplugin-mqbg8                            3/3     Running     0          53m
csi-cephfsplugin-provisioner-58888b54f5-fj8tf     5/5     Running     5          53m
csi-cephfsplugin-provisioner-58888b54f5-p2vpt     5/5     Running     9          53m
csi-rbdplugin-5mmsl                               3/3     Running     0          8m34s
csi-rbdplugin-d4966                               3/3     Running     0          53m
csi-rbdplugin-mcsxw                               3/3     Running     0          22m
csi-rbdplugin-provisioner-6ddbf76966-498gn        6/6     Running     11         53m
csi-rbdplugin-provisioner-6ddbf76966-z2vfq        6/6     Running     6          53m
rook-ceph-crashcollector-node1-69666f444d-5bgh9   1/1     Running     0          5m41s
rook-ceph-crashcollector-node2-6c5b88dcf5-th4xk   1/1     Running     0          6m4s
rook-ceph-crashcollector-node3-54b5c58544-hz6m8   1/1     Running     0          22m
rook-ceph-mgr-a-5d85bd689f-g2dfh                  1/1     Running     0          5m41s
rook-ceph-mon-a-76c84f876b-m62d7                  1/1     Running     0          6m38s
rook-ceph-mon-b-6dc744d5b8-bspvh                  1/1     Running     0          6m22s
rook-ceph-mon-c-67f5987779-4l8vf                  1/1     Running     0          6m4s
rook-ceph-operator-6659fb4ddd-wdxnp               1/1     Running     3          55m
rook-ceph-osd-0-57954bcb4f-8b48j                  1/1     Running     0          4m59s
rook-ceph-osd-1-7c59f96f47-xnfpc                  1/1     Running     0          4m48s
rook-ceph-osd-prepare-node1-hrzg6                 1/1     Running     0          5m38s
rook-ceph-osd-prepare-node2-c5wcl                 0/1     Completed   0          5m38s
rook-ceph-osd-prepare-node3-gkt7g                 0/1     Completed   0          5m38s
rook-discover-6ll64                               1/1     Running     0          8m34s
rook-discover-bvbsh                               1/1     Running     0          22m
rook-discover-cm5ls                               1/1     Running     0          54m
说明

(1) csi-cephfsplugin-*, csi-rbdplugin-* :   ceph-FS 和ceph-rbd CSI
(2) rook-ceph-crashcollector-*:  crash 收集器
(3) rook-ceph-mgr-*:  管理后台
(4) root-ceph-mon-*: Mon监视器，维护集群中的各种状态
(5) rook-ceph-osd-*: ceph-OSD，主要功能是数据的存储，本例中每个盘会起一个OSD
(6) rook-discorer-*:  检测符合要求的存储设备
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;12修改hosts文件&#34;&gt;1.2、修改hosts文件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hostnamectl set-hostname ceph1  #节点1上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph2  #节点2上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph3  #节点3上执行&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;13配置hosts&#34;&gt;1.3、配置hosts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt; /etc/hosts &amp;lt;&amp;lt;EOF  
192.168.0.13 ceph1
192.168.0.14 ceph2
192.168.0.16 ceph3
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;14关闭防火墙&#34;&gt;1.4、关闭防火墙&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都要执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld
setenforce 0
sed -i &#39;s/^SELINUX=.*/SELINUX=disabled/&#39; /etc/selinux/config
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;15时间同步&#34;&gt;1.5、时间同步&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;yum install -y chrony
systemctl enable --now chronyd

以ceph1作为时间参考，以下在三个节点都操作，添加server ceph1 prefer
vi /etc/chrony.conf
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
server ceph1 prefer

重启chrony
systemctl restart chronyd
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;16dockerlvm2和python3安装&#34;&gt;1.6、docker、lvm2和python3安装&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;三个节点都需要执行：
curl -fsSL get.docker.com -o get-docker.sh 
sh get-docker.sh
systemctl enable docker
systemctl restart docker

yum install -y python3

yum -y install lvm2
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2cephadm安装ceph1&#34;&gt;2、cephadm安装(ceph1)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;官方下载方法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;git下载&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;git clone --single-branch --branch octopus https://github.com/ceph/ceph.git&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cp ceph/src/cephadm/cephadm ./&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cephadm安装(ceph1)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;./cephadm add-repo --release octopus
./cephadm install

mkdir -p /etc/ceph
cephadm bootstrap --mon-ip 192.168.0.13

安装完成提示：

             URL: https://ceph1:8443/
            User: admin
        Password: 86yvswzdzd

INFO:cephadm:You can access the Ceph CLI with:

        sudo /usr/sbin/cephadm shell --fsid 3861dbf4-fd47-11ea-a373-5254228af870 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

INFO:cephadm:Please consider enabling telemetry to help improve Ceph:

        ceph telemetry on

For more information see:

        https://docs.ceph.com/docs/master/mgr/telemetry/

INFO:cephadm:Bootstrap complete.
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;配置ceph.pub
将ceph1的/etc/ceph/ceph.pub放入到ceph2和ceph3的.ssh/authorized_keys,ceph2和ceph3下root目录下没有.ssh，得先创建，最低权限为700，authorized_keys最低权限600。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ceph指令生效&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;cephadm shell&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;添加节点&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph3&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3osd部署&#34;&gt;3、OSD部署&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cephadm shell下执行&lt;/li&gt;
&lt;li&gt;若块设备没显示，可加&amp;ndash;refresh参数&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[ceph: root@ceph1 /]# ceph orch device ls
HOST   PATH      TYPE   SIZE  DEVICE_ID     MODEL  VENDOR  ROTATIONAL  AVAIL  REJECT REASONS
ceph1  /dev/vdc  hdd   50.0G  vol-e801hi3v  n/a    0x1af4  1           True
ceph1  /dev/vda  hdd   20.0G  i-2iwis9yr    n/a    0x1af4  1           False  locked
ceph1  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
ceph2  /dev/vdc  hdd   50.0G  vol-lqcchpox  n/a    0x1af4  1           True
ceph2  /dev/vda  hdd   20.0G  i-r53b2flp    n/a    0x1af4  1           False  locked
ceph2  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  locked, Insufficient space (&amp;lt;5GB)
ceph3  /dev/vdc  hdd    100G  vol-p0eksa5m  n/a    0x1af4  1           True
ceph3  /dev/vda  hdd   20.0G  i-f6njdmtq    n/a    0x1af4  1           False  locked
ceph3  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
[ceph: root@ceph1 /]#

devcice确认true后，执行
[ceph: root@ceph1 /]# ceph orch apply osd --all-available-devices
Scheduled osd.all-available-devices update...
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;查看集群状态,ceph方式及运行容器方式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;ceph方式：
[ceph: root@ceph1 /]# ceph -s
  cluster:
    id:     af1f33f0-fd69-11ea-8530-5254228af870
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 66s)
    mgr: ceph1.rbarkj(active, since 12m), standbys: ceph2.uzdnki
    osd: 3 osds: 3 up (since 14s), 3 in (since 14s)

  data:
    pools:   1 pools, 1 pgs
    objects: 1 objects, 0 B
    usage:   3.0 GiB used, 197 GiB / 200 GiB avail
    pgs:     1 active+clean

容器方式：
[root@ceph1 ceph]# docker ps -a
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES
b417841153a9        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-osd -…&amp;quot;   5 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-osd.2
9dbd720f4a6f        prom/prometheus:v2.18.1      &amp;quot;/bin/prometheus --c…&amp;quot;   6 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-prometheus.ceph1
f860282adf85        prom/alertmanager:v0.20.0    &amp;quot;/bin/alertmanager -…&amp;quot;   6 minutes ago       Up 6 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-alertmanager.ceph1
a6cb4b354f46        ceph/ceph-grafana:6.6.2      &amp;quot;/bin/sh -c &#39;grafana…&amp;quot;   16 minutes ago      Up 16 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-grafana.ceph1
b745f57895f3        prom/node-exporter:v0.18.1   &amp;quot;/bin/node_exporter …&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-node-exporter.ceph1
b00c1a2c7ef6        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-crash…&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-crash.ceph1
99c703f18f67        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mgr -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mgr.ceph1.rbarkj
e972d3d3f13a        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mon -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mon.ceph1

&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.ceph.com/en/latest/cephadm/install/&#34;&gt;https://docs.ceph.com/en/latest/cephadm/install/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>k8s-default-Storage-Class搭建</title>
      <link>https://Forest-L.github.io/post/k8s-default-storage-class-installer/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s-default-storage-class-installer/</guid>
      <description>&lt;h1 id=&#34;k8s-default-storage-class搭建&#34;&gt;k8s default Storage Class搭建&lt;/h1&gt;
&lt;p&gt;在k8s中，StorageClass为动态存储，存储大小设置不确定，对存储并发要求高和读写速度要求高等方面有很大优势；pv为静态存储，存储大小要确定。而default Storage Class的作用为pvc文件没有标识任何和storageclass相关联的信息，但通过annotations属性关联起来。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://Forest-L.github.io/img/storageclass.png&#34; alt=&#34;storageClass&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-创建storageclass&#34;&gt;1. 创建storageClass&lt;/h2&gt;
&lt;p&gt;要使用 StorageClass，我们就得安装对应的自动配置程序，比如我们这里存储后端使用的是 nfs，那么我们就需要使用到一个 nfs-client 的自动配置程序，我们也叫它 Provisioner，这个程序使用我们已经配置好的 nfs 服务器，来自动创建持久卷，也就是自动帮我们创建 PV。
nfs服务器参考博客&lt;a href=&#34;https://kubesphereio.com/post/linux-nfs-install/&#34;&gt;nfs服务器搭建&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;11-nfs-client-provisioner&#34;&gt;1.1 nfs-client-provisioner&lt;/h3&gt;
&lt;p&gt;前提：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes 1.9+&lt;/li&gt;
&lt;li&gt;Existing NFS Share&lt;/li&gt;
&lt;li&gt;helm&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;111-安装nfs-client-provisioner指令&#34;&gt;1.1.1 安装nfs-client-provisioner指令：&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;helm install --name nfs-client --set nfs.server=192.168.0.9 --set nfs.path=/nfsdatas stable/nfs-client-provisioner&lt;/code&gt;
如果安装报错，显示没有该helm的stable，在机器上添加helm 源
&lt;code&gt;helm repo add stable http://mirror.azure.cn/kubernetes/charts/&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;112-卸载指令&#34;&gt;1.1.2 卸载指令：&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;helm delete nfs-client&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;113-验证storageclass是否存在&#34;&gt;1.1.3 验证storageClass是否存在&lt;/h4&gt;
&lt;p&gt;在相应安装nfs-client-provisioner机器上执行：&lt;code&gt;kubectl get sc&lt;/code&gt;即可，如下所示：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@i-cj1r8a8m ~]# kubectl get sc
NAME              PROVISIONER                                   AGE
nfs-client    cluster.local/my-release-nfs-client-provisioner   1h
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2-defaultstorageclass&#34;&gt;2. DefaultStorageClass&lt;/h2&gt;
&lt;p&gt;在定义StorageClass时，可以在Annotation中添加一个键值对：storageclass.kubernetes.io/is-default-class: true，那么此StorageClass就变成默认的StorageClass了。&lt;/p&gt;
&lt;h3 id=&#34;21-第一种方法&#34;&gt;2.1 第一种方法&lt;/h3&gt;
&lt;p&gt;在这个PVC对象中添加一个声明StorageClass对象的标识，这里我们可以利用一个annotations属性来标识，如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvctest
  annotations:
    volume.beta.kubernetes.io/storage-class: &amp;quot;nfs-client&amp;quot;
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 10Mi
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;22-第二种方法&#34;&gt;2.2 第二种方法&lt;/h3&gt;
&lt;p&gt;用 kubectl patch 命令来更新：
&lt;code&gt;kubectl patch storageclass nfs-client -p &#39;{&amp;quot;metadata&amp;quot;: {&amp;quot;annotations&amp;quot;:{&amp;quot;storageclass.kubernetes.io/is-default-class&amp;quot;:&amp;quot;true&amp;quot;}}}&#39;&lt;/code&gt;
最后结果中包含default为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@i-cj1r8a8m ~]# kubectl get sc
NAME                 PROVISIONER                                   AGE
nfs-client (default)cluster.local/my-release-nfs-client-provisioner 2h
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>