<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pv-pvc-sc on 李林博客</title>
    <link>https://Forest-L.github.io/tags/pv-pvc-sc/</link>
    <description>Recent content in pv-pvc-sc on 李林博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <copyright>Copyright © 2008–2020</copyright>
    <lastBuildDate>Mon, 05 Oct 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://Forest-L.github.io/tags/pv-pvc-sc/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>k8s分布式存储总结</title>
      <link>https://Forest-L.github.io/post/k8s-storage-summary/</link>
      <pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s-storage-summary/</guid>
      <description>&lt;h2 id=&#34;1pvpvc和sc来源&#34;&gt;1、pv、pvc和sc来源&lt;/h2&gt;
&lt;p&gt;pv引入解耦了pod与底层存储；pvc引入分离声明与消费，分离开发与运维责任，存储由运维系统人员管理，开发人员只需要通过pvc声明需要存储的类型、大小和访问模式即可；sc引入使pv自动创建或删除，开发人员定义的pvc中声明stroageclassname以及大小等需求自动创建pv；运维人员只需要声明好sc以及quota配额即可，无需维护pv。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;早期pod使用volume方式，每次pod都需要配置存储，volume都需要配置存储插件的一堆配置，如果是第三方存储，配置非常复杂；强制开发人员需要了解底层存储类型和配置。从而引入了pv。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    volumeMounts:
    - mountPath: /data
      name: data
  volumes:
  - name: data
    capacity:
      storage: 10Gi
    cephfs:
      monitors:
      - 192.168.0.1:6789
      - 192.168.0.2:6789
      - 192.168.0.3:6789
      path: /opt/eshop_dir/eshop
      user: admin
      secretRef:
        name: ceph-secret

&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;pv的yaml文件，pv其实就是把volume的配置声明从pod中分离出来。存储系统由运维人员管理，开发人员不知道底层配置，所以引入了pvc。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: cephfs
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  cephfs:
    monitors:
    - 192.168.0.1:6789
    - 192.168.0.2:6789
    - 192.168.0.3:6789
    path: /opt/eshop_dir/eshop
    user: admin
    secretRef:
      name: ceph-secret
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;pvc的yaml文件，pvc会根据声明的大小、存储类型和accessMode等关键字查找pv，如果找到了匹配的pv，则会与之关联,而pod直接关联pvc。运维人员需要维护一堆pv，如果pv不够还需要手工创建新的pv，pv空闲还需要手动回收，所以引入了sc。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: cephfs
spec:
  accessModes:
      - ReadWriteMany
  resources:
      requests:
        storage: 8Gi
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;storageclass类似声明了一个非常大的存储池，其中一个最重要参数是provisioner，这个provisioner可以aws-ebs，ceph和nfs等。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: aws-gp2
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  fsType: ext4
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2存储发展过程&#34;&gt;2、存储发展过程&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;最初通过volume plugin实现，又称in-tree。&lt;/li&gt;
&lt;li&gt;1.8开始，新的插件形式支持外部存储系统，即FlexVolume,通过外部脚本集成外部存储接口。&lt;/li&gt;
&lt;li&gt;1.9开始，csi接入，存储厂商需要实现三个服务接口Identity Service、Controller Service、Node Service。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Identity service用于返回一些插件信息。
Controller Service实现Volume的curd操作。
Node Service运行在所有的Node节点，用于实现把volume挂载在当前Node节点的指定目录，该服务会监听一个socket，controller通过这个socket进行通信。
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;云原生分布式存储（Container Attached Storage）CAS&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;1、重新设计一个分布式存储，像openebs/longhorn/PortWorx/StorageOS。
2、已有的分布式存储包装管理，像Rook。
3、CAS：每个volume都由一个轻量级的controller来管理，这个controller可以是一个单独的pod；这个controller与使用该volume的应用pod在同一个node；不同的volume的数据使用多个独立的controller pod进行管理。
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3分布式存储分类&#34;&gt;3、分布式存储分类&lt;/h2&gt;
&lt;h4 id=&#34;31-块存储&#34;&gt;3.1 块存储&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;比如我们使用的ceph，ceph通过rbd实现块存储&lt;a href=&#34;https://kubesphereio.com/post/k8s-rook-install/&#34;&gt;rook搭建&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;32-共享存储&#34;&gt;3.2 共享存储&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;共享文件系统存储即提供文件系统存储接口，我们最常用的共享文件系统存储如NFS、CIFS、GlusterFS等，Ceph通过CephFS实现共享文件系统存储。&lt;a href=&#34;https://kubesphereio.com/post/linux-nfs-install/&#34;&gt;nfs搭建&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;33-对象存储&#34;&gt;3.3 对象存储&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Ceph通过RGW实现对象存储接口，RGW兼容AWS S3 API，因此Pod可以和使用S3一样使用Ceph RGW，比如Python可以使用boto3 SDK对桶和对象进行操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4各个存储fio性能测试供参考&#34;&gt;4、各个存储fio性能测试，供参考。&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://Forest-L.github.io/img/storage-fio.png&#34; alt=&#34;存储性能对比&#34;&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>