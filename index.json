[{"categories":null,"contents":"在共享云环境中成功部署、管理和共存应用的基础，取决于识别和声明应用资源需求和运行时依赖性。这个Predictable Demands模式是关于你应该如何声明应用需求，无论是硬性的运行时依赖还是资源需求。声明你的需求对于Kubernetes在集群中为你的应用找到合适的位置至关重要。\n存在问题 Kubernetes可以管理用不同编程语言编写的应用，只要该应用可以在容器中运行。然而，不同的语言有不同的资源需求。通常情况下，编译后的语言运行速度更快，而且经常是 与即时运行时或解释语言相比，需要更少的内存。考虑到很多同类别的现代编程语言对资源的要求都差不多，从资源消耗的角度来看，更重要的是领域、应用的业务逻辑和实际实现细节。\n很难预测容器可能需要多少资源才能发挥最佳功能，而知道服务运行的预期资源是开发人员（通过测试发现）。有些服务的CPU和内存消耗情况是固定的，有些服务则是瞬间的。有些服务需要持久性存储来存储数据；有些传统服务需要在主机上固定端口号才能正常工作。定义所有这些应用特性并将其传递给管理平台是云原生应用的基本前提。\n除了资源需求外，应用运行时还对平台管理的能力有依赖性，如数据存储或应用配置。\n解决方案 了解容器的运行时要求很重要，主要有两个原因。首先，在定义了所有的运行时依赖和资源需求设想后，Kubernetes可以智能地决定在集群上的哪里运行容器以获得最有效的硬件利用率。在大量优先级不同的进程共享资源的环境中，要想成功共存，唯一的办法就是提前了解每个进程的需求。然而，智能投放只是硬币的一面。\n容器资源配置文件必不可少的第二个原因是容量规划。根据具体的服务需求和服务总量，我们可以针对不同的环境做一些容量规划，得出性价比最高的主机配置文件，来满足整个集群的需求。服务资源配置文件和容量规划相辅相成，才能长期成功地进行集群管理。\n在深入研究资源配置文件之前，我们先来看看如何声明运行时依赖关系。\n运行时依赖 最常见的运行时依赖之一是用于保存应用程序状态的文件存储。容器文件系统是短暂的，当容器关闭时就会丢失。Kubernetes提供了volume作为Pod级的存储实用程序，可以在容器重启后幸存。\n最直接的卷类型是emptyDir，只要Pod存活，它就会存活，当Pod被删除时，它的内容也会丢失。卷需要有其他类型的存储机制支持，才能有一个在Pod重启后仍能存活的卷。如果你的应用程序需要向这种长时间的存储设备读写文件，你必须在容器定义中使用volumes明确声明这种依赖性。 如例1-1所示。\n如例1-1，依赖于PV apiVersion: v1 kind: Pod metadata: name: random-generator spec: containers: - image: k8spatterns/random-generator:1.0 name: random-generator volumeMounts: - mountPath:\u0026quot;/logs\u0026quot; name: log-volume volumes: - name: log-volume persistentVolumeClaim: claimName: random-generator-log 调度器会评估Pod所需要的卷类型，这将影响Pod的调度位置。如果Pod需要的卷不是由集群上的任何节点提供的，那么Pod根本不会被调度。卷是运行时依赖性的一个例子，它影响Pod可以运行什么样的基础设施，以及Pod是否可以被调度。\n当你要求Kubernetes通过hostPort方式暴露容器端口为主机上特定端口时，也会发生类似的依赖关系。hostPort的使用在节点上创建了另一个运行时依赖性，并限制了Pod的调度位置。 hostPort在集群中的每个节点上保留了端口，并限制每个节点最多调度一个Pod。由于端口冲突，你可以扩展到Kubernetes集群中有多少节点就有多少Pod。\n另一种类型的依赖是配置。几乎每个应用程序都需要一些配置信息，Kubernetes提供的推荐解决方案是通过ConfigMaps。你的服务需要有一个消耗设置的策略\u0026ndash;无论是通过环境变量还是文件系统。无论是哪种情况，这都会引入你的容器对名为ConfigMaps的运行时依赖性。如果没有创建所有预期的 ConfigMaps，则容器被调度在节点上，但它们不会启动。ConfigMaps和Secrets在第19章Configuratio资源中进行了更详细的解释，例1-2展示了如何将这些资源用作运行时依赖。\n实例1-2所示，依赖于ConfigMap apiVersion: v1 kind: Pod metadata: name: random-generator spec: containers: - image: k8spatterns/random-generator:1.0 name: random-generator env: - name: PATTERN valueFrom: configMapKeyRef: name: random-generator-config key: pattern 与ConfigMaps类似的概念是Secrets，它提供了一种略微更安全的方式将特定环境的配置分发到容器中。使用Secret的方式与使用ConfigMap的方式相同，它引入了从容器到namespace的相同依赖性。\n虽然ConfigMap和Secret对象的创建是我们必须执行的简单管理任务，但集群节点提供了存储和端口。其中一些依赖性限制了Pod被调度的位置（如果有的话），而其他依赖性则限制了Pod的运行。 可能会阻止Pod的启动。在设计带有这种依赖关系的容器化应用程序时，一定要考虑它们创建之后运行时的约束。\n资源配置文件 指定容器的依赖性，如ConfigMap、Secret和卷，是很直接的。我们需要更多的思考和实验来确定容器的资源需求。在Kubernetes的上下文中，计算资源被定义为可以被容器请求、分配给容器并从容器中获取的东西。资源分为可压缩的（即可以节制的，如CPU，或网络带宽）和不可压缩的（即不能节制的，如内存）。\n区分可压缩资源和不可压缩资源很重要。如果你的容器消耗了太多的可压缩资源（如CPU），它们就会被节流，但如果它们使用了太多的不可压缩资源（如内存），它们就会被杀死（因为没有其他方法可以要求应用程序释放分配的内存）。\n根据你的应用程序的性质和实现细节，你必须指定所需资源的最小量（称为请求）和它可以增长到的最大量（限制）。每个容器定义都可以以请求和限制的形式指定它所需要的CPU和内存量。在一个高层次上，请求/限制的概念类似于软/硬限制。例如，同样地，我们通过使用-Xms和-Xmx命令行选项来定义Java应用程序的堆大小。\n调度器将Pod调度到节点时，使用的是请求量（但不是限制）。对于一个给定的Pod，调度器只考虑那些仍有足够能力容纳Pod及其所有请求资源量相加容器的节点。从这个意义上说，每个容器的请求字段会影响到Pod可以被调度或不被调度的位置。例1-3显示了如何为Pod指定这种限制。\n实例1-3，资源限制 apiVersion: v1 kind: Pod metadata: name: random-generator spec: containers: - image: k8spatterns/random-generator:1.0 name: random-generator resources: requests: cpu: 100m memory: 100Mi limits: cpu: 200m memory: 200Mi 根据您是指定请求、限制，还是两者都指定，平台提供不同的服务质量（QoS）。\nBest-Effort 没有为其容器设置任何请求和限制的Pod。这样的Pod被认为是最低优先级的，当Pod的节点用完不可压缩资源时，会首先被干掉。\nBurstable 已定义请求和限制的Pod，但它们并不相等（而且限制比预期的请求大）。这样的Pod有最小的资源保证，但也愿意在可用的情况下消耗更多的资源，直至其极限。当节点面临不可压缩的资源压力时，如果没有Best-Effort Pods剩余，这些Pod很可能被干掉。\nGuaranteed 拥有同等数量请求和限制资源的Pod。这些是优先级最高的Pod，保证不会在Best-Effort和Burstable Pods之前被干掉。\n所以你为容器定义的资源特性或省略资源特性会直接影响到它的QoS，并定义了Pod在资源不足时的相对重要性。在定义你的Pod资源需求时，要考虑到这个后果。\nPod优先级 我们解释了容器资源声明如何也定义了Pod的QoS，并影响Kubelet在资源不足时干掉Pod中容器的顺序。另一个相关的功能，在写这篇文章的时候还在测试阶段，就是Pod优先和优先权。Pod优先级允许表明一个Pod相对于其他Pod的重要性，这影响了Pod的调度顺序。让我们在例子1-4中看到它的作用。\n实例1-4，pod优先级 apiVersion: scheduling.k8s.io/v1beta1 kind: PriorityClass metadata: name: high-priority value: 1000 globalDefault: false description: This is a very high priority Pod class --- apiVersion: v1 kind: Pod metadata: name: random-generator labels: env: random-generator spec: containers: - image: k8spatterns/random-generator:1.0 name: random-generator priorityClassName: high-priority 我们创建了一个PriorityClass，这是一个非命名空间的对象，用于定义一个基于整数的优先级。我们的PriorityClass被命名为high-priority，优先级为1,000。现在我们可以通过它的名字将这个优先级分配给Pods，如priorityClassName：high-riority。PriorityClass是一种表示Pods相对重要性的机制，数值越高表示Pods越重要。\n启用Pod Priority功能后，它会影响调度器将Pod调度在节点上的顺序。首先，优先权进入许可控制器使用priorityClass Name字段来填充新Pod的优先权值。当有多个Pod等待调度时，调度器按最高优先级对待放Pod队列进行排序。在调度队列中，任何待定的Pod都会被选在其他优先级较低的待定Pod之前，如果没有阻止其调度的约束条件，该Pod就会被调度。\n下面是关键部分。如果没有足够容量的节点来调度Pod，调度器可以从节点上抢占（移除）优先级较低的Pod，以释放资源，调度优先级较高的Pod。因此，如果满足其他所有调度要求，优先级较高的Pod可能比优先级较低的Pod更早被调度。这种算法有效地使集群管理员能够控制哪些Pod是更关键的工作负载，并通过允许调度器驱逐优先级较低的Pod，以便在工作节点上为优先级较高的Pod腾出空间，将它们放在第一位。如果一个Pod不能被调度，调度器就会继续调度其他优先级较低的Pod。\nPod QoS（前面已经讨论过了）和Pod优先级是两个正交的特性，它们之间没有联系，只有一点点重叠。QoS主要被Kubelet用来在可用计算资源较少时保持节点稳定性。==Kubelet在驱逐前首先考虑QoS，然后考虑Pods的PriorityClass。另一方面，调度器驱逐逻辑在选择抢占目标时完全忽略了Pods的QoS==。调度器试图挑选一组优先级最低的Pod，满足优先级较高的Pod等待调度的需求。\n当Pod具有指定的优先级时，它可能会对其他被驱逐的Pod产生不良影响。例如，当一个Pod的优雅终止策略受到重视，第10章中讨论的PodDisruptionBudget，单服务没有得到保证，这可能会打破一个依赖多数Pod数的较低优先级集群应用。\n另一个问题是恶意或不知情的用户创建了优先级最高的Pods，并驱逐了所有其他Pods。为了防止这种情况发生，ResourceQuota已经扩展到支持PriorityClass，较大的优先级数字被保留给通常不应该被抢占或驱逐的关键系统Pods。\n总而言之，Pod优先级应谨慎使用，因为用户指定的数字优先级，指导调度器和Kubelet调度或干掉哪些Pod，会受到用户的影响。任何改变都可能影响许多Pod，并可能阻止平台提供可预测的服务级别协议。\n项目资源 Kubernetes是一个自助服务平台，开发者可以在指定的隔离环境上运行他们认为合适的应用。然而，在一个共享的多租户平台中工作，也需要存在特定的边界和控制单元，以防止一些用户消耗平台的所有资源。其中一个这样的工具是ResourceQuota，它为限制命名空间中的聚合资源消耗提供了约束。通过ResourceQuotas，集群管理员可以限制消耗的计算资源（CPU、内存）和存储的总和。它还可以限制命名空间中创建的对象（如ConfigMaps、Secrets、Pods或Services）的总数。\n这方面的另一个有用的工具是LimitRange，它允许为每种类型的资源设置资源使用限制。除了指定不同资源类型的最小和最大允许量以及这些资源的默认值外，还可以控制请求和限制之间的比例，也就是所谓的超额承诺水平。表1-1给出了如何选择请求和限额的可能值的例子。\n表1-1. Limit和request ranges Type Resource Min Max Default limit Default request Lim/req ratio Container CPU 500m 2 500m 250m 4 Container Memory 250Mi 2Gi 500Mi 250Mi 4 LimitRanges对于控制容器资源配置非常有用，这样就不会出现需要的资源超过集群节点所能提供的资源的容器。它还可以防止集群用户创建消耗大量资源的容器，使节点不能为其他容器分配资源。考虑到请求(而不是限制)是调度器用来调度的主要容器特性，LimitRequestRatio允许你控制容器的请求和限制之间的差距有多大。在请求和限制之间有很大的综合差距，会增加节点上超负荷的机会，并且当许多容器同时需要比最初请求更多的资源时，可能会降低应用性能。\n容量规划 考虑到容器在不同的环境中可能会有不同的资源情况，以及不同数量的实例，显然，多用途环境的容量规划并不简单。例如，为了获得最佳的硬件利用率，在一个非生产集群上，你可能主要拥有Best-Effort和Burstable容器。在这样的动态环境中，很多容器都是同时启动和关闭的，即使有容器在资源不足的时候被平台干掉，也不会致命。在生产集群上，我们希望事情更加稳定和可预测，容器可能主要是Guaranteed类型，还有一些Burstable。如果一个容器被杀死，那很可能是一个信号，说明集群的容量应该增加。\n当然，在现实生活中，你使用Kubernetes这样的平台，更可能的原因是还有很多服务需要管理，有些服务即将退出，有些服务还在设计开发阶段。即使是一个不断移动的目标，根据前面描述的类似方法，我们可以计算出每个环境中所有服务所需要的资源总量。\n讨论 容器不仅对隔离进程和作为打包方式有用。在确定了资源概况后，它们也是成功进行产能规划的基石。进行一些早期测试，以发现每个容器的资源需求，并将该信息作为未来产能规划和预测的基础。\n然而，更重要的是，资源配置文件是应用程序与Kubernetes沟通的方式，以协助调度和管理决策。如果你的应用不提供任何请求或限制，Kubernetes能做的就是把你的容器当作不透明的盒子，当集群满了的时候就会丢掉。所以，每一个应用或多或少都要考虑和提供这些资源声明。\n现在你已经知道了如何确定我们应用的大小，在第3章 \u0026ldquo;声明式部署 \u0026ldquo;中，你将学习多种策略来让我们的应用在Kubernetes上安装和更新。\n","date":"2020-12-26T15:38:59+08:00","permalink":"https://Forest-L.github.io/post/predictable-demands/","section":"post","tags":["K8s"],"title":"K8s中可预测的需求"},{"categories":["storage"],"contents":"ceph介绍 ceph为块存储，以下通过cephadm安装ceph集群，ceph主要有三个基本角色：\n OSD 用于集群中所有数据与对象的存储，处理集群数据的复制、恢复、回填、再均衡，并向其他osd守护进程发送心跳，然后向 Monitor 提供一些监控信息。 Monitor 监控整个集群的状态，维护集群的 cluster MAP 二进制表，保证集群数据的一致性。 MDS (可选) 为 Ceph 文件系统提供元数据计算、缓存与同步。MDS 进程并不是必须的进程，只有需要使用 CephFS 时，才需要配置 MDS 节点。  1、环境准备 1.1、节点规划    节点  os  IP  磁盘 角色     ceph1 centos 192.168.0.13 vdc cephadm，mgr, mon,osd   ceph2 centos 192.168.0.14 vdc mon,osd   ceph3 centos 192.168.0.16 vdc mon,osd    1.2、修改hosts文件  hostnamectl set-hostname ceph1 #节点1上执行 hostnamectl set-hostname ceph2 #节点2上执行 hostnamectl set-hostname ceph3 #节点3上执行  1.3、配置hosts  三个节点都执行  cat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt;EOF 192.168.0.13 ceph1 192.168.0.14 ceph2 192.168.0.16 ceph3 EOF 1.4、关闭防火墙  三个节点都要执行  systemctl disable --now firewalld setenforce 0 sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config 1.5、时间同步 yum install -y chrony systemctl enable --now chronyd 以ceph1作为时间参考，以下在三个节点都操作，添加server ceph1 prefer vi /etc/chrony.conf #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst server ceph1 prefer 重启chrony systemctl restart chronyd 1.6、docker、lvm2和python3安装 三个节点都需要执行： curl -fsSL get.docker.com -o get-docker.sh sh get-docker.sh systemctl enable docker systemctl restart docker yum install -y python3 yum -y install lvm2 2、cephadm安装(ceph1)  官方下载方法  curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm\n git下载  git clone --single-branch --branch octopus https://github.com/ceph/ceph.git\ncp ceph/src/cephadm/cephadm ./\n cephadm安装(ceph1)  ./cephadm add-repo --release octopus ./cephadm install mkdir -p /etc/ceph cephadm bootstrap --mon-ip 192.168.0.13 安装完成提示： URL: https://ceph1:8443/ User: admin Password: 86yvswzdzd INFO:cephadm:You can access the Ceph CLI with: sudo /usr/sbin/cephadm shell --fsid 3861dbf4-fd47-11ea-a373-5254228af870 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring INFO:cephadm:Please consider enabling telemetry to help improve Ceph: ceph telemetry on For more information see: https://docs.ceph.com/docs/master/mgr/telemetry/ INFO:cephadm:Bootstrap complete.   配置ceph.pub 将ceph1的/etc/ceph/ceph.pub放入到ceph2和ceph3的.ssh/authorized_keys,ceph2和ceph3下root目录下没有.ssh，得先创建，最低权限为700，authorized_keys最低权限600。\n  ceph指令生效\n  cephadm shell\n 添加节点  ceph orch host add ceph2\nceph orch host add ceph3\n3、OSD部署  cephadm shell下执行 若块设备没显示，可加\u0026ndash;refresh参数  [ceph: root@ceph1 /]# ceph orch device ls HOST PATH TYPE SIZE DEVICE_ID MODEL VENDOR ROTATIONAL AVAIL REJECT REASONS ceph1 /dev/vdc hdd 50.0G vol-e801hi3v n/a 0x1af4 1 True ceph1 /dev/vda hdd 20.0G i-2iwis9yr n/a 0x1af4 1 False locked ceph1 /dev/vdb hdd 4096M n/a 0x1af4 1 False Insufficient space (\u0026lt;5GB), locked ceph2 /dev/vdc hdd 50.0G vol-lqcchpox n/a 0x1af4 1 True ceph2 /dev/vda hdd 20.0G i-r53b2flp n/a 0x1af4 1 False locked ceph2 /dev/vdb hdd 4096M n/a 0x1af4 1 False locked, Insufficient space (\u0026lt;5GB) ceph3 /dev/vdc hdd 100G vol-p0eksa5m n/a 0x1af4 1 True ceph3 /dev/vda hdd 20.0G i-f6njdmtq n/a 0x1af4 1 False locked ceph3 /dev/vdb hdd 4096M n/a 0x1af4 1 False Insufficient space (\u0026lt;5GB), locked [ceph: root@ceph1 /]# devcice确认true后，执行 [ceph: root@ceph1 /]# ceph orch apply osd --all-available-devices Scheduled osd.all-available-devices update...  查看集群状态,ceph方式及运行容器方式  ceph方式： [ceph: root@ceph1 /]# ceph -s cluster: id: af1f33f0-fd69-11ea-8530-5254228af870 health: HEALTH_OK services: mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 66s) mgr: ceph1.rbarkj(active, since 12m), standbys: ceph2.uzdnki osd: 3 osds: 3 up (since 14s), 3 in (since 14s) data: pools: 1 pools, 1 pgs objects: 1 objects, 0 B usage: 3.0 GiB used, 197 GiB / 200 GiB avail pgs: 1 active+clean 容器方式： [root@ceph1 ceph]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b417841153a9 ceph/ceph:v15 \u0026quot;/usr/bin/ceph-osd -…\u0026quot; 5 minutes ago Up 5 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-osd.2 9dbd720f4a6f prom/prometheus:v2.18.1 \u0026quot;/bin/prometheus --c…\u0026quot; 6 minutes ago Up 5 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-prometheus.ceph1 f860282adf85 prom/alertmanager:v0.20.0 \u0026quot;/bin/alertmanager -…\u0026quot; 6 minutes ago Up 6 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-alertmanager.ceph1 a6cb4b354f46 ceph/ceph-grafana:6.6.2 \u0026quot;/bin/sh -c 'grafana…\u0026quot; 16 minutes ago Up 16 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-grafana.ceph1 b745f57895f3 prom/node-exporter:v0.18.1 \u0026quot;/bin/node_exporter …\u0026quot; 17 minutes ago Up 17 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-node-exporter.ceph1 b00c1a2c7ef6 ceph/ceph:v15 \u0026quot;/usr/bin/ceph-crash…\u0026quot; 17 minutes ago Up 17 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-crash.ceph1 99c703f18f67 ceph/ceph:v15 \u0026quot;/usr/bin/ceph-mgr -…\u0026quot; 18 minutes ago Up 18 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-mgr.ceph1.rbarkj e972d3d3f13a ceph/ceph:v15 \u0026quot;/usr/bin/ceph-mon -…\u0026quot; 18 minutes ago Up 18 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-mon.ceph1 参考  https://docs.ceph.com/en/latest/cephadm/install/ ","date":"2020-10-06T00:00:00Z","permalink":"https://Forest-L.github.io/post/storage-summary/","section":"post","tags":["vg","lvs"],"title":"存储总结"},{"categories":["storage"],"contents":"1、pv、pvc和sc来源 pv引入解耦了pod与底层存储；pvc引入分离声明与消费，分离开发与运维责任，存储由运维系统人员管理，开发人员只需要通过pvc声明需要存储的类型、大小和访问模式即可；sc引入使pv自动创建或删除，开发人员定义的pvc中声明stroageclassname以及大小等需求自动创建pv；运维人员只需要声明好sc以及quota配额即可，无需维护pv。\n 早期pod使用volume方式，每次pod都需要配置存储，volume都需要配置存储插件的一堆配置，如果是第三方存储，配置非常复杂；强制开发人员需要了解底层存储类型和配置。从而引入了pv。  apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - image: nginx name: nginx volumeMounts: - mountPath: /data name: data volumes: - name: data capacity: storage: 10Gi cephfs: monitors: - 192.168.0.1:6789 - 192.168.0.2:6789 - 192.168.0.3:6789 path: /opt/eshop_dir/eshop user: admin secretRef: name: ceph-secret  pv的yaml文件，pv其实就是把volume的配置声明从pod中分离出来。存储系统由运维人员管理，开发人员不知道底层配置，所以引入了pvc。  apiVersion: v1 kind: PersistentVolume metadata: name: cephfs spec: capacity: storage: 10Gi accessModes: - ReadWriteMany cephfs: monitors: - 192.168.0.1:6789 - 192.168.0.2:6789 - 192.168.0.3:6789 path: /opt/eshop_dir/eshop user: admin secretRef: name: ceph-secret  pvc的yaml文件，pvc会根据声明的大小、存储类型和accessMode等关键字查找pv，如果找到了匹配的pv，则会与之关联,而pod直接关联pvc。运维人员需要维护一堆pv，如果pv不够还需要手工创建新的pv，pv空闲还需要手动回收，所以引入了sc。  kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cephfs spec: accessModes: - ReadWriteMany resources: requests: storage: 8Gi  storageclass类似声明了一个非常大的存储池，其中一个最重要参数是provisioner，这个provisioner可以aws-ebs，ceph和nfs等。  kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: aws-gp2 provisioner: kubernetes.io/aws-ebs parameters: type: gp2 fsType: ext4 2、存储发展过程  最初通过volume plugin实现，又称in-tree。 1.8开始，新的插件形式支持外部存储系统，即FlexVolume,通过外部脚本集成外部存储接口。 1.9开始，csi接入，存储厂商需要实现三个服务接口Identity Service、Controller Service、Node Service。  Identity service用于返回一些插件信息。 Controller Service实现Volume的curd操作。 Node Service运行在所有的Node节点，用于实现把volume挂载在当前Node节点的指定目录，该服务会监听一个socket，controller通过这个socket进行通信。  云原生分布式存储（Container Attached Storage）CAS  1、重新设计一个分布式存储，像openebs/longhorn/PortWorx/StorageOS。 2、已有的分布式存储包装管理，像Rook。 3、CAS：每个volume都由一个轻量级的controller来管理，这个controller可以是一个单独的pod；这个controller与使用该volume的应用pod在同一个node；不同的volume的数据使用多个独立的controller pod进行管理。 3、分布式存储分类 3.1 块存储  比如我们使用的ceph，ceph通过rbd实现块存储rook搭建  3.2 共享存储  共享文件系统存储即提供文件系统存储接口，我们最常用的共享文件系统存储如NFS、CIFS、GlusterFS等，Ceph通过CephFS实现共享文件系统存储。nfs搭建  3.3 对象存储  Ceph通过RGW实现对象存储接口，RGW兼容AWS S3 API，因此Pod可以和使用S3一样使用Ceph RGW，比如Python可以使用boto3 SDK对桶和对象进行操作。  4、各个存储fio性能测试，供参考。 ","date":"2020-10-05T00:00:00Z","permalink":"https://Forest-L.github.io/post/k8s-storage-summary/","section":"post","tags":["pv-pvc-sc","k8s"],"title":"k8s分布式存储总结"},{"categories":["storage"],"contents":"1、Pmem介绍 PMEM是硬件产品，Intel Optane DC持久存储模块，是一种具有大容量和数据持久性的创新存储技术。有2种运行模式。两级内存模式无需软件更改，DCPMM被视为更大的内存，并使用DRAM作为其缓存层。AppDirect模式将设备暴露为持久内存，支持软件栈，可用于加速不同的应用程序。在本文中，我们将使用AppDirect模式。\n2、环境信息  一台master和一台node构成的K8s1.18.6集群 redis镜像为根据源码编译为pmem-redis:4.0.0 ceph/neonsan块存储 centos7.7/内核5.8.7  3、测试两种模式  测试目的是体验PMEM对REDIS的加速效果。先在没有PMEM加速AOF模式，再有PMEM加速PBA模式。 AOF模式是append only file的意思，通常REDIS是一种内存数据库，数据掉电就丢失了。AOF模式可以把数据库记录随时备份到分布式存储里，这样可以使得REDIS具有掉电恢复的功能。 PBA模式是pointer based AOF模式，它是使用PMEM对AOF做了加速，原理是备份写盘时只把指针写到磁盘里，数据还在内存或PMEM里，使用PMEM作为缓存。这样既可以掉电恢复，又提升了性能。充分发挥了PMEM AD模式的优势。  4、pmem-redis镜像构建  git clone https://github.com/clayding/opencloud_benchmark.git cd opencloud_benchmark/k8s/redis/docker docker build -t pmem-redis:latest \u0026ndash;network host .  5、PBA模式的设置  ipmctl安装  cd /etc/yum.repos.d/ wget https://copr.fedorainfracloud.org/coprs/jhli/ipmctl/repo/epel-7/jhli-ipmctl-epel-7.repo wget https://copr.fedorainfracloud.org/coprs/jhli/safeclib/repo/epel-7/jhli-safeclib-epel-7.repo yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm el-ha-for-rhel-*-server-rpms\u0026quot; yum install ndctl ndctl-libs ndctl-devel libsafec rubygem-asciidoctor yum install ipmctl  app direct模式  sudo ipmctl delete -goal sudo ipmctl create -goal PersistentMemoryType=AppDirect A reboot is required to process new memory allocation goals: sudo reboot  检测Pmem能正常工作且为ad模式,ad是否有值  ipmctl show -memoryresources MemoryType | DDR | PMemModule | Total ======================================================== Volatile | 191.000 GiB | 0.000 GiB | 191.000 GiB AppDirect | - | 504.000 GiB | 504.000 GiB Cache | 0.000 GiB | - | 0.000 GiB Inaccessible | 1.000 GiB | 1.689 GiB | 2.689 GiB Physical | 192.000 GiB | 505.689 GiB | 697.689 GiB 当ad没有值时，ipmctl start -diagnostic诊断是否有错误消息 刚开始遇到这样的一个问题： The platform configuration check detected that PMem module 0x0001 is not configured. 分析为：新版Ipmctl有问题，用1.x的版本把pcd delete以后重新provision就可以了 6、Pmem-csi安装 下载， https://github.com/intel/pmem-csi/blob/devel/docs/install.md#install-pmem-csi-driver cd pmem-CSI Setting up certificates for securities # curl -L https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o _work/bin/cfssl --create-dirs # curl -L https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o _work/bin/cfssljson --create-dirs # chmod a+x _work/bin/cfssl _work/bin/cfssljson # export PATH=$PATH:$PWD/_work/bin # ./test/setup-ca-kubernetes.sh Deploying the driver to K8s using LVM mode, please choose yaml files corresponding to your kubernetes version # kubectl create -f deploy/kubernetes-1.18/pmem-csi-lvm.yaml Applying a storage class # kubectl apply -f deploy/kubernetes-1.18/pmem-storageclass-ext4.yaml pod状态 kubectl get pod NAME READY STATUS RESTARTS AGE pmem-csi-controller-0 2/2 Running 0 22s pmem-csi-node-tw4mw 2/2 Running 2 33h sc状态 kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE pmem-csi-sc-ext4 pmem-csi.intel.com Delete Immediate false 3d2h 7、benchmark安装  yum install autoconf automake make gcc-c++ yum install pcre-devel zlib-devel libmemcached-devel  Remove system libevent and install new version:\n sudo yum remove libevent wget https://github.com/downloads/libevent/libevent/libevent-2.0.21-stable.tar.gz tar xfz libevent-2.0.21-stable.tar.gz pushd libevent-2.0.21-stable ./configure make sudo make install popd export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:${PKG_CONFIG_PATH}  Build and install memtier:\n git clone https://github.com/RedisLabs/memtier_benchmark cd memtier_benchmark autoreconf -ivf ./configure \u0026ndash;disable-tls make sudo make install  8、测试对比  aof的yanl文件  apiVersion: v1 kind: PersistentVolumeClaim metadata: name: rbd-pvc spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 2Gi storageClassName: csi-neonsan --- kind: Pod apiVersion: v1 metadata: name: redis-aof labels: app: redis-aof spec: containers: - name: redis-aof image: pmem-redis:latest imagePullPolicy: IfNotPresent args: [\u0026quot;no\u0026quot;] ports: - containerPort: 6379 resources: limits: cpu: \u0026quot;1000m\u0026quot; memory: \u0026quot;16Gi\u0026quot; requests: cpu: \u0026quot;1000m\u0026quot; memory: \u0026quot;16Gi\u0026quot; volumeMounts: - mountPath: \u0026quot;/ceph\u0026quot; name: ceph-csi-volume volumes: - name: ceph-csi-volume persistentVolumeClaim: claimName: rbd-pvc --- apiVersion: v1 kind: Service metadata: name: redis labels: app: redis-aof spec: type: NodePort ports: - name: redis port: 6379 nodePort: 30379 targetPort: 6379 selector: app: redis-aof  pba的yanl文件  apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pmem-csi-pvc-ext4 spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi storageClassName: pmem-csi-sc-ext4 # defined in pmem-storageclass-ext4.yaml --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: rbd-pvc spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 2Gi storageClassName: csi-neonsan --- kind: Pod apiVersion: v1 metadata: name: redis-with-pba labels: app: redis-with-pba spec: containers: - name: redis-with-pba image: pmem-redis:latest imagePullPolicy: IfNotPresent args: [\u0026quot;yes\u0026quot;] ports: - containerPort: 6379 resources: limits: cpu: \u0026quot;1000m\u0026quot; memory: \u0026quot;16Gi\u0026quot; requests: cpu: \u0026quot;1000m\u0026quot; memory: \u0026quot;16Gi\u0026quot; volumeMounts: - mountPath: \u0026quot;/data\u0026quot; name: my-csi-volume - mountPath: \u0026quot;/ceph\u0026quot; name: ceph-csi-volume volumes: - name: ceph-csi-volume persistentVolumeClaim: claimName: rbd-pvc - name: my-csi-volume persistentVolumeClaim: claimName: pmem-csi-pvc-ext4 --- apiVersion: v1 kind: Service metadata: name: redis-pba labels: app: redis-with-pba spec: type: NodePort ports: - name: redis port: 6379 nodePort: 31379 targetPort: 6379 selector: app: redis-with-pba  先执行aof的yaml文件，kubectl apply -f aof.yaml，然后再执行memtier_benchmark  memtier_benchmark -s 172.30.35.9 -p 30379 -R -d 1024 --key-maximum=1000000 -n 10000 --ratio=1:9 | grep Totals\u0026gt;\u0026gt;aof_1024 Writing results to stdout [RUN #1] Preparing benchmark client... [RUN #1] Launching threads now... [RUN #1 100%, 37 secs] 0 threads: 2000000 ops, 53289 (avg: 53544) ops/sec, 7.23MB/sec (avg: 7.29MB/sec), 3.75 (avg: 3.73) msec latency [root@neonsan-10 scripts]# cat aof_1024 Totals 53508.29 26.75 48130.71 3.73350 3.27900 8.31900 18.43100 7456.87  先执行pba的yaml文件，kubectl apply -f pba.yaml，然后再执行memtier_benchmark,注意yaml文件里面同时挂载不同存储。  memtier_benchmark -s 172.30.35.9 -p 31379 -R -d 1024 --key-maximum=1000000 -n 10000 --ratio=1:9 | grep Totals\u0026gt;\u0026gt;pba_1024 Writing results to stdout [RUN #1] Preparing benchmark client... [RUN #1] Launching threads now... [RUN #1 100%, 24 secs] 0 threads: 2000000 ops, 81619 (avg: 81294) ops/sec, 11.07MB/sec (avg: 11.10MB/sec), 2.45 (avg: 2.46) msec latency [root@neonsan-10 scripts]# cat pba_1024 Totals 0.00 0.00 0.00 -nan 0.00000 0.00000 0.00000 0.00 Totals 82856.35 82.86 74487.86 2.45929 2.38300 4.79900 6.30300 11588.37  通过速度和延迟性比较两种存储的性能。 ","date":"2020-09-23T00:00:00Z","permalink":"https://Forest-L.github.io/post/pmem-versus-block-storage-performance/","section":"post","tags":["ceph","pmem","k8s"],"title":"Pmem与块存储性能对比"},{"categories":null,"contents":"CKA考试经验 CKA: Kubernetes管理员认证（CKA）旨在确保认证持有者具备履行Kubernetes管理员职责的技能，知识和能力。如果企业想要申请 KCSP ，条件之一是：至少需要三名员工拥有CKA认证。 1. 考试报名  CKA报名地址 注意事项：  1、报名成功之后，可在12个月之内进行考试，考试不过有一次补考机会。\n2、CKA：74分或以上可以获得证书。\n3、每年Cyber Monday（网络星期一，也就是黑五后的第一个星期一）有优惠或者某些辅导架构有优惠券。\n2. 备考  考试时你可以打开两个浏览器Tab，一个是考试窗口，一个用来查阅官方文档（仅允许访问https://kubernetes.io/docs/、https://github.com/kubernetes/ 和https://kubernetes.io/blog/ ） 查询文档的浏览器Tab可以弄成标签。 考试前一周看下官网k8s版本，然后部署一个同样版本的K8s练习。 可以参考考试大纲复习。 怎么复习，可以在自己搭建的环境下，操作指令，生成对应的。  3. 考前检查及考试环境  考试形式: 在线监控，需要共享桌面和摄像头 考试环境: 在一个密闭空间，例如书房、卧室、会议室等，电脑屏幕不能对着窗户，房间里除了考生不能存在第二个人，考试的桌面不能放其它东西，水杯也不行 考试时间及题目: CKA-3小时-24道题 选择考试时间: 报名成功之后可以在12个月之内进行考试，考试之前需要选择考试时间，选择考试时间的时候记得先选择北京时区，默认是0时区时间。 电脑要求: 可以在这里WebDelivery Compatibility Check检测自己的电脑环境和网络速度等 选择的是Linux-Foundation\u0026mdash;\u0026gt;CKA-English 考试前考官检查: 考试可以提前15分钟进入考试界面 考官会以发消息的方式和你交流（没有语音交流） 看不懂考官发的英文怎么办：可以在chrome浏览器右键翻译 考官会让你共享摄像头，共享桌面 考官会让你出示能确认你身份ID的证件，我当时用的是罗技C310摄像头，无法对焦，护照看上去模糊到不行，后来考官又叫我给护照打光还是不行，后面又叫我打开我的手机，用手机相机当作放大镜用，这样才能看清楚。（我考CKAD的时候，我护照还没举稳，考官就说可以了，应该是考过CKA，他们系统里面已经有我的信息了，就随便瞄了一眼而已） 考官会让你用摄像头环视房间一周，确认你的考试环境（当时我房间门开了一个小缝也要求我去把门关好，还是比较严格） 考官会让你用摄像头看你的整个桌面和桌子底下 考官会让你打开任务管理器，点击左下角简略信息，是否已关闭了其它后台服务。 考官会让你再次点一下桌面共享，然后你叫你点击取消，然后就开始进入考试了 考试的界面: 左边是题目 右边是终端 终端上面是共享摄像头、共享屏幕、考试信息等按钮（可以唤出记事本）  4. 考试心得  有Notepad记事本，可以记录下自己环境信息，哪道题还没做，题目中的信息等。 一定要记得用鼠标，拷贝和粘贴特别方便。 尽量在官网中拷贝yaml文件到答题环境中。 很多指令记得不清楚，请使用-h，比如etcdctl,node不能调度等。 特别重要，根据个人考试，然后在浏览器中收藏的记录为：  ","date":"2020-07-11T00:00:00Z","permalink":"https://Forest-L.github.io/post/cka-test-experience/","section":"post","tags":["K8s","CKA"],"title":"CkA考试经验"},{"categories":null,"contents":"背景： docker run 某个容器，忘记了-p/-P 映射端口操作时，怎么把容器端口映射到主机上呢？以下描述的是如何通过iptables指令把容器端口映射到外部宿主机端口操作，防止容器重新创建。 宿主机docker启了一个容器，在容器里面又部署了一个pod，而部署pod这个服务是后续操作的，宿主机docker启容器时没有把端口映射出来，如何通过宿主机去访问pod服务。\n1、相关联的认知  docker run -p: 指令中的小p为具体的宿主机端口映射到容器内部开放的网络端口上。 docker run -P: 指令中的大P为随机选择一个宿主机端口映射到容器内部开放的网络端口上。 docker run -p: 可以绑定多IP和端口（跟多个-p）。 kubectl expose \u0026ndash;type=nodePort: 指令将容器内部端口映射到主机上(宿主机为随机端口，范围30000-32767/在service中编辑修改为具体端口)。 kubectl expose \u0026ndash;type=lb: 指令，为直接将容器服务暴露出去。 kubectl ingress: 它允许你基于路径或者子域名来路由流量到后端服务,7层协议http/https。 kubectl port-forward: 将容器端口转发至本地端口，也可以转发TCP流量。 kubectl kube-proxy: 只能转发HTTP流量。  2、docker与iptables关系 源地址变换规则、目标地址变换规则、自定义限制外部ip规则、docker容器间通信iptables规则、docker网络与ip-forward和具体的用iptables指令将容器内部端口映射到外部宿主机端口操作指令。\n2.1、源ip地址变换规则  Docker安装完成后，将默认在宿主机系统上增加一些iptables规则，以用于Docker容器和容器之间以及和外界的通信，可以使用iptables-save命令查看。 其中nat表中的POSTROUTING链有这么一条规则  -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE 参数说明： -s ：源地址172.17.0.0/16 -o：指定数据报文流出接口为docker0 -j ：动作为MASQUERADE（地址伪装）  上面这条规则关系着Docker容器和外界的通信，含义是： 判断源地址为172.17.0.0/16的数据包（即Docker容器发出的数据），当不是从docker0网卡发出时做SNAT（源地址转换）。 这样一来，从Docker容器访问外网的流量，在外部看来就是从宿主机上发出的，外部感觉不到Docker容器的存在。  2.2、目标地址变换规则  从Docker容器访问外网的流量，在外部看来就是从宿主机上发出的，外部感觉不到Docker容器的存在。那么，外界想到访问Docker容器的服务时，同样需要相应的iptables规则. 以启动tomcat容器，将其8080端口映射到宿主机上的8080端口为例,然后通过iptables-save查看：  docker run -itd --name tomcat -p 8080:8080 tomcat:latest #iptables-save *nat -A POSTROUTING -s 172.18.0.2/32 -d 172.18.0.2/32 -p tcp -m tcp --dport 8080 -j MASQUERADE ... *filter -A DOCKER -d 172.18.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 8080 -j ACCEPT  可以看到，在nat、filter的Docker链中分别增加了一条规则 这两条规则将访问宿主机8080端口的流量转发到了172.17.0.4的8080端口上（即真正提供服务的Docker容器IP和端口）。所以外界访问Docker容器是通过iptables做DNAT（目的地址转换）实现的。  2.3、自定义限制外部ip规则  Docker的forward规则默认允许所有的外部IP访问容器 可以通过在filter的DOCKER链上添加规则来对外部的IP访问做出限制 只允许源IP192.168.0.0/16的数据包访问容器，需要添加如下规则： iptables -I DOCKER -i docker0 ! -s 192.168.0.0/16 -j DROP  2.4、docker容器间通信iptables规则  不仅仅是与外界间通信，Docker容器之间互个通信也受到iptables规则限制。 同一台宿主机上的Docker容器默认都连在docker0网桥上，它们属于一个子网，这是满足相互通信的第一步。 Docker daemon启动参数\u0026ndash;icc(icc参数表示是否允许容器间相互通信)设置为false时会在filter的FORWARD链中增加一条ACCEPT的规则（\u0026ndash;icc=true）： -A FORWARD -i docker0 -o docker0 -j ACCEPT 当Docker datemon启动参数\u0026ndash;icc设置为false时，以上规则会被设置为DROP，Docker容器间的相互通信就被禁止,默认是ACCEPT。 这种情况下，想让两个容器通信就需要在docker run时使用\u0026ndash;link选项。  2.5、docker网络与ip-forward  在Docker容器和外界通信的过程中，还涉及了数据包在多个网卡间的转发，如从docker0网卡转发到宿主机ens160网卡，这需要内核将ip-forward功能打开 即将ip_forward系统参数设1：echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward Docker daemon启动的时候默认会将其设为1（\u0026ndash;ip-forward=true） 永久生效的ip转发 vim /etc/sysctl.conf net.ipv4.ip_forward = 1 sysctl -p /etc/sysctl.conf  2.6、iptables指令映射  需要执行三条指令,其中就修改两个参数:  iptables -t nat -A DOCKER -p tcp --dport ${YOURPORT} -j DNAT --to-destination ${CONTAINERIP}:${YOURPORT} iptables -t nat -A POSTROUTING -j MASQUERADE -p tcp --source ${CONTAINERIP} --destination ${CONTAINERIP} --dport ${YOURPORT} iptables -A DOCKER -j ACCEPT -p tcp --destination ${CONTAINERIP} --dport ${YOURPORT}  ${CONTAINERIP} 就是对应容器的ip地址，比如我的容器ip地址是 172.18.0.2 ，（容器的IP可以通过如下方式查看：a.在容器中：ip addr;b.在宿主机中: docker inspect 容器名 |grep IPAddress ）所以我就把上述的参数换成我的IP地址。 ${YOURPORT} 就是要映射出来的端口，我配置的是一个console平台，其端口是30880  3、注意地方及参考  如果容器是pod形式启的，上面iptables指令映射不适合，其中有对docker链的操作。 映射port至存在的docker容器 k8s如何访问 ","date":"2020-07-07T00:00:00Z","permalink":"https://Forest-L.github.io/post/the-iptables-directive-is-a-guide-to-mapping-ports-inside-containers-to-ports-on-external-hosts/","section":"post","tags":["iptables","Linux tools"],"title":"iptables指令将容器内部端口映射到外部宿主机端口指南"},{"categories":null,"contents":"chart包文件结构，以wordpress包为例 wordpress/ Chart.yaml # 包含有关chart信息的YAML文件 LICENSE # OPTIONAL: 包含chart许可证的纯文本文件 README.md # OPTIONAL: 一个可读的README文件 requirements.yaml # OPTIONAL: 一个YAML文件，列出了chart的依赖关系 values.yaml # 该chart的默认配置值 charts/ # OPTIONAL: 包含此chart所依赖的任何chart的目录。 templates/ # OPTIONAL: 一个模板目录，当与values相结合时， # 将生成有效的Kubernetes清单文件 templates/NOTES.txt # OPTIONAL: 包含简短使用说明的纯文本文件 templates/_helpers.tpl # OPTIONAL:通过define 函数定义命名模板 crds/ # OPTIONAL: 自定义资源 以下说明chart包内容 Chart.yaml文件内容格式 name: chart的名称 (required) version: 一个SemVer 2(语义化版本)版本(required) description: 这个项目的单句描述 (optional) keywords: - 关于此项目的关键字列表 (optional) home: 该项目主页的URL(optional) sources: - 此项目的源代码URL列表 (optional) dependencies: chart依赖关系 (optional) - name: chart名字 (nginx) version: chart版本 (\u0026quot;1.2.3\u0026quot;) repository: url仓库 (\u0026quot;https://example.com/charts\u0026quot;) condition: (optional) 布尔值的yaml路径，用于启用/禁用图表 maintainers: # (optional) - name: 维护者的名称 (每个维护者都需要) email: 维护者的email (optional for each maintainer) url: 维护者的url (optional for each maintainer) engine: gotpl＃模板引擎的名称（可选，默认为gotpl） icon: 要用作图标的SVG或PNG图像的URL (optional) appVersion: 包含的应用程序版本（可选）这个不一定是SemVer deprecated: 此chart是否已被弃用（可选，布尔型） README.md内容  Introduction Prerequisites Installing the Chart Uninstalling the Chart Configuration  requirements.yaml介绍 在Helm中，一个chart可能取决于任何数量的其他chart。 这些依赖关系可以通过requirements.yaml文件动态链接，或者引入charts/目录并手动管理。\ndependencies: - name: apache version: 1.2.3 repository: http://example.com/charts alias: new-subchart-1  Name: 你想要的chart的名称。 Version: 你想要的chart的版本。 repository字段是图表存储库的完整URL。 请注意，您还必须使用helm repo add在本地添加该repository。 alias：别名。 一旦有一个依赖关系文件，可以运行helm dependency update，它会使用你的依赖关系文件为你下载所有指定的chart到你的charts/目录中。 可以在values.yaml定义true/false判断依赖包是否被启用，如  apache: enabled: true 依赖关系可以是chart压缩包（foo-1.2.3.tgz），也可以是未打包的chart目录。 依赖执行顺序：参考k8s负载自启动原理，所以我们可以不关心执行顺利。实际上交叉执行。\n说明：helm2是通过requirements.yaml文件描述依赖关系，helm3直接在Chart.yaml描述。 templates/k8s资源 templates下有多个deployment对象，可以命名不同名字。 执行顺序：参考k8s负载自启动原理，所以我们可以不关心执行顺利。 实际执行顺序为：\nvar InstallOrder KindSortOrder = []string{ \u0026quot;Namespace\u0026quot;, \u0026quot;NetworkPolicy\u0026quot;, \u0026quot;ResourceQuota\u0026quot;, \u0026quot;LimitRange\u0026quot;, \u0026quot;PodSecurityPolicy\u0026quot;, \u0026quot;PodDisruptionBudget\u0026quot;, \u0026quot;Secret\u0026quot;, \u0026quot;ConfigMap\u0026quot;, \u0026quot;StorageClass\u0026quot;, \u0026quot;PersistentVolume\u0026quot;, \u0026quot;PersistentVolumeClaim\u0026quot;, \u0026quot;ServiceAccount\u0026quot;, \u0026quot;CustomResourceDefinition\u0026quot;, \u0026quot;ClusterRole\u0026quot;, \u0026quot;ClusterRoleList\u0026quot;, \u0026quot;ClusterRoleBinding\u0026quot;, \u0026quot;ClusterRoleBindingList\u0026quot;, \u0026quot;Role\u0026quot;, \u0026quot;RoleList\u0026quot;, \u0026quot;RoleBinding\u0026quot;, \u0026quot;RoleBindingList\u0026quot;, \u0026quot;Service\u0026quot;, \u0026quot;DaemonSet\u0026quot;, \u0026quot;Pod\u0026quot;, \u0026quot;ReplicationController\u0026quot;, \u0026quot;ReplicaSet\u0026quot;, \u0026quot;Deployment\u0026quot;, \u0026quot;HorizontalPodAutoscaler\u0026quot;, \u0026quot;StatefulSet\u0026quot;, \u0026quot;Job\u0026quot;, \u0026quot;CronJob\u0026quot;, \u0026quot;Ingress\u0026quot;, \u0026quot;APIService\u0026quot;, }  两种方式可以提前执行,一种设置pre-install,另一种是设置权重： pre-install hooks，如：  apiVersion: v1 kind: Service metadata: name: foo annotations: \u0026quot;helm.sh/hook\u0026quot;: \u0026quot;pre-install\u0026quot;  定义权重，如：  annotations: \u0026quot;helm.sh/hook-weight\u0026quot;: \u0026quot;5\u0026quot; values.yaml  Release.Name: release的名称(不是chart的名称！) Release.Namespace: chart release的名称空间。 Release.Service: 进行release的服务。 通常这是Tiller。 chart版本可以作为Chart.Version获得。Chart：Chart.yaml 的内容。 templates下有多个deployment对象，可以命名不同名字，然后在values.yaml以不同名字打头定义值。，如以下格式定义：  mysql: name: image: repository: tag: pullPolicy: redis: name: image: repository: tag: pullPolicy: helm模板 helm模板语法嵌套在{{和}}之间，有三个常见的\n.Values.* 从value.yaml文件中读取或者--set获取（--set优先级最大）。 .Release.* 从运行Release的元数据读取,每次安装均会生成一个新的release template * . 从_helpers.tpl文件中读取，通过define 函数定义命名模板 .Chart.* 从Chart.yaml文件中读取 模板函数和管道 * | 管道，类似linux下的管道，以下实例效果是一样的。 {{ quote .Values.favorite.drink }}与 {{ .Values.favorite.drink | quote }} * default制定默认值 drink: {{ .Values.favorite.drink | default “tea” | quote }} * indent 模板函数，对左空出空格，左边空出两个空格 {{ include \u0026quot;mychart_app\u0026quot; . | indent 2 }} include 函数，与 template 类似功能 如实例，在_helpers.tpl中define模板，在资源对象中引用。 {{- define \u0026quot;mychart.labels\u0026quot; }} labels: generator: helm date: {{ now | htmlDate }} {{- end }} apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap {{- template \u0026quot;mychart.labels\u0026quot; }} data: 在模板中使用文件 apiVersion: v1 kind: ConfigMap metadata: name: conf data: {{ (.Files.Glob \u0026quot;foo/*\u0026quot;).AsConfig | indent 2 }} chart根目录下foo目录的所有文件配置为configmap的内容\n模板流程控制 常用的有 if/else 条件控制 with 范围控制 range 循环控制 如：values.yaml中定义变量，ConfigMap中.Values.favorite循环控制参数。\nfavorite: drink: coffee food: pizza apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap data: myvalue: \u0026quot;Hello World\u0026quot; {{- range $key, $val := .Values.favorite }} {{ $key }}: {{ $val | quote }} {{- end}} 在deployment.yaml文件中使用if/else语法，如：- end结束标志，双括号都有“-”。\n{{- if .Values.image.repository -}} image: {.Values.image.repository} {{- else -}} image: \u0026quot;***/{{ .Release.Name }}:{{ .Values.image.version }}\u0026quot; {{- end -}} ","date":"2020-04-15T00:00:00Z","permalink":"https://Forest-L.github.io/post/chart-package-practice-development-guide/","section":"post","tags":["helm","chart"],"title":"chart包实践开发指南"},{"categories":null,"contents":"ipv6地址搭建K8s 环境 centos: 7.7 k8s: v1.16.0\n提前准备  修改主机名 hostnamectl set-hostname node1 添加ipv6地址及主机名 vi /etc/hosts 添加操作系统的ipv6的参数，且使参数生效sysctl -p  vi /etc/sysctl.conf net.ipv6.conf.all.disable_ipv6 = 0 net.ipv6.conf.default.disable_ipv6 = 0 net.ipv6.conf.lo.disable_ipv6 = 0 net.ipv6.conf.all.forwarding=1  开启ipv6,添加如下内容  vi /etc/sysconfig/network NETWORKING_IPV6=yes  开启网卡的ipv6,添加如下内容，最后执行reboot生效。  vi /etc/sysconfig/network-scripts/ifcfg-eth0 IPV6INIT=yes IPV6_AUTOCONF=yes  关闭防火墙  systemctl stop firewalld systemctl disable firewalld setenforce 0 vi /etc/selinux/config SELINUX=disabled  关闭虚拟内存,添加如下内容，最后通过执行sysctl -p /etc/sysctl.d/k8s.conf生效。  swapoff -a vi /etc/sysctl.d/k8s.conf 添加下面一行： vm.swappiness=0 安装docker yum install -y yum-utils device-mapper-persistent-data lvm2 yum install wget -y wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo sudo sed -i 's+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+' /etc/yum.repos.d/docker-ce.repo yum makecache fast yum install docker-ce -y systemctl enable docker;systemctl restart docker docker的配置vi /etc/docker/daemon.json { \u0026quot;insecure-registry\u0026quot;:[\u0026quot;0.0.0.0/0\u0026quot;], \u0026quot;ipv6\u0026quot;: true, \u0026quot;fixed-cidr-v6\u0026quot;: \u0026quot;2001:db8:1::/64\u0026quot;, \u0026quot;host\u0026quot;:[\u0026quot;unix:///var/run/docker.sock\u0026quot;,\u0026quot;tcp://:::2375\u0026quot;], \u0026quot;log-level\u0026quot;:\u0026quot;debug\u0026quot; } systemctl restart docker echo \u0026quot;1\u0026quot; \u0026gt;/proc/sys/net/bridge/bridge-nf-call-iptables 安装kubectl、kubeadm和kubelet插件 添加k8s下载源 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 指定版本的安装 yum install kubelet-1.16.0 kubeadm-1.16.0 kubectl-1.16.0 -y systemctl enable kubelet \u0026amp;\u0026amp; sudo systemctl start kubelet 初始化的准备  查看安装过程需要哪些镜像 kubeadm config images list --kubernetes-version=v1.16.0 通过脚本下载所需的镜像。  vi images.sh #!/bin/bash images=(kube-proxy:v1.16.0 kube-scheduler:v1.16.0 kube-controller-manager:v1.16.0 kube-apiserver:v1.16.0 etcd:3.3.15-0 pause:3.1 coredns:1.6.2) for imageName in ${images[@]} ; do docker pull gcr.azk8s.cn/google-containers/$imageName docker tag gcr.azk8s.cn/google-containers/$imageName k8s.gcr.io/$imageName docker rmi gcr.azk8s.cn/google-containers/$imageName done   执行如下指令下载：chmod +x images.sh \u0026amp;\u0026amp; ./images.sh\n  拷贝kubeadm.yaml文件，需要注意advertiseAddress参数为本机ipv6地址\n  vi kubeadm.yaml apiVersion: kubeadm.k8s.io/v1beta2 kind: InitConfiguration localAPIEndpoint: advertiseAddress: \u0026quot;2402:e7c0:0:a00:ffff:ffff:fffe:fffb\u0026quot; bindPort: 6443 nodeRegistration: taints: - effect: PreferNoSchedule key: node-role.kubernetes.io/master --- apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.16.0 networking: podSubnet: 1100::/52 serviceSubnet: fd00:4000::/112  执行如下安装指令：kubeadm init --config=kubeadm.yaml 如果使1.16.0之前版本需要安装指令后面添加如下参数执行：--ignore-preflight-errors=HTTPProxy 如果以下安装有问题，需要重置，先执行kubeadm reset,再执行以上kubeadm init指令，安装成功之后，需要做如下操作：  mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config kubectl taint node node1 node-role.kubernetes.io/master- 安装网络插件，如calico  下载calico.yaml文件  curl https://docs.projectcalico.org/v3.11/manifests/calico.yaml -O 需要修改及添加的内容为,总共三处： \u0026quot;ipam\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;calico-ipam\u0026quot;, \u0026quot;assign_ipv4\u0026quot;: \u0026quot;false\u0026quot;, \u0026quot;assign_ipv6\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;ipv4_pools\u0026quot;: [\u0026quot;172.16.0.0/16\u0026quot;, \u0026quot;default-ipv4-ippool\u0026quot;], \u0026quot;ipv6_pools\u0026quot;: [\u0026quot;1100::/52\u0026quot;, \u0026quot;default-ipv6-ippool\u0026quot;] }, - name: CALICO_IPV4POOL_CIDR value: \u0026quot;172.16.0.0/16\u0026quot; - name: IP6 value: \u0026quot;autodetect\u0026quot; - name: CALICO_IPV6POOL_CIDR value: \u0026quot;1100::/52\u0026quot; # Disable IPv6 on Kubernetes. - name: FELIX_IPV6SUPPORT value: \u0026quot;true\u0026quot;  calico的镜像,可以提前下载 calico/cni:v3.11.1 calico/pod2daemon-flexvol:v3.11.1 calico/node:v3.11.1 calico/kube-controllers:v3.11.1 执行calico，kubectl apply -f calico.yaml  验证： kubectl get pod --all-namespaces -o wide kubectl get nodes -o wide\n 部署tomcat应用验证  kubectl run tomcat --image=tomcat:8.0 --port=8080 kubectl get pod kubectl expose deployment tomcat --port=8080 --target-port=8080 --type=NodePort # kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP fd00:4000::1 \u0026lt;none\u0026gt; 443/TCP 33m tomcat NodePort fd00:4000::bf3e \u0026lt;none\u0026gt; 8080:30693/TCP 22m curl -6g [2402:e7c0:0:a00:ffff:ffff:fffe:fffb]:32012 参考 https://www.kubernetes.org.cn/5173.html\n","date":"2020-01-14T00:00:00Z","permalink":"https://Forest-L.github.io/post/ipv6-address-setup-k8s/","section":"post","tags":["iptables","Linux tools"],"title":"ipv6地址搭建K8s"},{"categories":["storage"],"contents":"NetApp是向目前的数据密集型企业提供统一存储解决方案的居世界最前列的公司，其 Data ONTAP是全球首屈一指的存储操作系统。NetApp 的存储解决方案涵盖了专业化的硬件、软件和服务，为开放网络环境提供了无缝的存储管理。\nOntap数据管理软件支持高速闪存、低成本旋转介质和基于云的对象存储等存储配置，为通过块或文件访问协议读写数据的应用程序提供统一存储。 Trident是一个由NetApp维护的完全支持的开源项目。以帮助您满足容器化应用程序的复杂持久性需求。 KubeSphere 是一款开源项目，在目前主流容器调度平台 Kubernetes 之上构建的企业级分布式多租户容器管理平台，提供简单易用的操作界面以及向导式操作方式，在降低用户使用容器调度平台学习成本的同时，极大降低开发、测试、运维的日常工作的复杂度。\n1、整体方案 在VMware Workstation环境下安装ONTAP;ONTAP系统上创建SVM(Storage Virtual Machine)且对接nfs协议；在已有k8s环境下部署Trident,Trident将使用ONTAP系统上提供的信息（svm、managementLIF和dataLIF）作为后端来提供卷；在已创建的k8s和StorageClass卷下部署kubesphere。\n2、版本信息 Ontap: 9.5 Trident: v19.07 k8s: 1.15 kubesphere: 2.0.2\n3、步骤 主要描述ontap搭建及配置、Trident搭建和配置和kubesphere搭建及配置等方面。参考ontap搭建\n3.1 ontap搭建及配置 在VMware Workstation上Simulate_ONTAP_9.5P6_Installation_and_Setup_Guide运行，ontap启动之后，按下面操作配置，其中以cluster base license、feature licenses for the non-ESX build配置证书、e0c、ip address：192.168.*.20、netmask:255.255.255.0、集群名:cluster1、密码等信息。 https://IP address,以上设置的iP地址，用户名和密码： 3.2 Trident搭建及配置  下载安装包trident-installer-19.07.0.tar.gz，解压进入trident-installer目录，执行trident安装指令: ./tridentctl install -n trident 结合ontap的提供的参数创建第一个后端vi backend.json。  { \u0026quot;version\u0026quot;: 1, \u0026quot;storageDriverName\u0026quot;: \u0026quot;ontap-nas\u0026quot;, \u0026quot;backendName\u0026quot;: \u0026quot;customBackendName\u0026quot;, \u0026quot;managementLIF\u0026quot;: \u0026quot;10.0.0.1\u0026quot;, \u0026quot;dataLIF\u0026quot;: \u0026quot;10.0.0.2\u0026quot;, \u0026quot;svm\u0026quot;: \u0026quot;trident_svm\u0026quot;, \u0026quot;username\u0026quot;: \u0026quot;cluster-admin\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;password\u0026quot; } 生成后端卷./tridentctl -n trident create backend -f backend.json\n 创建StorageClass,vi storage-class-ontapnas.yaml  apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ontapnasudp provisioner: netapp.io/trident mountOptions: [\u0026quot;rw\u0026quot;, \u0026quot;nfsvers=3\u0026quot;, \u0026quot;proto=udp\u0026quot;] parameters: backendType: \u0026quot;ontap-nas\u0026quot; 创建StorageClass指令kubectl create -f storage-class-ontapnas.yaml\n3.3 kubesphere的安装及配置  在 Kubernetes 集群中创建名为 kubesphere-system 和 kubesphere-monitoring-system 的 namespace。  $ cat \u0026lt;\u0026lt;EOF | kubectl create -f - --- apiVersion: v1 kind: Namespace metadata: name: kubesphere-system --- apiVersion: v1 kind: Namespace metadata: name: kubesphere-monitoring-system EOF  创建 Kubernetes 集群 CA 证书的 Secret。  $ kubectl -n kubesphere-system create secret generic kubesphere-ca \\ --from-file=ca.crt=/etc/kubernetes/pki/ca.crt \\ --from-file=ca.key=/etc/kubernetes/pki/ca.key  若 etcd 已经配置过证书，则参考如下创建（以下命令适用于 Kubeadm 创建的 Kubernetes 集群环境）：  $ kubectl -n kubesphere-monitoring-system create secret generic kube-etcd-client-certs \\ --from-file=etcd-client-ca.crt=/etc/kubernetes/pki/etcd/ca.crt \\ --from-file=etcd-client.crt=/etc/kubernetes/pki/etcd/healthcheck-client.crt \\ --from-file=etcd-client.key=/etc/kubernetes/pki/etcd/healthcheck-client.key   修改kubesphere.yaml中存储的设置参数和对应的参数即可 kubectl apply -f kubesphere.yaml\n  访问 KubeSphere UI 界面。\n  参考文档 http://docs.netapp.com/ontap-9/index.jsp?lang=zh_CN https://netapp-trident.readthedocs.io/en/stable-v19.07/introduction.html https://github.com/kubesphere/ks-installer/blob/master/README_zh.md\n","date":"2019-12-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/netapp-stored-on-kubesphere-practice/","section":"post","tags":["netapp","trident","k8s"],"title":"netapp存储在kubesphere上的实践"},{"categories":["storage"],"contents":"netapp联合k8s在kubesphere应用 配置说明  k8s: 1.13+ ontap: 9.5 trident: v19.07 kubesphere: 2.0.2  前期准备 需要准备材料在以下链接上，链接：https://pan.baidu.com/s/1q3KugGrz-XWJzqhgD7Ze9g 提取码：rhyw\n包括ontap的安装说明，Workstation上ontap9.5模拟器的ova，ontap使用文档，对接各种存储的协议证书，\n1. ontap的安装 本次测试的环境是安装在VMware Workstation，具体参考链接上这个文档Simulate_ONTAP_9.5P6_Installation_and_Setup_Guide，大致流程为：\n window机器的资源配置 开启VT 为模拟ONTAP配置VMware Workstation 在VMware Workstation上配置网络适配器，选择bridge网络 开启模拟ONTAP  2. 模拟ontap的配置 以上ontap启动之后，按下面操作配置，其中以cluster base license、feature licenses for the non-ESX build配置证书、e0c、ip address：192.168.*.20、netmask:255.255.255.0、集群名:cluster1、密码等信息。\n 2.1 Press Ctrl-C for Boot 菜单消息显示时，按 Ctrl-C  md1.uzip: 39168 x 16384 blocks md2.uzip: 5760 x 16384 blocks ******************************* * * * Press Ctrl-C for Boot Menu. * * * ******************************* ^C Boot Menu will be available.  2.2 选择4配置  Please choose one of the following: (1) Normal Boot. (2) Boot without /etc/rc. (3) Change password. (4) Clean configuration and initialize all disks. (5) Maintenance mode boot. (6) Update flash from backup config. (7) Install new software first. (8) Reboot node. Selection (1-8)? 4  2.3 确认reset 和 确定  Zero disks, reset config and install a new file system?: y This will erase all the data on the disks, are you sure?: y  2.4 创建集群,填写参数  Enter the cluster management interface port [e0d]: e0c Enter the cluster management interface IP address: 192.168.x.20 Enter the cluster management interface netmask: 255.255.255.0 Enter the cluster management interface default gateway: \u0026lt;Enter\u0026gt; A cluster management interface on port e0c with IP address 192.168.x. 20 has been created. You can use this address to connect to and manager the cluster. Do you want to create a new cluster or join an existing cluster? {create}: create Enter the cluster name: cluster1 login: admin Password: \u0026lt;password you defined\u0026gt; Enter the cluster base license key:SMKQROWJNQYQSDAAAAAAAAAAAAAA 3. 登录ontap界面 参考链接中的m_SL10537_gui_nas_basic_concepts_v2.1.0文档，这里需要配置的信息为，对接各个存储的协议证书、子网的设置、聚合的创建、svm创建和导出策略的配置。\n  https://IP address,以上设置的iP地址，用户名和密码：   对接各个存储的协议证书 登录平台之后，配置\u0026ndash;》许可证\u0026ndash;》添加对应的证书，显示为绿色的勾就添加正确。\n  子网的设置 登录平台，网络\u0026ndash;》子网\u0026ndash;》创建\n  聚合的创建 登录平台，存储\u0026ndash;》聚合和磁盘\u0026ndash;》聚合\u0026ndash;》创建\n  svm的创建，此处需要选择对应的存储协议、 存储中的聚合和权限、管理(LIF)和数据（LIF）等信息。 登录平台，存储\u0026ndash;》SVM\u0026ndash;》创建\n  导出策略的设置，svm创建之后，点击svm设置\u0026ndash;》导出策略，在规则索引下添加客户端规范0.0.0.0/0，协议和权限。\n  4. trident安装部署 介质在链接中，包括所需要的镜像和trident安装包和想要的配置文件。\n 如果多台机器情形，需要在每台机器上执行docker load -i trident.tar 解压trident安装包，tar -xf $BASE_FOLDER/trident-installer-19.07.0.tar.gz 进入trident-installer目录，执行trident安装指令：./tridentctl install -n trident 检查是否安装成功kubectl get pod -n trident 创建并验证第一个后端,注意backend.json填写正确的ontap参数， ./tridentctl -n trident create backend -f backend.json 验证后端是否生成：./tridentctl -n trident get backend 创建storage class：kubectl create -f sample-input/storage-class-basic.yaml  5. kubesphere安装部署 参考官方部署文档为：https://github.com/kubesphere/ks-installer\n","date":"2019-12-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/netapp-works-with-k8s-in-kubesphere/","section":"post","tags":["netApp","ontap","trident"],"title":"netapp联合k8s在kubesphere应用"},{"categories":["storage"],"contents":"#Kubesphere基于Velero做集群的迁移 使用Velero 快速完成云原生应用迁移至备份集群中。\n环境信息 集群A（生产）： master：192.168.11.6、192.168.11.13、192.168.11.16 lb：192.168.11.252 node：192.168.11.22 nfs：192.168.11.14 集群B（容灾）： master：192.168.11.8、192.168.11.10、192.168.11.17 lb：192.168.11.253 node：192.168.11.18 nfs：192.168.11.14\nVelero安装部署 集群A和集群B都需要安装velero，安装过程参考官方文档velero安装,大致流程为：\n1、安装velero客户端安装包，A和B集群都需要。 1.1、wget https://github.com/vmware-tanzu/velero/releases/download/v1.0.0/velero-v1.0.0-linux-amd64.tar.gz 1.2、解压安装包，且将velero拷贝至/usr/local/bin目录下。 2、安装velero服务端，A和B集群都需要。 2.1、本地创建密钥文件，vi credentials-velero [default] aws_access_key_id = minio aws_secret_access_key = minio123 2.2、集群B环境，运下载和运行00-minio-deployment.yaml文件，且需要将其中的ClusterIP改成NodePort，添加nodePort: 31860，集群A环境不需要执行这步。 kubectl apply -f examples/minio/00-minio-deployment.yaml 2.3、集群B环境，启动服务端,需要在密钥文件同级目录下执行： velero install \\ --provider aws \\ --bucket velero \\ --secret-file ./credentials-velero \\ --use-volume-snapshots=false \\ --backup-location-config region=minio,s3ForcePathStyle=\u0026quot;true\u0026quot;,s3Url=http://minio.velero.svc:9000 \\ --plugins velero/velero-plugin-for-aws:v1.0.0 2.4、集群A环境，启动服务端，注意：需要在集群A中获取velero的外部curl： 2.4.1、集群A中，kubectl get svc -n velero,获取9000映射的端口，如：9000:31860，根据情况而定 2.4.2、启动指令： velero install \\ --provider aws \\ --bucket velero \\ --secret-file ./credentials-velero \\ --use-volume-snapshots=false \\ --backup-location-config region=minio,s3ForcePathStyle=\u0026quot;true\u0026quot;,s3Url=http://192.168.11.8:31860 \\ --plugins velero/velero-plugin-for-aws:v1.0.0 集群A数据的备份及集群B恢复 具体备份指令，定时备份，参考官方文档备份指令 在集群A中模拟了带有持久化的有状态和无状态的应用，备份维度以namespace为基准，为test,将pv的模式改成retain形式。 备份指令为：velero backup create test-backup \u0026ndash;include-namespaces test 集群A所有的机器关机且在机器B恢复验证：\n[root@master1 velero-v1.2.0-linux-amd64]# velero restore create --from-backup test-backup Restore request \u0026quot;test-backup-20191121141336\u0026quot; submitted successfully. Run `velero restore describe test-backup-20191121141336` or `velero restore logs test-backup-20191121141336` for more details. [root@master1 velero-v1.2.0-linux-amd64]# kubectl get ns NAME STATUS AGE default Active 43h demo Active 42h kube-node-lease Active 43h kube-public Active 43h kube-system Active 43h kubesphere-controls-system Active 43h kubesphere-monitoring-system Active 43h kubesphere-system Active 43h openpitrix-system Active 23h test Active 12s velero Active 24m [root@master1 velero-v1.2.0-linux-amd64]# kubectl get pod -n test NAME READY STATUS RESTARTS AGE mysql-v1-0 1/1 Running 0 22s tomcattest-v1-554c8875cd-26fz4 1/1 Running 0 22s tomcattest-v1-554c8875cd-cmm2z 1/1 Running 0 22s tomcattest-v1-554c8875cd-dc7mr 1/1 Running 0 22s tomcattest-v1-554c8875cd-fcgn4 1/1 Running 0 22s tomcattest-v1-554c8875cd-wqb4t 1/1 Running 0 22s wordpress-v1-65d58448f8-g5bh8 1/1 Running 0 22s ","date":"2019-11-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/kubesphere-does-cluster-migration-based-on-velero/","section":"post","tags":["k8s","velero"],"title":"Kubesphere基于Velero做集群的迁移"},{"categories":null,"contents":"使用kubeadm安装k8s 1.15版本 k8s 1.15版本中，kubeadm对HA集群的配置已经达到了beta可用，这一版本更新主要是针对稳定性的持续改善和可扩展性。其中用到的镜像和rpm包在百度云上，链接如下。 https://pan.baidu.com/s/1LoKvv86Fs5ilZ-TYQdN35A cos3\n1.准备 1.1系统准备 需要将主机ip和主机名放在每台机器的vi /etc/hosts下 192.168.11.21 i-fahx5c7k 192.168.11.22 i-ouaaujhz 如果各个主机启用了防火墙，需要开启各个组件所需要的端口，可以查看https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ 这里各个节点禁用防火墙 systemctl stop firewalld systemctl disable firewalld 禁用selinux setenforce 0 vi /etc/selinux/config SELINUX=disabled 创建vi /etc/sysctl.d/k8s.conf文件，添加如下内容： net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 执行命令使修改生效 modprobe br_netfilter sysctl -p /etc/sysctl.d/k8s.conf\n1.2 kube-proxy开启ipvs的前置条件 在所有的节点上执行如下脚本：\ncat \u0026gt; /etc/sysconfig/modules/ipvs.modules \u0026lt;\u0026lt;EOF #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF chmod 755 /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; bash /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4 各个节点需要安装 ipset ipvsadm yum install ipset ipvsadm -y 1.3 docker安装 安装docker的yum源,国内寻找清华源 yum install wget -y yum install -y yum-utils device-mapper-persistent-data lvm2 wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo sed -i 's+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+' /etc/yum.repos.d/docker-ce.repo yum makecache fast yum install docker-ce -y 重启docker：systemctl enable docker;systemctl restart docker 修改docker cgroup driver为systemd 创建或修改vi /etc/docker/daemon.json：\n{ \u0026quot;exec-opts\u0026quot;: [\u0026quot;native.cgroupdriver=systemd\u0026quot;] } 重启docker:systemctl restart docker\n2. 使用kubeadm部署kubernetes 2.1 安装kubeadm和kubelet 下面在各节点安装kubeadm和kubelet和kubectl，请在百度云上面下载rpm包，在rmp目录下执行如下指令： yum install -y cri-tools-1.13.0-0.x86_64.rpm kubernetes-cni-0.7.5-0.x86_64.rpm kubelet-1.15.1-0.x86_64.rpm kubectl-1.15.1-0.x86_64.rpm kubeadm-1.15.1-0.x86_64.rpm  k8s 1.8开始要求关闭系统的swap,不关闭，kubelet将无法启动，执行指令方法： swapoff -a 修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载，使用free -m确认swap已经关闭。 swappiness参数调整，修改vi /etc/sysctl.d/k8s.conf添加下面一行： vm.swappiness=0 执行sysctl -p /etc/sysctl.d/k8s.conf使修改生效。 因为这里本次测试主机上还运行其他服务，关闭swap可能会对其他服务产生影响，所以这里修改kubelet的配置去掉这个限制。 修改vi /etc/sysconfig/kubelet，加入：KUBELET_EXTRA_ARGS=--fail-swap-on=false\n2.2 使用kubeadm init初始化集群 在各节点开机启动kubelet服务：systemctl enable kubelet 使用kubeadm config print init-defaults可以打印集群初始化默认的使用的配置:\napiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 1.2.3.4 bindPort: 6443 nodeRegistration: criSocket: /var/run/dockershim.sock name: node1 taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: k8s.gcr.io kind: ClusterConfiguration kubernetesVersion: v1.14.0 networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 scheduler: {} 从默认的配置中可以看到，可以使用imageRepository定制在集群初始化时拉取k8s所需镜像的地址。advertiseAddress的ip替换本机，基于默认配置定制出本次使用kubeadm初始化集群所需的配置文件且在11.21机器上vi kubeadm.yaml：\napiVersion: kubeadm.k8s.io/v1beta2 kind: InitConfiguration localAPIEndpoint: advertiseAddress: 192.168.11.21 bindPort: 6443 nodeRegistration: taints: - effect: PreferNoSchedule key: node-role.kubernetes.io/master --- apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.15.0 networking: podSubnet: 10.244.0.0/16 说 明 :  使用kubeadm默认配置初始化的集群，会在master节点打上node-role.kubernetes.io/master:NoSchedule的污点，阻止master节点接受调度运行工作负载。这里测试环境只有两个节点，所以将这个taint修改为node-role.kubernetes.io/master:PreferNoSchedule。\n在开始初始化集群之前，需要从百度云上面下载tar包镜像下来，tar通过scp分别传到各个节点执行docker load -i解压， 镜像列表:\nk8s.gcr.io/kube-proxy v1.15.0 d235b23c3570 5 weeks ago 82.4MB k8s.gcr.io/kube-apiserver v1.15.0 201c7a840312 5 weeks ago 207MB k8s.gcr.io/kube-scheduler v1.15.0 2d3813851e87 5 weeks ago 81.1MB k8s.gcr.io/kube-controller-manager v1.15.0 8328bb49b652 5 weeks ago 159MB gcr.io/kubernetes-helm/tiller v2.14.1 ac22eb1f780e 7 weeks ago 94.2MB quay.io/coreos/flannel v0.11.0-amd64 ff281650a721 6 months ago 52.6MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 8 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 接下来使用kubeadm初始化集群，选择node1作为Master Node，在node1上执行下面的命令：kubeadm init --config kubeadm.yaml --ignore-preflight-errors=Swap\nkubeadm join 192.168.11.21:6443 --token k6s0kn.nyao1ulwwmlm2org \\ --discovery-token-ca-cert-hash sha256:b721d25356668f921c90bf5f554836cb346b827a62b78797d0bcbcba24214725 \n其中关键步骤： * [kubelet-start] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml” * [certs]生成相关的各种证书 * [kubeconfig]生成相关的kubeconfig文件 * [control-plane]使用/etc/kubernetes/manifests目录中的yaml文件创建apiserver、controller-manager、scheduler的静态pod * [bootstraptoken]生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到 * 下面的命令是配置常规用户如何使用kubectl访问集群： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config * 最后给出了将节点加入集群的命令kubeadm join 192.168.99.11:6443 –token 4qcl2f.gtl3h8e5kjltuo0r \\ –discovery-token-ca-cert-hash sha256:7ed5404175cc0bf18dbfe53f19d4a35b1e3d40c19b10924275868ebf2a3bbe6e 需要在11.21机器上执行： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 查看集群状态，确认组件都处于healthy状态： kubectl get cs 集群初始化如果遇到问题，可以使用下面的命令进行清理： kubeadm reset\n2.3 安装Pod Network 接下来安装flannel network add-on： mkdir -p ~/k8s/ cd ~/k8s curl -O https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml kubectl apply -f kube-flannel.yml 这里注意kube-flannel.yml这个文件里的flannel的镜像是0.11.0，quay.io/coreos/flannel:v0.11.0-amd64 如果Node有多个网卡的话，参考https://github.com/kubernetes/kubernetes/issues/39701， 目前需要在kube-flannel.yml中使用–iface参数指定集群主机内网网卡的名称，否则可能会出现dns无法解析。需要将kube-flannel.yml下载到本地，flanneld启动参数加上–iface=\ncontainers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth1 ...... 使用kubectl get pod –-all-namespaces -o wide确保所有的Pod都处于Running状态。\nkube-flannel.yml [root@i-fahx5c7k k8s]# kubectl get pod --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system coredns-5c98db65d4-nbb4w 1/1 Running 0 6m29s 10.244.0.2 i-fahx5c7k \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-5c98db65d4-wtm58 1/1 Running 0 6m29s 10.244.0.3 i-fahx5c7k \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system etcd-i-fahx5c7k 1/1 Running 0 5m26s 192.168.11.21 i-fahx5c7k \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-apiserver-i-fahx5c7k 1/1 Running 0 5m37s 192.168.11.21 i-fahx5c7k \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-controller-manager-i-fahx5c7k 1/1 Running 0 5m45s 192.168.11.21 i-fahx5c7k \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-flannel-ds-amd64-bqswg 1/1 Running 0 58s 192.168.11.21 i-fahx5c7k \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-zhzxj 1/1 Running 0 6m29s 192.168.11.21 i-fahx5c7k \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-scheduler-i-fahx5c7k 1/1 Running 0 5m20s 192.168.11.21 i-fahx5c7k \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 2.4 测试集群DNS是否可用 kubectl run curl --image=radial/busyboxplus:curl -it 进入后执行nslookup kubernetes.default确认解析正常:\n[ root@curl-6bf6db5c4f-f8jjn:/ ]$ nslookup kubernetes.default Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes.default Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local 2.5 向Kubernetes集群中添加Node节点 在master上查看添加节点指令：kubeadm token create --print-join-command 下面将11.22这个主机添加到Kubernetes集群中，在11.22机器上执行: kubeadm join 192.168.11.21:6443 --token k6s0kn.nyao1ulwwmlm2org \\ --discovery-token-ca-cert-hash sha256:b721d25356668f921c90bf5f554836cb346b827a62b78797d0bcbcba24214725 11.22加入集群很是顺利，下面在master节点上执行命令查看集群中的节点：\n[root@i-fahx5c7k k8s]# kubectl get nodes NAME STATUS ROLES AGE VERSION i-fahx5c7k Ready master 13m v1.15.1 i-ouaaujhz Ready \u0026lt;none\u0026gt; 50s v1.15.1 2.5.1 如何从集群中移除Node 如果需要从集群中移除11.22这个Node执行下面的命令： 在master节点上执行： kubectl drain i-ouaaujhz --delete-local-data --force --ignore-daemonsets\nkubectl delete node i-ouaaujhz 在11.22上执行： kubeadm reset\nkube-proxy开启ipvs 修改ConfigMap的kube-system/kube-proxy中的config.conf，mode: “ipvs” kubectl edit cm kube-proxy -n kube-system 之后重启各个节点上的kube-proxy pod： kubectl get pod -n kube-system | grep kube-proxy | awk '{system(\u0026quot;kubectl delete pod \u0026quot;$1\u0026quot; -n kube-system\u0026quot;)}' 日志查看：kubectl logs kube-proxy-62ntf -n kube-system出现ipvs即开启。\n3.Kubernetes常用组件部署 使用Helm这个Kubernetes的包管理器，这里也将使用Helm安装Kubernetes的常用组件。\n3.1 Helm的安装 Helm由客户端命helm令行工具和服务端tiller组成，Helm的安装十分简单。 下载helm命令行工具到master节点node1的/usr/local/bin下，这里下载的2.14.1版本：\ncurl -O https://get.helm.sh/helm-v2.14.1-linux-amd64.tar.gz tar -zxvf helm-v2.14.1-linux-amd64.tar.gz cd linux-amd64/ cp helm /usr/local/bin/ 为了安装服务端tiller，还需要在这台机器上配置好kubectl工具和kubeconfig文件，确保kubectl工具可以在这台机器上访问apiserver且正常使用。 这里的11.21节点已经配置好了kubectl。 因为Kubernetes APIServer开启了RBAC访问控制，所以需要创建tiller使用的service account: tiller并分配合适的角色给它。这里简单起见直接分配cluster-admin这个集群内置的ClusterRole给它。创建vi helm-rbac.yaml文件：\napiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system kubectl create -f helm-rbac.yaml 接下来使用helm部署tiller: helm init --service-account tiller --skip-refresh tiller默认被部署在k8s集群中的kube-system这个namespace下：\n[root@i-fahx5c7k centosrepo]# kubectl get pod -n kube-system -l app=helm NAME READY STATUS RESTARTS AGE tiller-deploy-7bf78cdbf7-46bv5 1/1 Running 0 22s helm version\n","date":"2019-11-04T00:00:00Z","permalink":"https://Forest-L.github.io/post/k8s1-15-install/","section":"post","tags":["K8s"],"title":"k8s1.15安装"},{"categories":["image"],"contents":"构建arm/x86架构的docker image操作指南 由于arm环境越来越受欢迎，镜像不单单满足x86结构的docker镜像，还需要arm操作系统的镜像，以下说明在x86机器上如何build一个arm结构的镜像，使用buildx指令来同时构建arm/x86结构的镜像。\n1.\t启动一台ubuntu的机器，并安装docker 19.03 在测试过程中发现 Centos7.5 有下面的问题，这里我们直接绕过 issue docker安装参考docker安装\n2.\t运行下列命令安装并测试qemu  查看机器的架构  uname -m x86_64  正常测试docker启动一个arm镜像容器  docker run --rm -t arm64v8/ubuntu uname -m standard_init_linux.go:211: exec user process caused \u0026quot;exec format error\u0026quot;  添加特权模式安装qemu，且启动一个arm镜像容器  docker run --rm --privileged multiarch/qemu-user-static --reset -p yes docker run --rm -t arm64v8/ubuntu uname -m aarch64 3.\t启用docker buildx 命令 docker buildx 为跨平台构建 docker 镜像所使用的命令。目前为实验特性，可以设置dokcer cli的配置，将实验特性开启。\n将下面配置添加到CLI配置文件当中~/.docker/config.json\n{ \u0026quot;experimental\u0026quot;: \u0026quot;enabled\u0026quot; } 4.\t创建新的builder实例（默认的docker实例不支持镜像导出） docker buildx create --name ks-all docker buildx use ks-all docker buildx inspect --bootstrap\n执行下面命令可以看到 builder 已经创建好，并且支持多种平台的构建。\ndocker buildx ls NAME/NODE DRIVER/ENDPOINT STATUS PLATFORMS ks-all * docker-container ks-all0 unix:///var/run/docker.sock running linux/amd64, linux/arm64, linux/riscv64, linux/ppc64le, linux/s390x, linux/386, linux/arm/v7, linux/arm/v6 default docker default default running linux/amd64, linux/386 5.\t执行构建命令（以ks-installer为例） 在 ks-installer目录下执行命令可以构建 arm64与amd64的镜像，并自动推送到镜像仓库中。 docker buildx build -f /root/ks-installer/Dockerfile --output=type=registry --platform linux/arm64 -t lilinlinlin/ks-installer:2.1.0-arm64 .\n(需要注意现在 ks-installer 的 Dockerfile中 go build 命令带有 GOOS GOARCH等，这些要删除)\n构建成功之后，可以在dockerhub下图当中可以看到是支持两种arch\nDIGEST OS/ARCH COMPRESSED SIZE 97dd2142cac6 linux/amd64 111.13 MB ce366ad696cb linux/arm64 111.13 MB 6.\t构建并保存为tar 文件  可以参考 buildx 的官方文档 https://github.com/docker/buildx#-o---outputpath-typetypekeyvalue  docker buildx build --output=type=docker,dest=/root/ks-installer.tar --platform linux/arm64 -t lilinlinlin/ks-installer:2.1.0-arm64 ./pkg/db/\n构建tar包时需要注意output的类型需要是docker，而不是tar\n7. 多架构镜像管理 相同镜像格式代表arm64和amd64的镜像。\n 确保docker manifest命令被使能  # ~/.docker/config.json中添加 \u0026quot;experimental\u0026quot;: \u0026quot;enabled\u0026quot; cat ~/.docker/config.json { \u0026quot;experimental\u0026quot;: \u0026quot;enabled\u0026quot; }   将多架构镜像push到dockerhub中 docker push lilinlinlin/ks-installer-amd64:v3.0.0 docker push lilinlinlin/ks-installer-arm64:v3.0.0\n  创建对应镜像的 manifest list,特别重要\n  docker manifest create lilinlinlin/ks-installer:v3.0.0 \\ lilinlinlin/ks-installer-amd64:v3.0.0 \\ lilinlinlin/ks-installer-arm64:v3.0.0 --amend  查看修正manifest list内容,可以查看到arm和amd架构的字样   docker manifest inspect lilinlinlin/ks-installer:v3.0.0 如果相关信息不正确可使用annotate命令修正 docker manifest annotate --arch arm64 \\ lilinlinlin/ks-installer:v3.0.0 \\ lilinlinlin/ks-installer-arm64:v3.0.0  上传manifest list至dockerhub docker manifest push lilinlinlin/ks-installer:v3.0.0  更多参考 https://github.com/docker/buildx https://github.com/multiarch/qemu-user-static https://community.arm.com/developer/tools-software/tools/b/tools-software-ides-blog/posts/getting-started-with-docker-for-arm-on-linux\n","date":"2019-10-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/docker-image-operation-guide-for-building-arm-x86-architecture/","section":"post","tags":["arm","amd","buildx"],"title":"构建arm-x86架构的docker-image操作指南"},{"categories":null,"contents":"*介绍centos、ubuntu和pip三大核心系统的离线依赖源的制作方法及制作完成之后如何使用\n1、pip安装离线本地包,pip版本（19.2.3）  导出本地已有的依赖包,需要创建一个空的requirements.txt文件。 pip freeze \u0026gt; requirements.txt 下载到某个目录下（提前创建目录/packs），指定pip源。 pip download -r requirements.txt -d /packs -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com 安装requirements.txt依赖,可以通过pip \u0026ndash;help获取相关指令。  在线安装： pip install -r requirements.txt 离线安装： 将/packs目录下的包拷贝到离线环境的机器某目录上（/packs）， pip install --no-index --find-links=\u0026quot;/packs\u0026quot; -r requirements.txt 2、centos安装离线本地包  制作离线包  新的机器上需要在/etc/yum.conf下打开缓存，keepcache=1; 新建目录存放rpm包，如mkdir -p /root/centos-repo; 安装单个工具，yum install -y iotop --downloaddir=/root/centos-repo; 创建本地源，createrepo /root/centos-repo; 制作iso包：mkisofs -r -o /root/centos-7.5-amd64.iso /root/centos-repo/ 3、ubuntu安装离线本地包  制作离线包  创建存放目录：mkdir -p /home/ubuntu/packs； 安装软件包dpkg-dev:apt-get install dpkg-dev 拷贝dep包至存放目录：sudo cp -r /var/cache/apt/archives/* /home/ubuntu/packs； 进入packs目录下，生成包的依赖信息：dpkg-scanpackages packs /dev/null |gzip \u0026gt; packs/Packages.gz 制作iso包：mkisofs -r -o /home/ubuntu/ubuntu-16.04.5-amd64.iso /home/ubunut/packs  使用离线包,如Ubuntu16.04.5  创建目录：mkdir -p /kubeinstaller/apt_repo/16.04.5/iso 挂载至/etc/fstab下在iso包目录下：sh -c \u0026quot;echo 'ubuntu-16.04.5-server-amd64.iso /kubeinstaller/apt_repo/16.04.5/iso iso9660 loop 0 0' \u0026gt;\u0026gt; /etc/fstab\u0026quot; 备份之前源：mv -f /etc/apt/sources.list /etc/apt/sources.list-bak 添加新的源：sh -c \u0026quot;echo 'deb [trusted=yes] file:///kubeinstaller/apt_repo/16.04.5/iso/ /' \u0026gt; /etc/apt/sources.list\u0026quot; 挂载生效：mount -a # clean the process using apt or dpkg apt_process=`ps -aux | grep -E 'apt|dpkg' | grep -v 'grep' | awk '{print $2}'` for process in ${apt_process} do kill -9 $process done # remove the apt lock sudo rm -f /var/lib/apt/lists/lock 参考文献 pip: https://www.cnblogs.com/zengchunyun/p/9344664.html centos: https://blog.csdn.net/hao_rh/article/details/73275071 ubuntu: https://www.cnblogs.com/gzxbkk/p/7809296.html\n","date":"2019-10-10T00:00:00Z","permalink":"https://Forest-L.github.io/post/methods-of-making-and-using-centos-ubuntu-pip-offline-dependent-packages/","section":"post","tags":["yum","apt-get","Linux tools"],"title":"centos、ubuntu和pip离线依赖包的制作和使用方法"},{"categories":null,"contents":"k8s之etcd备份与恢复 etcd是一款开源的分布式一致性键值存储。目前有版本为V3以上，但是它的API又有v2和v3之分，以至于操作指令也不一样。 查看etcd版本etcdctl --version\n说 明 :  若使用 v3 备份数据时存在 v2 的数据则不影响恢复\n若使用 v2 备份数据时存在 v3 的数据则恢复失败\n1、对于API2备份与恢复方法   snap: 存放快照数据,etcd防止WAL文件过多而设置的快照，存储etcd数据状态。\n  wal: 存放预写式日志,最大的作用是记录了整个数据变化的全部历程。在etcd中，所有数据的修改在提交前，都要先写入到WAL中。\n  备份指令： etcdctl backup --data-dir /home/etcd/ --backup-dir /home/etcd_backup\n  恢复指令： etcd -data-dir=/home/etcd_backup/ -force-new-cluster\n  恢复时会覆盖 snapshot 的元数据(member ID 和 cluster ID)，所以需要启动一个新的集群。\n2、对于API3备份与恢复方法 在使用 API 3 时需要使用环境变量 ETCDCTL_API 明确指定。 export ETCDCTL_API=3\n2.1备份数据 etcdctl --endpoints localhost:2379 \\ --cert=/etc/ssl/etcd/ssl/node-master.pem \\ --key=/etc/ssl/etcd/ssl/node-master-key.pem \\ --cacert=/etc/ssl/etcd/ssl/ca.pem \\snapshot save \\ /var/backups/kube_etcd/snapshot.db 说 明 : \n  /var/backups/kube_etcd这个目录是根宿主机的/var/lib/etcd目录相映射的，所以备份在这个目录在对应的宿主机上也是能看见的。\n  这些证书对应文件可以直接在etcd容器内通过ps aux|more看见 其中–cert-file对应–cert，–key对应–key-file –cacert对应–trusted-ca-file\n  2.2恢复数据   停止三台master节点的kube-apiserver，指令为： mv /etc/kubernetes/manifests/kube-apiserver.yaml /etc/kubernetes/kube-apiserver.yaml\n  在三个master节点停止 etcd 服务,指令为： systemctl stop etcd\n  在三个master节点转移并备份当前 etcd 集群数据,指令为： mv /var/lib/etcd /var/lib/etcd.bak\n  将最新的etcd备份数据恢复至三个master节点，其中master_ip为不同master主机的IP 指令为：\n  export ETCDCTL_API=3 和 etcdctl snapshot restore /var/backups/kube_etcd/etcd-******/snapshot.db \\ --endpoints=master_ip:2379 \\ --cert=/etc/ssl/etcd/ssl/node-master.pem \\ --key=/etc/ssl/etcd/ssl/node-master-key.pem \\ --cacert=/etc/ssl/etcd/ssl/ca.pem \\ --data-dir=/var/lib/etcd   执行 etcd 恢复命令,指令为： systemctl restart etcd\n  重启 kube-apiserver,指令为： mv /etc/kubernetes/kube-apiserver.yaml /etc/kubernetes/manifests/kube-apiserver.yaml \n  检查是否正常,指令为： kubectl get pod --all-namespaces\n  检查etcd集群状态及成员指令为： etcdctl --endpoints=https://192.168.0.91:2379 --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-ks-allinone.pem --key=/etc/ssl/etcd/ssl/node-ks-allinone-key.pem member list\n ","date":"2019-10-09T00:00:00Z","permalink":"https://Forest-L.github.io/post/backup-and-restore-etcd-data-of-k8s/","section":"post","tags":["iptables","Linux tools"],"title":"k8s之etcd数据的备份与恢复"},{"categories":null,"contents":"centos系统k8s-1.16版本安装 k8s1.16版本相对之前版本变化不小，亮点和升级参看v1.16说明。相关联的镜像和v1.16二进制包上传至百度云上，链接如下k8s1.16介质，ftq5\n1.准备 1.1系统准备 需要将主机ip和主机名放在每台机器的vi /etc/hosts下\n192.168.11.21 i-fahx5c7k 192.168.11.22 i-ouaaujhz\n如果各个主机启用了防火墙，需要开启各个组件所需要的端口，可以查看https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ 这里各个节点禁用防火墙\nsystemctl stop firewalld systemctl disable firewalld\n禁用selinux\nsetenforce 0\nvi /etc/selinux/config SELINUX=disabled 创建vi /etc/sysctl.d/k8s.conf文件，添加如下内容：\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\n执行命令使修改生效\nmodprobe br_netfilter sysctl -p /etc/sysctl.d/k8s.conf\n1.2 kube-proxy开启ipvs的前置条件 在所有的节点上执行如下脚本：\ncat \u0026gt; /etc/sysconfig/modules/ipvs.modules \u0026lt;\u0026lt;EOF #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF chmod 755 /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; bash /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4 各个节点需要安装 ipset ipvsadm yum install ipset ipvsadm -y 1.3 docker安装 安装docker的yum源,国内寻找清华源\nyum install wget -y yum install -y yum-utils device-mapper-persistent-data lvm2 wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo sed -i 's+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+' /etc/yum.repos.d/docker-ce.repo yum makecache fast yum install docker-ce -y 重启docker：systemctl enable docker;systemctl restart docker 修改docker cgroup driver为systemd 创建或修改vi /etc/docker/daemon.json：\n{ \u0026quot;exec-opts\u0026quot;: [\u0026quot;native.cgroupdriver=systemd\u0026quot;] } 重启docker:systemctl restart docker\n2. 使用kubeadm部署kubernetes 2.1 安装kubeadm和kubelet 下面在各节点安装kubeadm和kubelet和kubectl，请在百度云上面下载rpm包，在k8s116目录下执行如下指令：\nyum install -y cri-tools-1.13.0-0.x86_64.rpm cni-0.7.5-0.x86_64.rpm kubelet-1.16.0-0.x86_64.rpm kubectl-1.16.0-0.x86_64.rpm kubeadm-1.16.0-0.x86_64.rpm \nk8s 1.8开始要求关闭系统的swap,不关闭，kubelet将无法启动，执行指令方法：\nswapoff -a\n修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载，使用free -m确认swap已经关闭。 swappiness参数调整，修改vi /etc/sysctl.d/k8s.conf添加下面一行：\nvm.swappiness=0\n执行sysctl -p /etc/sysctl.d/k8s.conf使修改生效。 因为这里本次测试主机上还运行其他服务，关闭swap可能会对其他服务产生影响，所以这里修改kubelet的配置去掉这个限制。 修改vi /etc/sysconfig/kubelet，加入：KUBELET_EXTRA_ARGS=--fail-swap-on=false\n2.2 使用kubeadm init初始化集群 在各节点开机启动kubelet服务：systemctl enable kubelet 使用kubeadm config print init-defaults可以打印集群初始化默认的使用的配置:\napiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 1.2.3.4 bindPort: 6443 nodeRegistration: criSocket: /var/run/dockershim.sock name: node1 taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: k8s.gcr.io kind: ClusterConfiguration kubernetesVersion: v1.14.0 networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 scheduler: {} 从默认的配置中可以看到，可以使用imageRepository定制在集群初始化时拉取k8s所需镜像的地址。advertiseAddress的ip替换本机，基于默认配置定制出本次使用kubeadm初始化集群所需的配置文件且在11.21机器上vi kubeadm.yaml：\napiVersion: kubeadm.k8s.io/v1beta2 kind: InitConfiguration localAPIEndpoint: advertiseAddress: 192.168.11.21 bindPort: 6443 nodeRegistration: taints: - effect: PreferNoSchedule key: node-role.kubernetes.io/master --- apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v1.16.0 networking: podSubnet: 10.244.0.0/16 说 明 :  使用kubeadm默认配置初始化的集群，会在master节点打上node-role.kubernetes.io/master:NoSchedule的污点，阻止master节点接受调度运行工作负载。这里测试环境只有两个节点，所以将这个taint修改为node-role.kubernetes.io/master:PreferNoSchedule。\n在开始初始化集群之前，kubeadm config images pull查看需要哪些镜像,需要从百度云上面下载tar包镜像下来，tar通过scp分别传到各个节点执行docker load -i解压， 镜像列表:\nk8s.gcr.io/kube-apiserver v1.16.0 b305571ca60a 42 hours ago 217MB k8s.gcr.io/kube-proxy v1.16.0 c21b0c7400f9 42 hours ago 86.1MB k8s.gcr.io/kube-controller-manager v1.16.0 06a629a7e51c 42 hours ago 163MB k8s.gcr.io/kube-scheduler v1.16.0 301ddc62b80b 42 hours ago 87.3MB k8s.gcr.io/etcd 3.3.15-0 b2756210eeab 2 weeks ago 247MB k8s.gcr.io/coredns 1.6.2 bf261d157914 5 weeks ago 44.1MB gcr.io/kubernetes-helm/tiller v2.14.1 ac22eb1f780e 3 months ago 94.2MB quay.io/coreos/flannel v0.11.0-amd64 ff281650a721 7 months ago 52.6MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 21 months ago 742kB radial/busyboxplus curl 71fa7369f437 5 years ago 4.23MB 接下来使用kubeadm初始化集群，选择node1作为Master Node，在node1上执行下面的命令： kubeadm init --config kubeadm.yaml --ignore-preflight-errors=Swap\nkubeadm join 192.168.11.21:6443 --token k6s0kn.nyao1ulwwmlm2org \\ --discovery-token-ca-cert-hash sha256:b721d25356668f921c90bf5f554836cb346b827a62b78797d0bcbcba24214725 \n其中关键步骤： * [kubelet-start] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml” * [certs]生成相关的各种证书 * [kubeconfig]生成相关的kubeconfig文件 * [control-plane]使用/etc/kubernetes/manifests目录中的yaml文件创建apiserver、controller-manager、scheduler的静态pod * [bootstraptoken]生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到 * 下面的命令是配置常规用户如何使用kubectl访问集群： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config * 最后给出了将节点加入集群的命令kubeadm join 192.168.99.11:6443 –token 4qcl2f.gtl3h8e5kjltuo0r \\ –discovery-token-ca-cert-hash sha256:7ed5404175cc0bf18dbfe53f19d4a35b1e3d40c19b10924275868ebf2a3bbe6e 需要在11.21机器上执行： mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n查看集群状态，确认组件都处于healthy状态： kubectl get cs\n集群初始化如果遇到问题，可以使用下面的命令进行清理： kubeadm reset\n2.3 安装Pod Network 接下来安装flannel network add-on： mkdir -p ~/k8s/\ncd ~/k8s\ncurl -O https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\nkubectl apply -f kube-flannel.yml\n这里注意kube-flannel.yml这个文件里的flannel的镜像是0.11.0，quay.io/coreos/flannel:v0.11.0-amd64 注意需要在vi /var/lib/kubelet/kubeadm-flags.env文件配置中去掉\u0026ndash;network-plugin=cni,然后重启kubelet,\nsystemctl daemon-reload systemctl restart kubelet 如果Node有多个网卡的话，参考https://github.com/kubernetes/kubernetes/issues/39701， 目前需要在kube-flannel.yml中使用–iface参数指定集群主机内网网卡的名称，否则可能会出现dns无法解析。需要将kube-flannel.yml下载到本地，flanneld启动参数加上–iface=\ncontainers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth1 ...... 使用kubectl get pods –-all-namespaces -o wide确保所有的Pod都处于Running状态。\nkube-flannel.yml [root@i-fahx5c7k k8s]# kubectl get pods --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system coredns-5c98db65d4-nbb4w 1/1 Running 0 6m29s 10.244.0.2 i-fahx5c7k \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-5c98db65d4-wtm58 1/1 Running 0 6m29s 10.244.0.3 i-fahx5c7k \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system etcd-i-fahx5c7k 1/1 Running 0 5m26s 192.168.11.21 i-fahx5c7k \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-apiserver-i-fahx5c7k 1/1 Running 0 5m37s 192.168.11.21 i-fahx5c7k \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-controller-manager-i-fahx5c7k 1/1 Running 0 5m45s 192.168.11.21 i-fahx5c7k \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-flannel-ds-amd64-bqswg 1/1 Running 0 58s 192.168.11.21 i-fahx5c7k \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-zhzxj 1/1 Running 0 6m29s 192.168.11.21 i-fahx5c7k \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-scheduler-i-fahx5c7k 1/1 Running 0 5m20s 192.168.11.21 i-fahx5c7k \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 2.4 测试集群DNS是否可用 kubectl run curl --image=radial/busyboxplus:curl -it\n进入后执行nslookup kubernetes.default确认解析正常:\n[ root@curl-6bf6db5c4f-f8jjn:/ ]$ nslookup kubernetes.default Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes.default Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local 2.5 向Kubernetes集群中添加Node节点 下面将11.22这个主机添加到Kubernetes集群中，在11.22机器上执行: kubeadm join 192.168.11.21:6443 --token k6s0kn.nyao1ulwwmlm2org \\ --discovery-token-ca-cert-hash sha256:b721d25356668f921c90bf5f554836cb346b827a62b78797d0bcbcba24214725 11.22加入集群很是顺利，下面在master节点上执行命令查看集群中的节点：\n[root@i-fahx5c7k k8s]# kubectl get nodes NAME STATUS ROLES AGE VERSION i-fahx5c7k Ready master 13m v1.15.1 i-ouaaujhz Ready \u0026lt;none\u0026gt; 50s v1.15.1 2.5.1 如何从集群中移除Node 如果需要从集群中移除11.22这个Node执行下面的命令： 在master节点上执行： kubectl drain i-ouaaujhz --delete-local-data --force --ignore-daemonsets\nkubectl delete node i-ouaaujhz\n在11.22上执行： kubeadm reset\nkube-proxy开启ipvs 修改ConfigMap的kube-system/kube-proxy中的config.conf，mode: “ipvs”\nkubectl edit cm kube-proxy -n kube-system\n之后重启各个节点上的kube-proxy pod： kubectl get pod -n kube-system | grep kube-proxy | awk '{system(\u0026quot;kubectl delete pod \u0026quot;$1\u0026quot; -n kube-system\u0026quot;)}'\n日志查看：kubectl logs kube-proxy-62ntf -n kube-system出现ipvs即开启。\n3.Kubernetes常用组件部署 使用Helm这个Kubernetes的包管理器，这里也将使用Helm安装Kubernetes的常用组件。\n3.1 Helm的安装 Helm由客户端命helm令行工具和服务端tiller组成，Helm的安装十分简单。 下载helm命令行工具到master节点node1的/usr/local/bin下，这里下载的2.14.1版本：\ncurl -O https://get.helm.sh/helm-v2.14.1-linux-amd64.tar.gz tar -zxvf helm-v2.14.1-linux-amd64.tar.gz cd linux-amd64/ cp helm /usr/local/bin/ 为了安装服务端tiller，还需要在这台机器上配置好kubectl工具和kubeconfig文件，确保kubectl工具可以在这台机器上访问apiserver且正常使用。 这里的11.21节点已经配置好了kubectl。 helm init --output yaml \u0026gt; tiller.yaml 更新 tiller.yaml 两处：apiVersion 版本;增加选择器\napiVersion: apps/v1 kind: Deployment ... spec: replicas: 1 strategy: {} selector: matchLabels: app: helm name: tiller 创建：kubectl create -f tiller.yaml 因为Kubernetes APIServer开启了RBAC访问控制，所以需要创建tiller使用的service account: tiller并分配合适的角色给它。这里简单起见直接分配cluster-admin这个集群内置的ClusterRole给它。\nkubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system tiller-deploy -p '{\u0026quot;spec\u0026quot;:{\u0026quot;template\u0026quot;:{\u0026quot;spec\u0026quot;:{\u0026quot;serviceAccount\u0026quot;:\u0026quot;tiller\u0026quot;}}}}' 检查helm是否安装成功helm list\n安装k8s1.16脚本的安装 解压包，然后执行脚本install-k8s.sh tar -xzvf k8s116.tar.gz ./install-k8s.sh\n参考文档 kubeadm安装 kubeadm创建集群\n","date":"2019-09-22T00:00:00Z","permalink":"https://Forest-L.github.io/post/k8s1.16-installed-on-centos-system/","section":"post","tags":["K8s","centos"],"title":"K8s1.16在centos安装"},{"categories":null,"contents":"k8s1.15.3在ubuntu系统部署 国内环境下，k8s1.15.3在ubuntu系统部署，相关的镜像以及docker的deb包和k8s核心组件的deb包在以下百度链接下。 k8s介质 提取码：05ef\n配置 2核4G k8s：v1.15.3 ubuntu:18.04\n1. 前期准备   关闭ufw防火墙,Ubuntu默认未启用,无需设置。 sudo ufw disable\n  禁用SELINUX （ubuntu19.04默认不安装） sudo setenforce 0\n  开启数据包转发,修改/etc/sysctl.conf，开启ipv4转发 net.ipv4.ip_forward=1 注释取消\n  防火墙修改FORWARD链默认策略 sudo iptables -P FORWARD ACCEPT\n  禁用swap sudo swapoff -a\n  配置iptables参数，使得流经网桥的流量也经过iptables/netfilter防火墙\n  sudo tee /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt;-'EOF' net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system 2.docker安装  安装deb包,通过dpkg指令  dpkg -i containerd.io_1.2.5-1_amd64.deb \u0026amp;\u0026amp; \\ dpkg -i docker-ce-cli_18.09.5~3-0~ubuntu-bionic_amd64.deb \u0026amp;\u0026amp; \\ dpkg -i docker-ce_18.09.5~3-0~ubuntu-bionic_amd64.deb  docker使用加速器（阿里云加速器）  tee /etc/docker/daemon.json \u0026lt;\u0026lt;- 'EOF' { \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;https://5xcgs6ii.mirror.aliyuncs.com\u0026quot;] } EOF  设置docker开机自启动 sudo systemctl enable docker \u0026amp;\u0026amp; sudo systemctl start docker  3.安装kubeadm、kubelet、kubectl  通过dpkg -i 来安装k8s核心组件，指令如下  dpkg -i cri-tools_1.13.0-00_amd64.deb kubernetes-cni_0.7.5-00_amd64.deb socat_1.7.3.2-2ubuntu2_amd64.deb conntrack_1%3a1.4.4+snapshot20161117-6ubuntu2_amd64.deb kubelet_1.15.3-00_amd64.deb kubectl_1.15.3-00_amd64.deb kubeadm_1.15.3-00_amd64.deb  设置开机自启动 sudo systemctl enable kubelet \u0026amp;\u0026amp; sudo systemctl start kubelet  4.kubeadm init初始化集群  先要将需要的镜像解压 docker load -i k8s1153.tar 查看Kubernetes需要哪些镜像 kubeadm config images list --kubernetes-version=v1.15.3 注意apiserver-advertise-address要换成本机的IP  sudo kubeadm init --apiserver-advertise-address=192.168.11.21 --pod-network-cidr=172.16.0.0/16 --service-cidr=10.233.0.0/16 --kubernetes-version=v1.15.3   创建kubectl使用的kubeconfig文件 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config\n  创建flannel的pod，命令如下 以下两个文件在百度下链接下。 kubectl create -f kube-flannel.yml kubectl apply -f weave-net.yml\n  5.检查集群及重新添加节点  检查node是否ready kubectl get nodes 检查pod是否running kubectl get pod --all-namespaces -o wide 添加节点，如果忘记token了，可以在master上面执行如下指令获取 kubeadm token list 添加节点，需要提前在新的机器上安装kubelet等服务及需要把相关的镜像拷贝过去解压。最后通过如下指令添加： kubeadm join –token=4fccd2.b0e0f8918bd95d3e 192.168.11.21:6443  参考文档 参考\n","date":"2019-09-02T00:00:00Z","permalink":"https://Forest-L.github.io/post/k8s1-15-3-install-on-the-ubuntu/","section":"post","tags":["iptables","Linux tools"],"title":"k8s1.15.3在ubuntu系统部署"},{"categories":null,"contents":"docker部署mysql5.7 越来越多服务容器化，下面以mysql5.7版本为例容器化部署。按三种方式：最简单、配置文件映射和数据映射来展开。\n准备条件 docker官方镜像：mysql:5.7 个人docker账号：lilinlinlin/mysql:5.7\n1. 最简单的部署mysql docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=rootroot -d lilinlinlin/mysql:5.7 Navicat 测试连接 用户名:root 密码:rootroot 端口:3306\n2. 数据映射到本机部署mysql 先在本机新建一个目录，mkdir -p /var/lib/mysql 然后run起来,-v前面是宿主机的目录，\u0026ndash;restart always表示重启容器 docker run -p 3306:3306 --restart always -v /var/lib/mysql:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=mysqlroot -d 镜像ID\n3. 配置文件映射到本机部署mysql 在本机/etc下新建vi my.cnf配置文件，如果有的话先删除原来的，再加入创建新的\n[mysql] #设置mysql客户端默认字符集 default-character-set=utf8 socket=/var/lib/mysql/mysql.sock [mysqld] #mysql5.7以后的不兼容问题处理 sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock # Disabling symbolic-links is recommended to prevent assorted security risks symbolic-links=0 # Settings user and group are ignored when systemd is used. # If you need to run mysqld under a different user or group, # customize your systemd unit file for mariadb according to the # instructions in http://fedoraproject.org/wiki/Systemd #允许最大连接数 max_connections=200 #服务端使用的字符集默认为8比特编码的latin1字符集 character-set-server=utf8 #创建新表时将使用的默认存储引擎 default-storage-engine=INNODB lower_case_table_names=1 max_allowed_packet=16M #设置时区 default-time_zone='+8:00' [mysqld_safe] log-error=/var/log/mariadb/mariadb.log pid-file=/var/run/mariadb/mariadb.pid # # include all files from the config directory # !includedir /etc/mysql/conf.d/ !includedir /etc/mysql/mysql.conf.d/ 运行的指令：\u0026ndash;privileged=true 获取临时的selinux的权限。 docker run -p 3306:3306 --privileged=true -v /etc/my.cnf:/etc/mysql/mysql.conf.d/mysqld.cnf -e MYSQL_ROOT_PASSWORD=mysql密码 -d 镜像ID\n查看容器启动情况 docker ps -a|grep mysql\n","date":"2019-08-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/docker-deploy-mysql5-7/","section":"post","tags":["mysql","Linux tools"],"title":"docker部署mysql5.7"},{"categories":null,"contents":"k8s之helm部署及用法 Helm是Kubernetes的一个包管理工具，用来简化Kubernetes应用的部署和管理。可以把Helm比作CentOS的yum工具。所以可以把该包在不同环境下部署起来,前提需要部署k8s环境。\n1. helm部署 Helm由两部分组成，客户端helm和服务端tiller。\n tiller运行在Kubernetes集群上，管理chart安装的release helm是一个命令行工具，可在本地运行，一般运行在CI/CD Server上。一般我们用helm操作  1.1 客户端helm和服务端tiller安装，以192.168.11.20为例 下载地址：https://github.com/helm/helm/releases 这里可以下载的是helm v2.14.1，解压缩后将可执行文件helm拷贝到/usr/local/bin下。这样客户端helm就在这台机器上安装完成了。 通过helm version显示客户端安装好了，但是服务端没有好.\nhelm version Client: \u0026amp;version.Version{SemVer:\u0026quot;v2.14.1\u0026quot;, GitCommit:\u0026quot;5270352a09c7e8b6e8c9593002a73535276507c0\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;} Error: Get http://localhost:8080/api/v1/namespaces/kube-system/pods?labelSelector=app%3Dhelm%2Cname%3Dtiller: dial tcp [::1]:8080: connect: connection refused 为了安装服务端tiller，还需要在这台机器上配置好kubectl工具和kubeconfig文件，确保kubectl工具可以在这台机器上访问apiserver且正常使用。 kubectl get cs 使用helm在k8s上部署tiller： helm init --service-account tiller --skip-refresh 说明: 如果网络原因不能访问gcr.io，可以通过helm init –service-account tiller –tiller-image /tiller:2.7.2 –skip-refresh使用私有镜像仓库中的tiller镜像。ps:lilinlinlin/tiller:2.7.2 tiller默认被部署在k8s集群中的kube-system这个namespace下。 kubectl get pod -n kube-system -l app=helm 再次helm version可以打印客户端和服务端的版本：\nhelm version Client: \u0026amp;version.Version{SemVer:\u0026quot;v2.7.2\u0026quot;, GitCommit:\u0026quot;8 Server: \u0026amp;version.Version{SemVer:\u0026quot;v2.7.2\u0026quot;, 2. kubernetes RBAC配置 因为我们将tiller部署在Kubernetes 1.8上，Kubernetes APIServer开启了RBAC访问控制，所以我们需要创建tiller使用的service account: tiller并分配合适的角色给它。 这里简单起见直接分配cluster-admin这个集群内置的ClusterRole给它。创建vi helm-rbac.yaml文件：\napiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system kubectl create -f helm-rbac.yaml\n3.添加国内helm源 helm repo add stable http://mirror.azure.cn/kubernetes/charts/ helm repo add apphub https://apphub.aliyuncs.com 更新chart repo: helm repo update\n4. helm的基本使用 下面我们开始尝试创建一个chart，这个chart用来部署一个简单的服务\nhelm create hello-test Creating hello-test tree hello-test/ hello-test/ ├── charts ├── Chart.yaml ├── templates │ ├── deployment.yaml │ ├── _helpers.tpl │ ├── ingress.yaml │ ├── NOTES.txt │ └── service.yaml └── values.yaml  charts目录中是本chart依赖的chart，当前是空的 Chart.yaml这个yaml文件用于描述Chart的基本信息，如名称版本等 templates是Kubernetes manifest文件模板目录，模板使用chart配置的值生成Kubernetes manifest文件。 templates/NOTES.txt 纯文本文件，可在其中填写chart的使用说明 value.yaml 是chart配置的默认值 在 values.yaml 中，可以看到，默认创建的是一个 Nginx 应用。为了方便外网访问测试，将 values.yaml 中 service 的属性修改为:  service: type: NodePort port: 30080 4.1 部署应用 helm install ./hello-test 但实际上可以这样部署为,.tgz为chart包，.yaml类似与values.yaml把变量文件定义出来。 helm upgrade --install \u0026lt;name\u0026gt; **.tgz **.yaml --namespace \u0026lt;namespace-name\u0026gt;\n4.2 查看部署应用 helm list\n4.3 删除部署应用 helm delete \u0026lt;name\u0026gt;\n4.4 打包chart helm package \u0026lt;name\u0026gt;\n参考： https://www.kubernetes.org.cn/3435.html\n","date":"2019-08-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/helm-deployment-and-guide/","section":"post","tags":["iptables","Linux tools"],"title":"k8s之helm部署及用法"},{"categories":null,"contents":"","date":"2019-08-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/kubekey-source-code-interpretation/","section":"post","tags":["iptables","Linux tools"],"title":"kubekey源码解读"},{"categories":null,"contents":"软件一致性尤为重要，它可以避免分裂，使众厂商将精力聚焦于共同推动软件发展而不是自成一家。2017年CNCF启动了Kubernetes一致性认证计划，CNCF提供一套测试工具，各厂商按照操作指导进行测试自身的产品，将测试报告上传给CNCF社区，CNCF审核测试报告后，会给符合条件的企业颁发一个证书。 大致流程：1. 环境信息  亚太机器ubuntu（16.04.6）两台，192.168.0.3/192.168.0.4 k8s1.18.3（1master+1node）（至少两台机器） sonobuoy0.18.3  2.k8s及云平台的部署  参考官网链接部署K8s及云平台的部署 K8s版本为1.18.3及KubeSphere版本为v3.0.0，指令如下：  curl -O -k https://kubernetes.pek3b.qingstor.com/tools/kubekey/kk chmod +x kk ./kk create cluster --with-kubernetes v1.18.3 --with-kubesphere 3. sonobuoy组件部署及运行  sonobuoy二进制文件下载  wget https://github.com/vmware-tanzu/sonobuoy/releases/download/v0.18.3/sonobuoy_0.18.3_linux_amd64.tar.gz  解压，tar -xzvf sonobuoy_0.18.3_linux_amd64.tar.gz 配置环境变量，sonobuoy执行文件拷贝到/usr/local/bin下 cp sonobuoy /usr/local/bin/  4. sonobuoy执行及相关指令  在集群中部署一个sonobuoy的pod，\u0026ndash;mode=certified-conformance参数在Kubernetesv1.16(Sonobuoy v0.16)需要添加，指令为：  sonobuoy run --mode=certified-conformance  sonobuoy运行状态  sonobuoy status  详细的日志  sonobuoy logs  通过sonobuoy status显示“completed”，可以通过如下指令获取输出结果，需要提交的内容在plugins/e2e/results/global/{e2e.log,junit_01.xml}目录下  sonobuoy retrieve  删除sonobuoy组件  sonobuoy delete 5. 提交的pr包含内容  5.1 fork k8s-conformance代码到你GitHub账号下，然后git clone到本地。 5.2 在本地找到对应的k8s版本号，然后在里面建相关的名字即可，比如在v1.18版本下创建KubeSphere目录。 5.3 自己项目目录下，需要包含以下四个文件，e2e.log、junit_01.xml、PRODUCT.yaml和README.md 5.4 e2e.log和junit_01.xml两个文件是通过上面四步骤下解压的两个文件。 5.5 PRODUCT.yaml包含的内容大致为：  vendor: 组织结构 name: 项目名 version: 版本号 website_url: 项目官方浏览页 repo_url: 项目官方镜像仓库地址 documentation_url: 项目官方文档 product_logo_url: 项目log图标 type: 开源/非开源 description: 项目的描述  5.6 README.md包含的内容大致为：  To reproduce: 本身项目的安装方法 sonobuoy项目的安装方法  5.7 提交代码到自己GitHub账号下，然后提pr即可。  6. 参考文章 https://github.com/cncf/k8s-conformance/blob/master/instructions.md\n","date":"2019-08-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/kubernetes-compliance-certification-submission-instructions/","section":"post","tags":["iptables","Linux tools"],"title":"kubernetes一致性认证的提交操作指南"},{"categories":null,"contents":"kubesphere2.1-HA环境，一个master或者两个master宕机恢复 kubesphere2.1版本提供了master节点的高可用功能，为生产环境保驾护航。然而很多事情都有意外，当其中一个master或者两个master卡住了，或者重启都不能自动恢复的情况下，那么怎么恢复呢，以下分一个master宕机和两个master宕机的恢复方法。\n验证环境信息 os: centos7.5 master1: 192.168.11.6 master2: 192.168.11.8 master3: 192.168.11.13 node1: 192.168.11.14 lb: 192.168.11.253 nfs服务端: 192.168.11.14 一个master宕机的模拟及恢复方法  正常的环境：nodes都running，etcd服务都正常。  kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 19m v1.15.5 master2 Ready master 16m v1.15.5 master3 Ready master 16m v1.15.5 node1 Ready worker 14m v1.15.5 export ETCDCTL_API=3 etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status 192.168.11.6:2379, b3487da7c562b17, 3.2.18, 3.8 MB, true, 5, 4434 192.168.11.8:2379, 3d04d74d80b7f07b, 3.2.18, 3.8 MB, false, 5, 4434 192.168.11.13:2379, 3ad323c042cc4c05, 3.2.18, 3.8 MB, false, 5, 4434  一个master宕机情况，把master2重置，看nodes和etcd情况。还需在界面创建一些带有存储的pod用例。  kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 57m v1.15.5 master2 NotReady master 55m v1.15.5 master3 Ready master 55m v1.15.5 node1 Ready worker 52m v1.15.5 etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status Failed to get the status of endpoint 192.168.11.8:2379 (context deadline exceeded) 192.168.11.6:2379, b3487da7c562b17, 3.2.18, 11 MB, true, 5, 14953 192.168.11.13:2379, 3ad323c042cc4c05, 3.2.18, 11 MB, false, 5, 14972  恢复方法：修改脚本中hosts.ini文件，需要注意顺序 在[all]组里面，master2行整体放在master3下面，[kube-master]和[etcd]组中master2放在master3下面,然后在另一个终端机器上重新执行安装脚本。以下恢复情况，etcd正常，nodes也正常，业务数据存在且业务pod没有中断正常使用。  kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 83m v1.15.5 master2 Ready master 80m v1.15.5 master3 Ready master 80m v1.15.5 node1 Ready worker 78m v1.15.5 etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status 192.168.11.6:2379, b3487da7c562b17, 3.2.18, 11 MB, true, 11, 20292 192.168.11.8:2379, 3d04d74d80b7f07b, 3.2.18, 11 MB, false, 11, 20297 192.168.11.13:2379, 3ad323c042cc4c05, 3.2.18, 11 MB, false, 11, 20298 docker ps | grep tomcat 6863620b07cf 882487b8be1d \u0026quot;catalina.sh run\u0026quot; 17 minutes ago Up 17 minutes k8s_tomcat-2jl160_tomcat-v1-6c7745494-k64w5_demo_5b406841-df9b-4d5c-b018-998c400a2c3e_1 两个master宕机的模拟及恢复方法  两个master宕机情况，把master2和master3重置，看nodes和etcd情况，nodes不正常，etcd两个不正常。  kubectl get nodes Unable to connect to the server: EOF [root@master1 ~]# [root@master1 ~]# [root@master1 ~]# etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status Failed to get the status of endpoint 192.168.11.8:2379 (context deadline exceeded) Failed to get the status of endpoint 192.168.11.13:2379 (context deadline exceeded) 192.168.11.6:2379, b3487da7c562b17, 3.2.18, 11 MB, false, 12, 27944  恢复方法： 1、同样需要在host.ini文件修改master的顺序，由于我们重置2和3，所有此处不用修改顺序； 2、在已解压的安装介质目录下，进入k8s/roles/kubernetes/preinstall/tasks/main.yml文件，用#注释如下内容，重跑安装脚本，作用说明：在重置的master2和master3机器上安装docker和etcd，但整个集群还需修复。。  #- import_tasks: 0020-verify-settings.yml # when: # - not dns_late # tags: # - asserts 3、在master2机器上，临时修复master2节点的etcd服务，执行如下指令：\n停止etcd：systemctl stop etcd 备份etcd数据：mv /var/lib/etcd /var/lib/etcd-bak 从master1的/var/backups/kube_etcd/备份目录下拷贝最近时间的snapshot.db至master2机器上 在master2先转为版本3指令令：export ETCDCTL_API=3 在master2恢复指令：etcdctl snapshot restore /root/snapshot.db --endpoints=192.168.11.8:2379 --cert=/etc/ssl/etcd/ssl/node-master2.pem --key=/etc/ssl/etcd/ssl/node-master2-key.pem --cacert=/etc/ssl/etcd/ssl/ca.pem --data-dir=/var/lib/etcd 重启etcd: systemctl restart etcd 4、修改host.ini文件master2和master3顺序，把[all][kube-master][etcd]三个组的master3放在最后面，再次跑安装脚本。其中的host.ini文件为\n[all] master1 ansible_connection=local ip=192.168.11.6 master2 ansible_host=192.168.11.8 ip=192.168.11.8 ansible_ssh_pass= master3 ansible_host=192.168.11.13 ip=192.168.11.13 ansible_ssh_pass= node1 ansible_host=192.168.11.14 ip=192.168.11.14 ansible_ssh_pass= [kube-master] master1 master2 master3 [kube-node] node1 [etcd] master1 master2 master3 [k8s-cluster:children] kube-node kube-master 5、由第四步只是临时修复master2的etcd，并不完全修复，先全部修复作如下处理：\n第四步执行过程中，z在“wait for etcd up”会报错，先ctrl +c 终止脚本； 在master2机器上，停止etcd服务：systemctl stop etcd 在master2机器上，删除etcd数据：rm -rf /var/lib/etcd 再次修改host.ini文件，master2和master3顺序，把[all][kube-master][etcd]三个组的master2放在最后面，再次跑安装脚本即可。 验证结果  集群中的nodes、etcd和带有存储数据的业务pod情况： 集群中的nodes、etcd和带有存储数据的业务pod情况,nodes恢复正常，etcd服务也回复正常，带有存储的pod一直运行，集群恢复成功：  [root@master1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 4h56m v1.15.5 master2 Ready master 4h53m v1.15.5 master3 Ready master 4h53m v1.15.5 node1 Ready worker 4h51m v1.15.5 [root@master1 ~]# [root@master1 ~]# [root@master1 ~]# [root@master1 ~]# etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status 192.168.11.6:2379, b3487da7c562b17, 3.2.18, 12 MB, false, 1372, 32116 192.168.11.8:2379, 3d04d74d80b7f07b, 3.2.18, 12 MB, true, 1372, 32116 192.168.11.13:2379, 3ad323c042cc4c05, 3.2.18, 12 MB, false, 1372, 32116 docker ps | grep tomcat 6863620b07cf 882487b8be1d \u0026quot;catalina.sh run\u0026quot; 4 hours ago Up 4 hours k8s_tomcat-2jl160_tomcat-v1-6c7745494-k64w5_demo_5b406841-df9b-4d5c-b018-998c400a2c3e_1 ","date":"2019-08-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/kubesphere2-1-ha-environment-one-master-or-two-masters-down-to-recover/","section":"post","tags":["etcdctl","k8s"],"title":"kubesphere2-1-HA环境，一个master或者两个master宕机恢复"},{"categories":null,"contents":"kubesphere2.1版本提供了master节点的高可用功能，为生产环境保驾护航。然而在生产环境中，为了业务更正常运行，当以下两种情形发生时，告诉大家怎么恢复。第一种情形是：其中某台master机器的etcd服务不能正常提供服务，而需要在另外一台机器上部署一个etcd服务加入到现etcd集群中；第二种情形是：其中某台master宕机，需要在另外一台机器上部署master，并加入到现master集群中。\n环境信息 os: centos7.5 master1: 192.168.11.6 master2: 192.168.11.16 master3: 192.168.11.13 node1: 192.168.11.14 lb: 192.168.11.253 nfs服务端: 192.168.11.14 新加机器master2: 192.168.11.8 安装介质机器：192.168.11.6 一个master宕机，在另外一台服务器上恢复方法 假设为master2（192.168.11.16）宕机了，现以新机器192.168.11.8作为恢复master2的机器。以下操作都在master1机器上执行。 1、在安装介质机器即master1机器上面，进入/etc/ssl/etcd/ssl目录下，将含有master2相关联的文件证书删掉（那个master有问题就出来那个）。 rm -rf admin-master2-key.pem admin-master2.pem member-master2-key.pem member-master2.pem node-master2-key.pem node-master2.pem 2、将etcd集群中master2的节点移除。\n第一步先转为etcd3版本：export ETCDCTL_API=3 查看etcd集群的成员： etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem member list b3487da7c562b17, started, etcd1, https://192.168.11.6:2380, https://192.168.11.6:2379 3ad323c042cc4c05, started, etcd2, https://192.168.11.13:2380, https://192.168.11.13:2379 52a4779150442b41, started, etcd3, https://192.168.11.16:2380, https://192.168.11.16:2379 第三步：移除master2节点，如192.168.11.16 etcdctl member remove 52a4779150442b41 --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem 3、新打开master1机器窗口进入解压包目录下修改conf/hosts.ini,一定要新打开窗口（由于上一步操作使master1机器为etcd3版本指令）。 master2的IP由原来的192.168.11.16改成192.168.11.8；在[all]组里面，master2行整体放在master3下面，[kube-master]和[etcd]组中master2放在master3下面,然后执行安装脚本即可。 4、验证结果： 用kubectl get nodes -o wide指令看master2IP是否替换； 看etcd集群中是否包含新的master2IP，指令为：\n1、开启etcd3版本：export ETCDCTL_API=3 2、etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status 一个master中etcd服务不正常，在另外一台服务器上恢复etcd方法 假设为master2（192.168.11.16）宕机了，现以新机器192.168.11.8作为恢复master2的机器。以下操作都在master1机器上执行。 1、在安装介质机器即master1机器上面，进入/etc/ssl/etcd/ssl目录下，将含有master2相关联的文件证书删掉（那个master有问题就出来那个）。 rm -rf admin-master2-key.pem admin-master2.pem member-master2-key.pem member-master2.pem node-master2-key.pem node-master2.pem 2、将etcd集群中master2的节点移除。\n第一步先转为etcd3版本：export ETCDCTL_API=3 查看etcd集群的成员： etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem member list b3487da7c562b17, started, etcd1, https://192.168.11.6:2380, https://192.168.11.6:2379 3ad323c042cc4c05, started, etcd2, https://192.168.11.13:2380, https://192.168.11.13:2379 52a4779150442b41, started, etcd3, https://192.168.11.16:2380, https://192.168.11.16:2379 第三步：移除master2节点，如192.168.11.16 etcdctl member remove 52a4779150442b41 --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem 3、新打开master1机器窗口进入解压包目录下修改conf/hosts.ini,一定要新打开窗口（由于上一步操作使master1机器为etcd3版本指令）。 master2的IP由原来的192.168.11.16改成192.168.11.8；在[all]组里面，master2行整体放在master3下面，[kube-master]和[etcd]组中master2放在master3下面。 4、进入解压包，scripts目录下，编辑install.sh脚本，用#将如下内容注释掉： ansible-playbook -i $BASE_FOLDER/../k8s/inventory/my_cluster/hosts.ini $BASE_FOLDER/../kubesphere/kubesphere.yml -b 5、进入解压包，k8s目录下，编辑cluster.yml文件，用#将以下开头的内容至结尾都注释掉\n- hosts: k8s-cluster any_errors_fatal: \u0026quot;{{ any_errors_fatal | default(true) }}\u0026quot; roles: - { role: kubespray-defaults} - { role: kubernetes/node, tags: node } environment: \u0026quot;{{ proxy_env }}\u0026quot; 6、重新到脚本目录，执行install.sh脚本即可。 7、验证结果： 看etcd集群中是否包含新的master2IP，指令为：\n1、开启etcd3版本：export ETCDCTL_API=3 2、etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem member list ","date":"2019-08-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/kubesphere2-1-ha-environment-a-master-or-master-etcd-down-a-new-machine-recovery-method/","section":"post","tags":["etcdctl","k8s"],"title":"kubesphere2-1-HA环境，某台master或者master的etcd宕机，新加机器恢复方法"},{"categories":null,"contents":"tcpdump抓包实战教程 做 web 开发，接口对接过程中，分析 http 请求报文数据包格式是否正确，定位问题，省去无用的甩锅过程，再比如抓取 tcp/udp 报文，分析 tcp 连接过程中的三次握手和四次挥手。windows使用wireshark工具，Linux使用的是tcpdump工具，也可以生成.pcap文件在wireshark图形化工具上分析。\n命令行  -i 选择网卡接口 -n： 不解析主机名 -nn：不解析端口 port 80： 抓取80端口上面的数据 tcp： 抓取tcp的包 udp：抓取udp的包 -w： 保存成pcap文件 dst：目的ip src：源ip -c：\u0026lt;数据包数目\u0026gt; -s0表示可按包长显示完整的包  tcp标志位  SYN，显示为S，同步标志位，用于建立会话连接，同步序列号； ACK，显示为.，确认标志位，对已接收的数据包进行确认； FIN，显示为F，完成标志位，表示我已经没有数据要发送了，即将关闭连接； RESET，显示为R，重置标志位，用于连接复位、拒绝错误和非法的数据包； PUSH，显示为P，推送标志位，表示该数据包被对方接收后应立即交给上层应用，而不在缓冲区排队； URGENT，显示为U，紧急标志位，表示数据包的紧急指针域有效，用来保证连接不被阻断，并督促中间设备尽  nginx连接问题。  抓取nginx(端口30880)交互的包：我们知道nginx交互其实是tcp协议，因此使用如下命令 tcpdump -i eth0 tcp and port 30880 -n -nn -C 20 -W 50 -s 0 tcp建立连接：先在服务的机器上执行如下命令，接着把multinode.ks.dev.chenshaowen.com:30880的url在浏览器访问即可。以下标志位含义可以参考上面描述。  [root@master ~]#tcpdump -i eth0 tcp and port 30880 -n -nn -C 20 -W 50 -s 0 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 10:44:36.391603 IP 139.198.254.12.61476 \u0026gt; 192.168.12.2.30880: Flags [S], seq 3950767038, win 64240, options [mss 1360,nop,wscale 8,nop,nop,sackOK], length 0 10:44:36.391864 IP 192.168.12.2.30880 \u0026gt; 139.198.254.12.61476: Flags [S.], seq 1197638836, ack 3950767039, win 29200, options [mss 1460,nop,nop,sackOK,nop,wscale 7], length 0 10:44:36.447960 IP 139.198.254.12.61478 \u0026gt; 192.168.12.2.30880: Flags [.], ack 1, win 515, length 0 10:44:36.448242 IP 139.198.254.12.61476 \u0026gt; 192.168.12.2.30880: Flags [.], ack 1, win 515, length 0 10:44:36.592856 IP 192.168.12.2.30880 \u0026gt; 139.198.254.12.61476: Flags [P.], seq 5250:6642, ack 1511, win 252, length 1392 10:44:36.647585 IP 139.198.254.12.61476 \u0026gt; 192.168.12.2.30880: Flags [.], ack 6642, win 515, length 0 10:44:41.592442 IP 192.168.12.2.30880 \u0026gt; 139.198.254.12.61476: Flags [F.], seq 6642, ack 1511, win 252, length 0 简单分析  当url一打开，tcpdump就有数据显示。看到的S标志位，建立会话连接；接着看到.标志位，对接受包进行确认；然后看到P标志位，表示数据包被对方接受上交给上层应用；最后看到F标志位，表示完成，没有数据要发送。  非服务端tcpdump客户端的数据  一般情况下，我们都是在服务端使用tcpdump工具抓包的；如果需要在非服务端使用tcpdump抓包可以通过网络流量镜像方式，使服务端的流量到目标地址上。  某服务不正常tcpdump测试结果  抓取某不正常的服务(端口30881)交互的包：我们知道nginx交互其实是tcp协议，因此使用如下命令 tcpdump -i eth0 tcp and port 30881 -n -nn -C 20 -W 50 -s 0 tcp建立连接：先在服务的机器上执行如下命令，接着把multinode.ks.dev.chenshaowen.com:30881的url在浏览器访问即可。以下标志位含义可以参考上面描述。  [root@master ~]#tcpdump -i eth0 tcp and port 30881 -n -nn -C 20 -W 50 -s 0 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 10:56:00.569543 IP 139.198.254.12.61608 \u0026gt; 192.168.12.2.30881: Flags [S], seq 1702724216, win 64240, options [mss 1360,nop,wscale 8,nop,nop,sackOK], length 0 10:56:00.569723 IP 192.168.12.2.30881 \u0026gt; 139.198.254.12.61608: Flags [R.], seq 0, ack 1702724217, win 0, length 0 10:56:00.571570 IP 139.198.254.12.61609 \u0026gt; 192.168.12.2.30881: Flags [S], seq 97188145, win 64240, options [mss 1360,nop,wscale 8,nop,nop,sackOK], length 0 10:56:00.571637 IP 192.168.12.2.30881 \u0026gt; 139.198.254.12.61609: Flags [R.], seq 0, ack 97188146, win 0, length 0 简单分析  当url一打开，一开始出现S标志位，表示建立会话连接；接着出现R标志位，表示重置，用于连接复位，拒绝错误和非法的数据包；最后有出现S标志位，再次建立会话连接，一直S与R交替出现，说明该服务不正常。  参考文章 https://dreamgoing.github.io/tcpdump%E5%AE%9E%E6%88%98.html https://klionsec.github.io/2017/01/31/tcpdump-sniffer-pass/#menu https://hubinwei.me/2018/07/25/tcpdump%E6%8A%93%E5%8C%85%E7%BB%83%E4%B9%A0/\n","date":"2019-08-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/tcpdump-package-capture-tutorial/","section":"post","tags":["iptables","Linux tools"],"title":"tcpdump抓包实战教程"},{"categories":null,"contents":"ubuntu快速安装wecenter 以镜像的形式安装wecenter，简化了nginx和php环境的单独安装。WeCenter（wecenter.com）是一款建立知识社区的开源程序（免费版），专注于企业和行业社区内容的整理、归类、检索和分享，是知识化问答社区的首选软件。后台使用PHP开发，MVC架构，前端使用Bootstrap框架。\n准备 mysql需要搭建，参考： https://lilinlinlin.github.io/2019/08/13/docker%E9%83%A8%E7%BD%B2mysql5-7/#more 镜像：wecenter/wecenter:3.3.2\nwecenter安装 docker run --name wecenter1 -p 8081:80 -d wecenter/wecenter:3.3.2 通过外网ip加端口访问,链接为： eip:8081/install\n安装成功，浏览器打开之后  点击下一步，进入配置系统，正确填入数据库的主机、账号、密码和数据库的名称（默认wecenter），点开始安装。 上步成功之后，管理员配置，用户名admin，密码自己定义。  参考文档： http://help.websoft9.com/lamp-guide/installation/wecenter/install.html\n","date":"2019-08-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/ubuntu-quickly-installs-wecenter/","section":"post","tags":["iptables","Linux tools"],"title":"ubuntu快速安装wecenter"},{"categories":null,"contents":"基于keepalived+haproxy部署kubesphere高可用方案 通过keepalived + haproxy实现的，其中keepalived提供一个VIP，通过VIP关联所有的Master节点；然后haproxy提供端口转发功能。由于VIP还是存在Master的机器上的，默认配置API Server的端口是6443，所以我们需要将另外一个端口关联到这个VIP上，一般用8443。\n环境信息： 青云机器操作系统：centos7.5 master1:192.168.0.10 master2:192.168.0.11 master3:192.168.0.12 node:192.168.0.6 vip:192.168.0.200 提前说明及遇到过坑  keepalived提供VIP时，需提前规划的vip不能ping通。 腾讯云服务器上不提供由keepalived方式产生VIP，需要提前在云平台界面HAVIP上创建，创建出的vip再在keepalived.conf中配置。（特别注意这点）。 通过keepalived服务，vip只能在其中的某一个master中看到，如果ip a方式在每个master都看到，说明keepalived有问题。 keepalived+haproxy正常安装之后，检查node节点可以和vip通信。 common.yaml配置文件需要填写正确的ip和转发的端口。 hosts.ini文件master需要添加master1、master2和master3。  keepalived安装和配置，三台master机器都要安装。  安装keepalived, yum install -y keepalived /etc/keepalived/keepalived.conf文件下修改配置,需要修改自己场景的vIP,填写正确服务器的网卡名如：eth0。 其中killall组件，还需要安装yum install psmisc -y。  global_defs { router_id lb-backup } vrrp_script check_haproxy { script \u0026quot;/usr/bin/killall -0 haproxy\u0026quot; interval 2 weight 2 } vrrp_instance VI-kube-master { state MASTER priority 110 dont_track_primary interface eth0 virtual_router_id 90 advert_int 3 virtual_ipaddress { 192.168.0.200 } track_script { check_haproxy } } 重启keepalived服务及断电自启：systemctl enable keepalived;systemctl restart keepalived\nhaproxy安装和配置，三台master机器都要安装。  安装haproyx，yum install -y haproxy /etc/haproxy/haproxy.cfg 文件下修改配置，server服务端分别为master的IP值。  global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy #stats socket /run/haproxy/admin.sock mode 660 level admin stats timeout 30s user haproxy group haproxy daemon nbproc 1 defaults log global timeout connect 5000 timeout client 50000 timeout server 50000 listen kube-master bind 0.0.0.0:8443 mode tcp option tcplog balance roundrobin server master1 192.168.0.10:6443 check inter 10000 fall 2 rise 2 weight 1 server master2 192.168.0.11:6443 check inter 10000 fall 2 rise 2 weight 1 server master3 192.168.0.12:6443 check inter 10000 fall 2 rise 2 weight 1 重启haproxy服务及断电自启：systemctl enable haproxy;systemctl restart haproxy\nhosts.ini配置和common.yaml配置。  hosts.ini配置实例如下：  [all] master1 ansible_connection=local ip=192.168.0.10 master2 ansible_host=192.168.0.11 ip=192.168.0.11 ansible_ssh_pass=**** master3 ansible_host=192.168.0.12 ip=192.168.0.12 ansible_ssh_pass=**** node1 ansible_host=192.168.0.6 ip=192.168.0.6 ansible_ssh_pass=**** [kube-master] master1 master2 master3 [kube-node] node1 [etcd] master1 master2 master3 [k8s-cluster:children] kube-node kube-master  common.yaml的lb配置，注意此处填写VIP，及转发的端口8443。  # apiserver_loadbalancer_domain_name: \u0026quot;lb.kubesphere.local\u0026quot; loadbalancer_apiserver: address: 192.168.0.200 port: 8443 安装kubesphere及结果 kubesphere-all-v2.1.0/scripts目录下，执行./install.sh，选择2+yes即可，然后等待脚本的安装。 node正常的结果：\nkubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master1 Ready master 95m v1.15.5 192.168.0.10 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-862.el7.x86_64 docker://18.9.7 master2 Ready master 88m v1.15.5 192.168.0.11 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-862.el7.x86_64 docker://18.9.7 master3 Ready master 88m v1.15.5 192.168.0.12 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-862.el7.x86_64 docker://18.9.7 node1 Ready worker 86m v1.15.5 192.168.0.6 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-862.el7.x86_64 docker://18.9.7 apiserver中vip生效的结果\ntelnet 192.168.0.200 8443 Trying 192.168.0.200... Connected to 192.168.0.200. Escape character is '^]'. ^CConnection closed by foreign host. ","date":"2019-08-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/deploy-kubesphere-high-availability-solution-based-on-keepalived-haproxy/","section":"post","tags":["iptables","Linux tools"],"title":"基于keepalived-haproxy部署kubesphere高可用方案"},{"categories":null,"contents":"当服务器遇到问题，或者提前查看服务器质量怎么样，一般可以通过以下几点分析：服务器整体情况，CPU使用情况，内存，磁盘，磁盘io，网络io等。\n不同环境插件安装  centos  yum install sysstat -y\n ubuntu  apt install sysstat -y\n1、整体分析之top  执行top指令  root@master2:~# top top - 10:41:30 up 6 days, 11:23, 1 user, load average: 0.98, 0.66, 0.57 Tasks: 304 total, 1 running, 200 sleeping, 0 stopped, 2 zombie %Cpu(s): 3.1 us, 0.7 sy, 0.0 ni, 95.5 id, 0.6 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 32937840 total, 25930000 free, 2666144 used, 4341696 buff/cache KiB Swap: 0 total, 0 free, 0 used. 30586896 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 28484 root 20 0 1468632 1.280g 68832 S 39.1 4.1 260:25.96 kube-apiserver 25442 root 20 0 10.277g 308160 80692 S 9.3 0.9 166:39.40 etcd  第 1 行：系统时间、运行时间、登录终端数、系统负载（三个数值分别为1分钟、5分钟、15分钟内的平均值，数值越小意味着负载越低）。 第 2 行：进程总数、运行中的进程数、睡眠中的进程数、停止的进程数、僵死的进程数。一般情况下，只要没有僵死的进程，就没啥大问题。 第 3 行：用户占用资源百分比、系统内核占用资源百分比、改变过优先级的进程资源百分比、空闲的资源百分比等。 第 4 行：物理内存总量、内存空闲量、内存使用量、作为内核缓存的内存量。 第 5 行：虚拟内存总量、虚拟内存空闲量、虚拟内存使用量、已被提前加载的内存量。 第 6 行里面主要看 PID 和 COMMAND 这两个参数，其中 PID 就是进程 ID ， COMMAND 就是执行的命令，能够看到比较靠前的两个进程都是k8s进程。 干掉僵尸进程的指令，ps -A -ostat,ppid | grep -e '^[Zz]' | awk '{print $2}' | xargs kill -HUP \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 在当前这个界面，按下数字键盘 1 能够看到各个 CPU 的详细利用率  top - 10:46:53 up 6 days, 11:29, 1 user, load average: 0.15, 0.53, 0.57 Tasks: 303 total, 1 running, 200 sleeping, 0 stopped, 2 zombie %Cpu0 : 1.7 us, 1.3 sy, 0.0 ni, 96.3 id, 0.7 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu1 : 2.0 us, 0.7 sy, 0.0 ni, 97.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu2 : 0.7 us, 0.3 sy, 0.0 ni, 99.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st 2、cpu分析之vmstat 一般 vmstat 工具的使用是通过两个数字参数来完成的，第一个参数是采样的时间间隔，单位是秒，第二个参数是采样的次数，这次的命令是：vmstat -n 3 2 意思就是隔 3 秒取样一次，一共取样 2 次。\n 执行vmstat指令  root@master2:~# vmstat -n 3 2 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 25910152 684436 3669440 0 0 1 38 4 0 5 2 89 1 3 3 0 0 25910008 684436 3669488 0 0 0 441 8905 15261 1 1 97 1 0 2.1、procs:  r ：运行和等待 CPU 时间片的进程数，一般来说整个系统的运行队列不要超过总核数的 2 倍，要不然系统压力太大了。 b : 等待资源的进程数，比如正在等待磁盘 IO ，网络 IO 这种。  2.2、cpu：  us ：用户进程消耗 CPU 时间百分比， us 值高的话，说明用户进程消耗 CPU 时间比较长，如果长期大于 50% 的话，那就说明程序还有需要优化的地方。 sy ：内核进程消耗的 CPU 时间百分比。 us + sy 参考值为 80% ，如果大于 80% 的话，说明可能存在 CPU 不足。  3、内存分析之free 一般我们使用free -m即可\nroot@master2:~# free -m total used free shared buff/cache available Mem: 32165 2608 25354 13 4203 29861 Swap: 0 0 0  如果应用程序可用内存/系统物理内存大于 70% 的话，说明内存是充足的，没啥问题，但是如果小于 20% 的话，就要考虑增加内存了。  4、磁盘分析之df 排查磁盘问题，首先要排查磁盘空间够不够，df和du就可以。df查看磁盘使用情况；du查看目录占用磁盘情况及子文件占用情况。\nroot@master2:~# df -hT Filesystem Type Size Used Avail Use% Mounted on udev devtmpfs 16G 0 16G 0% /dev tmpfs tmpfs 3.2G 14M 3.2G 1% /run /dev/vda2 ext4 99G 9.4G 84G 11% / tmpfs tmpfs 16G 0 16G 0% /dev/shm tmpfs tmpfs 5.0M 0 5.0M 0% /run/lock tmpfs tmpfs 16G 0 16G 0% /sys/fs/cgroup root@master2:~# du -sh 516M . root@master2:~# du -sh * 16K 1.sh 434M etcd 82M etcd.tar.gz 4.0K etcd.txt root@master2:~# du -h --max-depth=1 4.0K ./.cache 8.0K ./.gnupg 8.0K ./.ansible 434M ./etcd 948K ./.kube 8.0K ./.ssh 8.0K ./.vim 516M . 5、磁盘io分析之iostat 在对数据库进行操作时，第一要考虑就是磁盘io操作，因为相对来说，如果在某个时间段给磁盘进行大量的写入操作会造成程序等待时间长，导致客户端那边好久都没啥反应。\nroot@master2:~# iostat -xdk 3 2 Linux 4.15.0-115-generic (master2) 09/11/2020 _x86_64_ (16 CPU) Device r/s w/s rkB/s wkB/s rrqm/s wrqm/s %rrqm %wrqm r_await w_await aqu-sz rareq-sz wareq-sz svctm %util loop0 0.00 0.00 0.02 0.00 0.00 0.00 0.00 0.00 1.75 0.00 0.00 18.07 0.00 0.16 0.00 loop1 0.05 0.00 0.07 0.00 0.00 0.00 0.00 0.00 1.66 0.00 0.00 1.49 0.00 0.06 0.00 vda 0.74 19.38 18.68 154.10 0.00 10.14 0.07 34.35 6.74 17.52 0.28 25.39 7.95 0.79 1.60 Device r/s w/s rkB/s wkB/s rrqm/s wrqm/s %rrqm %wrqm r_await w_await aqu-sz rareq-sz wareq-sz svctm %util loop0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 loop1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 vda 0.00 7.00 0.00 61.33 0.00 6.00 0.00 46.15 0.00 1.33 0.00 0.00 8.76 0.00 0.00  rkB/s ：每秒读取数据量 kB 。 wkB/s ：每秒写入数据量 kB 。 svctm ：I/O 请求的平均服务时间，单位毫秒。 util ：一秒中有百分之几的时间用于 I/O 操作，如果接近 100% 说明磁盘带宽跑满了，这个时候就要优化程序或者增加磁盘了。  6、网络io分析之sar 网络 IO 的话，可以通过 sar -n DEV 3 2 这条命令来看，和上面的差不多，意思就是每隔 3 秒取样一次，一共取样 2 次。\nroot@master2:~# sar -n DEV 3 2 Linux 4.15.0-115-generic (master2) 09/11/2020 _x86_64_ (16 CPU) 11:06:46 AM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil 11:06:49 AM eth0 854.33 859.33 199.93 210.99 0.00 0.00 0.00 0.00 11:06:49 AM tunl0 113.67 108.33 10.92 9.86 0.00 0.00 0.00 0.00 11:06:49 AM calide035c655d8 98.33 108.33 11.04 12.63 0.00 0.00 0.00 0.00 11:06:49 AM lo 58.00 58.00 13.18 13.18 0.00 0.00 0.00 0.00 11:06:49 AM cali3d31c61f18c 6.00 3.33 4.41 2.08 0.00 0.00 0.00 0.00 11:06:49 AM nodelocaldns 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 11:06:49 AM kube-ipvs0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 11:06:49 AM cali7d0b83575fc 7.00 8.00 2.46 0.81 0.00 0.00 0.00 0.00 11:06:49 AM cali056accc6554 24.33 21.00 2.93 10.81 0.00 0.00 0.00 0.00 11:06:49 AM docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 11:06:49 AM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil 11:06:52 AM eth0 623.67 620.00 109.84 127.33 0.00 0.00 0.00 0.00 11:06:52 AM tunl0 97.33 90.00 6.64 8.74 0.00 0.00 0.00 0.00 11:06:52 AM calide035c655d8 93.67 104.00 10.52 8.68 0.00 0.00 0.00 0.00 11:06:52 AM lo 65.67 65.67 11.60 11.60 0.00 0.00 0.00 0.00 11:06:52 AM cali3d31c61f18c 6.00 3.67 5.62 2.78 0.00 0.00 0.00 0.00 11:06:52 AM nodelocaldns 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 11:06:52 AM kube-ipvs0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 11:06:52 AM cali7d0b83575fc 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 11:06:52 AM cali056accc6554 3.67 2.33 0.24 0.39 0.00 0.00 0.00 0.00 11:06:52 AM docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Average: IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil Average: eth0 739.00 739.67 154.89 169.16 0.00 0.00 0.00 0.00 Average: tunl0 105.50 99.17 8.78 9.30 0.00 0.00 0.00 0.00 Average: calide035c655d8 96.00 106.17 10.78 10.65 0.00 0.00 0.00 0.00 Average: lo 61.83 61.83 12.39 12.39 0.00 0.00 0.00 0.00 Average: cali3d31c61f18c 6.00 3.50 5.02 2.43 0.00 0.00 0.00 0.00 Average: nodelocaldns 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Average: kube-ipvs0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Average: cali7d0b83575fc 3.50 4.00 1.23 0.40 0.00 0.00 0.00 0.00 Average: cali056accc6554 14.00 11.67 1.58 5.60 0.00 0.00 0.00 0.00 Average: docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  IFACE ：LAN 接口 rxpck/s ：每秒钟接收的数据包 txpck/s ：每秒钟发送的数据包 rxKB/s ：每秒接收的数据量，单位 KByte txKB/s ：每秒发出的数据量，单位 KByte rxcmp/s ：每秒钟接收的压缩数据包 txcmp/s ：每秒钟发送的压缩数据包 rxmcst/s：每秒钟接收的多播数据包 ","date":"2019-08-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/tools-to-troubleshoot-linux-servers/","section":"post","tags":["iptables","Linux tools"],"title":"排查linux服务器的工具"},{"categories":null,"contents":"提前预防K8s集群资源不足的处理方式配置 在管理集群的时候我们常常会遇到资源不足的情况，在这种情况下我们要保证整个集群可用，并且尽可能减少应用的损失。根据该问题提出以下两种方案：一种为优化kubelet参数，另一种为脚本化诊断处理。\n概念解释  CPU 的使用时间是可压缩的，换句话说它本身无状态，申请资源很快，也能快速正常回收。 内存（memory）大小是不可压缩的，因为它是有状态的（内存里面保存的数据），申请资源很慢（需要计算和分配内存块的空间），并且回收可能失败（被占用的内存一般不可回收）。  优化kubelet参数 优化kubelet参数通过k8s资源表示、节点资源配置及kubelet参数设置、应用优先级和资源动态调整这几个方面来介绍。k8s资源表示为yaml文件中如何添加requests和limites参数。节点资源配置及kubelet参数设置描述为一个node上面资源配置情况，从而来优化kubelet参数。应用优先级描述为当资源不足时，优先保留那些pod不被驱逐。资源动态调整描述为运算能力的增减，如：HPA 、VPA和Cluster Auto Scaler。\nk8s资源表示  在k8s中，资源表示配置字段是 spec.containers[].resource.limits/request.cpu/memory。yaml格式如下：  spec: template: ... spec: containers: ... resources: limits: cpu: \u0026quot;1\u0026quot; memory: 1000Mi requests: cpu: 20m memory: 100Mi 资源动态调整 动态调整的思路：应用的实际流量会不断变化，因此使用率也是不断变化的，为了应对应用流量的变化，我们应用能够自动调整应用的资源。比如在线商品应用在促销的时候访问量会增加，我们应该自动增加 pod 运算能力来应对；当促销结束后，有需要自动降低 pod 的运算能力防止浪费。 运算能力的增减有两种方式：改变单个 pod 的资源，已经增减 pod 的数量。这两种方式对应了 kubernetes 的 HPA 和 VPA和Cluster Auto Scaler。\n HPA: 横向 pod 自动扩展的思路是这样的：kubernetes 会运行一个 controller，周期性地监听 pod 的资源使用情况，当高于设定的阈值时，会自动增加 pod 的数量；当低于某个阈值时，会自动减少 pod 的数量。自然，这里的阈值以及 pod 的上限和下限的数量都是需要用户配置的。 VPA: VPA 调整的是单个 pod 的 request 值（包括 CPU 和 memory）VPA 包括三个组件： （1）Recommander：消费 metrics server 或者其他监控组件的数据，然后计算 pod 的资源推荐值 （2）Updater：找到被 vpa 接管的 pod 中和计算出来的推荐值差距过大的，对其做 update 操作（目前是 evict，新建的 pod 在下面 admission controller 中会使用推荐的资源值作为 request） （3）Admission Controller：新建的 pod 会经过该 Admission Controller，如果 pod 是被 vpa 接管的，会使用 recommander 计算出来的推荐值 CLuster Auto Scaler：能够根据整个集群的资源使用情况来增减节点。Cluster Auto Scaler 就是监控这个集群因为资源不足而 pending 的 pod，根据用户配置的阈值调用公有云的接口来申请创建机器或者销毁机器。  节点资源配置及kubelet参数设置 节点资源的配置一般分为 2 种：\n 资源预留：为系统进程和 k8s 进程预留资源 pod 驱逐：节点资源到达一定使用量，开始驱逐 pod     Node Capacity     kube-reserved   system-reserved   eviction-threshold   Allocatable     Node Capacity：Node的所有硬件资源 kube-reserved：给kube组件预留的资源：kubelet,kube-proxy以及docker等 system-reserved：给system进程预留的资源 eviction-threshold：kubelet eviction的阈值设定 Allocatable：真正scheduler调度Pod时的参考值（保证Node上所有Pods的request resource不超过Allocatable）  allocatable的值即对应 describe node 时看到的allocatable容量，pod 调度的上限\n计算公式：节点上可配置值 = 总量 - 预留值 - 驱逐阈值 Allocatable = Capacity - Reserved(kube+system) - Eviction Threshold 以上配置均在kubelet 中添加，涉及的参数有：\n--kube-reserved=cpu=200m,memory=250Mi \\ --system-reserved=cpu=200m,memory=250Mi \\ --eviction-hard=memory.available\u0026lt;5%,nodefs.available\u0026lt;10%,imagefs.available\u0026lt;10% \\ --eviction-soft=memory.available\u0026lt;10%,nodefs.available\u0026lt;15%,imagefs.available\u0026lt;15% \\ --eviction-soft-grace-period=memory.available=2m,nodefs.available=2m,imagefs.available=2m \\ --eviction-max-pod-grace-period=120 \\ --eviction-pressure-transition-period=30s \\ --eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=500Mi 以上配置均为百分比，举例：\n以2核4GB内存40GB磁盘空间的配置为例，Allocatable是1.6 CPU，3.3Gi 内存，25Gi磁盘。当pod的总内存消耗大于3.3Gi或者磁盘消耗大于25Gi时，会根据相应策略驱逐pod。 Allocatable = 4Gi - 250Mi -250Mi - 4Gi*5% = 3.3Gi\n（1） 配置 k8s组件预留资源的大小，CPU、Mem\n指定为k8s系统组件（kubelet、kube-proxy、dockerd等）预留的资源量， 如：--kube-reserved=cpu=1,memory=2Gi,ephemeral-storage=1Gi。 这里的kube-reserved只为非pod形式启动的kube组件预留资源，假如组件要是以static pod（kubeadm）形式启动的，那并不在这个kube-reserved管理并限制的cgroup中，而是在kubepod这个cgroup中。 （ephemeral storage需要kubelet开启feature-gates，预留的是临时存储空间（log，EmptyDir），生产环境建议先不使用） ephemeral-storage是kubernetes1.8开始引入的一个资源限制的对象，kubernetes 1.10版本中kubelet默认已经打开的了,到目前1.11还是beta阶段，主要是用于对本地临时存储使用空间大小的限制，如对pod的empty dir、/var/lib/kubelet、日志、容器可读写层的使用大小的限制。 （2）配置 系统守护进程预留资源的大小（预留的值需要根据机器上容器的密度做一个合理的值）\n含义：为系统守护进程(sshd, udev等)预留的资源量， 如：--system-reserved=cpu=500m,memory=1Gi,ephemeral-storage=1Gi。 注意，除了考虑为系统进程预留的量之外，还应该为kernel和用户登录会话预留一些内存。 （3）配置 驱逐pod的硬阈值\n含义：设置进行pod驱逐的阈值，这个参数只支持内存和磁盘。 通过--eviction-hard标志预留一些内存后，当节点上的可用内存降至保留值以下时， kubelet 将会对pod进行驱逐。 配置：--eviction-hard=memory.available\u0026lt;5%,nodefs.available\u0026lt;10%,imagefs.available\u0026lt;10% （4）配置 驱逐pod的软阈值\n--eviction-soft=memory.available\u0026lt;10%,nodefs.available\u0026lt;15%,imagefs.available\u0026lt;15% （5）定义达到软阈值之后，持续时间超过多久才进行驱逐\n--eviction-soft-grace-period=memory.available=2m,nodefs.available=2m,imagefs.available=2m （6）驱逐pod前最大等待时间=min(pod.Spec.TerminationGracePeriodSeconds, eviction-max-pod-grace-period)，单位为秒\n--eviction-max-pod-grace-period=120 （7）至少回收的资源量\n--eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=500Mi （8）防止波动,kubelet 多久才上报节点的状态\n--eviction-pressure-transition-period=30s 应用优先级 当资源不足时，配置了如上驱逐参数，pod之间的驱逐顺序是怎样的呢？以下描述设置不同优先级来确保集群中核心的组件不被驱逐还正常运行，OOM 的优先级如下,pod oom 值越低，也就越不容易被系统杀死。：\nBestEffort Pod \u0026gt; Burstable Pod \u0026gt; 其它进程（内核init进程等） \u0026gt; Guaranteed Pod \u0026gt; kubelet/docker 等 \u0026gt; sshd 等进程 kubernetes 把 pod 分成了三个 QoS 等级，而其中和limits和requests参数有关：\n Guaranteed：oom优先级最低，可以考虑数据库应用或者一些重要的业务应用。除非 pods 使用超过了它们的 limits，或者节点的内存压力很大而且没有 QoS 更低的 pod，否则不会被杀死。 Burstable：这种类型的 pod 可以多于自己请求的资源（上限有 limit 指定，如果 limit 没有配置，则可以使用主机的任意可用资源），但是重要性认为比较低，可以是一般性的应用或者批处理任务。 Best Effort：oom优先级最高，集群不知道 pod 的资源请求情况，调度不考虑资源，可以运行到任意节点上（从资源角度来说），可以是一些临时性的不重要应用。pod 可以使用节点上任何可用资源，但在资源不足时也会被优先杀死。 Pod 的 requests 和 limits 是如何对应到这三个 QoS 等级上的，可以用下面一张表格概括：     request是否配置 limits是否配置 两者的关系 Qos 说明     是 是 requests=limits Guaranteed 所有容器的cpu和memory都必须配置相同的requests和limits   是 是 request\u0026lt;limit Burstable 只要有容器配置了cpu或者memory的request和limits就行   是 否  Burstable 只要有容器配置了cpu或者memory的request就行   否 是  Guaranteed/Burstable 如果配置了limits，k8s会自动把对应资源的request设置和limits一样。如果所有容器所有资源都配置limits，那就是Guaranteed;如果只有部分配置了limits，就是Burstable   否 否  Best Effort 所有的容器都没有配置资源requests或limits    说明：\n request和limits相同，可以参考资源动态调整中的VPA设置合理值。 如果只配置了limits，没有配置request，k8s会把request值和limits值一样。 如果只配置了request，没有配置limits，该pod共享node上可用的资源，实际上很反对这样设置。  总结 动态地资源调整通过 kubelet 驱逐程序进行的，但需要和应用优先级配合使用才能达到很好的效果，否则可能驱逐集群中核心组件。\n脚本化诊断处理 什么叫脚本化诊断处理呢？它的含义为：当集群中的某台机器资源（一般指memory）用到85%-90%时，脚本自动检查到且该节点为不可调度。缺点为：背离了资源动态调整中CLuster Auto Scaler特点。\n 集群中每台机器都可以执行kubectl指令： 如果没有设置，可将master机器上的$HOME/.kube/config文件拷贝到node机器上。 可以通过shell-operator自动诊断机器资源且做cordon操作处理 脚本中关键说明 （1）获取本地IP：ip a | grep \u0026lsquo;state UP\u0026rsquo; -A2| grep inet | grep -v inet6 | grep -v 127 | sed \u0026rsquo;s/^[ \\t]*//g\u0026rsquo; | cut -d ' ' -f2 | cut -d \u0026lsquo;/\u0026rsquo; -f1 （2）获取本地ip对应的node名：kubectl get nodes -o wide | grep \u0026ldquo;本地ip\u0026rdquo; | awk \u0026lsquo;{print $1}\u0026rsquo; （3）不可调度：kubectl cordon node \u0026lt;node名\u0026gt; （4）获取总内存： free -m | awk \u0026lsquo;NR==2{print $2}\u0026rsquo; （5）获取使用内存： free -m | awk \u0026lsquo;NR==2{print $3}\u0026rsquo;  参考文档 https://kubernetes.io/zh/docs/tasks/administer-cluster/out-of-resource/ https://cizixs.com/2018/06/25/kubernetes-resource-management/ https://segmentfault.com/a/1190000021402192\n","date":"2019-08-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/prevent-the-configuration-of-the-k8s-cluster-from-under-resourcing-in-advance/","section":"post","tags":["iptables","Linux tools"],"title":"提前预防K8s集群资源不足的处理方式配置"},{"categories":null,"contents":" iperf工具是测量服务器网络速度工具，它通过测量服务器可以处理的最大网络吞吐量来测试网络速度，在遇到网络问题时特别有用。  1、下载源代码（服务端和客户端都要安装） wget https://iperf.fr/download/source/iperf-2.0.8-source.tar.gz\n 安装编译环境 yum install gcc-c++ -y 解压并安装iperf  tar -xvf iperf-2.0.8-source.tar.gz cd iperf-2.0.8/ ./configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install 2、测试 2.1、服务端执行iperf指令 iperf -s -p 12345 -i 1\n -s表示以服务器模式运行。 -p设置服务监听端口，测试时该端口在服务上没有被占用即可。 -i设置每次报告之间的时间间隔，单位为s。  iperf -s -p 12345 -i 1 ------------------------------------------------------------ Server listening on TCP port 12345 TCP window size: 85.3 KByte (default) ------------------------------------------------------------ [ 4] local 192.168.0.8 port 12345 connected with 192.168.0.10 port 52648 [ ID] Interval Transfer Bandwidth [ 4] 0.0- 1.0 sec 119 MBytes 999 Mbits/sec [ 4] 1.0- 2.0 sec 60.3 MBytes 506 Mbits/sec [ 4] 2.0- 3.0 sec 61.7 MBytes 517 Mbits/sec [ 4] 3.0- 4.0 sec 60.9 MBytes 511 Mbits/sec [ 4] 4.0- 5.0 sec 59.9 MBytes 503 Mbits/sec [ 4] 5.0- 6.0 sec 61.1 MBytes 512 Mbits/sec 2.2、客户端执行iperf指令 iperf -c XX.XX.XX.XX -p 1234 -i 1\n 其中XX.XX.XX.XX为服务端的ip。 -p要和服务端设置的相同。  iperf -c 192.168.0.8 -p 12345 -i 1 ------------------------------------------------------------ Client connecting to 192.168.0.8, TCP port 12345 TCP window size: 45.0 KByte (default) ------------------------------------------------------------ [ 3] local 192.168.0.10 port 52648 connected with 192.168.0.8 port 12345 [ ID] Interval Transfer Bandwidth [ 3] 0.0- 1.0 sec 123 MBytes 1.03 Gbits/sec [ 3] 1.0- 2.0 sec 60.1 MBytes 504 Mbits/sec [ 3] 2.0- 3.0 sec 62.0 MBytes 520 Mbits/sec [ 3] 3.0- 4.0 sec 60.4 MBytes 506 Mbits/sec [ 3] 4.0- 5.0 sec 60.2 MBytes 505 Mbits/sec [ 3] 5.0- 6.0 sec 60.6 MBytes 509 Mbits/sec [ 3] 6.0- 7.0 sec 62.1 MBytes 521 Mbits/sec [ 3] 7.0- 8.0 sec 59.4 MBytes 498 Mbits/sec 参考 iperf\n","date":"2019-08-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/iperf-cognition-as-a-network-speed-measurement-tool/","section":"post","tags":["iptables","Linux tools"],"title":"测量网络速度工具之iperf认知"},{"categories":null,"contents":"通常情况下，我们的kubernetes集群是内网环境，如果希望通过本地访问这个集群，怎么办呢？大家想到的是Kubeadm在初始化的时候会为管理员生成一个 Kubeconfig文件，把它下载下来 是不是就可以？事实证明这样不行， 因为这个集群是内网集群，Kubeconfig文件 中APIServer的地址是内网ip。解决方案很简单，把公网ip签到证书里面就可以，其中有apiServerCertSANs这个选项，只要把公网IP写到这里，再启动这个集群的时候，这个公网ip就会签到证书里。\n1. 环境信息  安装方式：kubeadm 内网IP：192.168.0.8 外网IP：139.198.19.37 证书目录：/etc/kubernetes/pki kubeadm配置文件目录：/etc/kubernetes/kubeadm-config.yaml  2. 查看apiserver的证书包含的ip,进入到证书目录执行 cd /etc/kubernetes/pki openssl x509 -in apiserver.crt -noout -text\nopenssl x509 -in apiserver.crt -noout -text Certificate: Data: ................ Validity Not Before: Jun 5 02:26:44 2020 GMT Not After : Jun 5 02:26:44 2021 GMT .................. X509v3 extensions: .......... X509v3 Subject Alternative Name: IP Address:192.168.0.8 ....... 3. 添加公网IP到apiserver 绑定的公网ip为 139.198.19.37 ，确保公网ip的防火墙已经打开6443端口\n 3.1 登录到主节点，进入 /etc/kubernetes/目录下 3.2 修改kubeadm-config.yaml，找到 ClusterConfiguration 中的 certSANs (如无，在 apiServer 下添加这一配置)，如下。添加刚才绑定的 139.198.19.37 到 certSANs 下，保存文件。  apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration etcd: external: endpoints: - https://192.168.0.8:2379 apiServer: extraArgs: authorization-mode: Node,RBAC timeoutForControlPlane: 4m0s certSANs: - 192.168.0.8 - 139.198.19.37  3.3 执行如下命令更新 apiserver.crt apiserver.key 注意需要把之前apiserver.crt apiserver.key做备份,进入到pki目录下，执行如下指令做备份： mv apiserver.crt apiserver.crt-bak mv apiserver.key apiserver.key-bak 备份完之后，回到/etc/kubernetes目录下，执行公网ip添加到apiserver操作指令为： kubeadm init phase certs apiserver \u0026ndash;config kubeadm-config.yaml  kubeadm init phase certs apiserver --config kubeadm-config.yaml [certs] Generating \u0026quot;apiserver\u0026quot; certificate and key [certs] apiserver serving cert is signed for DNS names [192.168.0.8 139.198.19.37]  3.4 再次查看apiserver中证书包含的ip，指令如下,看的公网ip则操作成功。 openssl x509 -in pki/apiserver.crt -noout -text | grep 139.198.19.37  openssl x509 -in pki/apiserver.crt -noout -text | grep 139.198.19.37 IP Address:192.168.0.8, IP Address:139.198.19.37  3.5 重启kube-apiserver 如果是高可用集群，直接杀死当前节点的kube-apiserver进程，等待kubelet拉起kube-apiserver即可。需要在三个节点执行步骤1到步骤4，逐一更新。 如果是非高可用集群，杀死kube-apiserver可能会导致服务有中断，需要在业务低峰的时候操作。 进入/etc/kubernetes/manifests目录下，mv kube-apiserver.yaml文件至别的位置，然后又移回来即可 3.6 修改kubeconfig中的server ip地址为 139.198.19.37，保存之后就可以直接通过公网访问kubernetes集群 kubectl --kubeconfig config config view kubectl --kubeconfig config get node  4. 附录-证书过期处理方式  kubeadm部署的k8s集群，默认证书目录为：/etc/kubernetes/pki,如果非pki，以ssl为例，需要创建软链接。证书过期包含核心组件apiserver和node上的token。 4.1 master节点-apiserver处理方式  1、查看证书有效期 cd /etc/kubernetes openssl x509 -in ssl/apiserver.crt -noout -enddate 2. 更新过期证书（/etc/kubernetes） (先在master1 节点执行) 创建软连接pki -\u0026gt; ssl ： ln -s ssl/ pki (如pki存在，可略过) kubeadm alpha certs renew apiserver kubeadm alpha certs renew apiserver-kubelet-client kubeadm alpha certs renew front-proxy-client 3. 更新kubeconfig（/etc/kubernetes）(master1 节点) 需更新admin.conf / scheduler.conf / controller-manager.conf / kubelet.conf kubeadm alpha certs renew admin.conf kubeadm alpha certs renew controller-manager.conf kubeadm alpha certs renew scheduler.conf 特别注意：以master1为例，将如下master1替换实际的节点名称。 kubeadm alpha kubeconfig user --client-name=system:node:master1 --org=system:nodes \u0026gt; kubelet.conf 4. 如上述kubeconfig中apiserver地址非lb地址，则修改为lb地址：(master1 节点) https://192.168.0.13:6443 -\u0026gt; https://{ lb domain or ip }:6443 5. 重启k8s master组件：(master1 节点) docker ps -af name=k8s_kube-apiserver* -q | xargs --no-run-if-empty docker rm -f docker ps -af name=k8s_kube-scheduler* -q | xargs --no-run-if-empty docker rm -f docker ps -af name=k8s_kube-controller-manager* -q | xargs --no-run-if-empty docker rm -f systemctl restart kubelet 6. 验证kubeconfig有效性及查看节点状态 (master1 节点) kubectl get node –kubeconfig admin.conf kubectl get node –kubeconfig scheduler.conf kubectl get node –kubeconfig controller-manager.conf kubectl get node –kubeconfig kubelet.conf 7. 特别注意：同步master1证书/etc/kubernetes/ssl至master2、master3的对应路径中/etc/kubernetes/ssl（同步前建议备份旧证书） 证书路径：/etc/kubernetes/ssl 8. 更新kubeconfig（/etc/kubernetes）(master2, master3) kubeadm alpha certs renew admin.conf kubeadm alpha certs renew controller-manager.conf kubeadm alpha certs renew scheduler.conf 特别注意：以下命令中以master2、master3为例，将如下master2/master3替换实际的节点名称。 kubeadm alpha kubeconfig user --client-name=system:node:master2 --org=system:nodes \u0026gt; kubelet.conf （master2） kubeadm alpha kubeconfig user --client-name=system:node:master3 --org=system:nodes \u0026gt; kubelet.conf （master3） 9. 如上述kubeconfig中apiserver地址非lb地址，则修改为lb地址：(master2、master3) https://192.168.0.13:6443 -\u0026gt; https://{ lb domain or ip }:6443 注：涉及文件：admin.conf、controller-manager.conf、scheduler.conf、kubelet.conf 10. 重启master2、master3中对应master组件 docker ps -af name=k8s_kube-apiserver* -q | xargs --no-run-if-empty docker rm -f docker ps -af name=k8s_kube-scheduler* -q | xargs --no-run-if-empty docker rm -f docker ps -af name=k8s_kube-controller-manager* -q | xargs --no-run-if-empty docker rm -f systemctl restart kubelet 11. 验证kubeconfig有效性 （master2、master3） kubectl get node –kubeconfig admin.conf kubectl get node –kubeconfig scheduler.conf kubectl get node –kubeconfig controller-manager.conf kubectl get node –kubeconfig kubelet.conf 12. 更新~/.kube/config （master1、master2、master3） cp admin.conf ~/.kube/config 注：如node节点也需使用kubectl，将master1上的~/.kube/config拷贝至对应node节点~/.kube/config 13. 验证~/.kube/config有效性： kubectl get node 查看集群状态  4.2 node节点token证书处理方式  1. kubeadm token list 查看输出若为空或显示日期过期，则需重新生成。 2. kubeadm token create 重新生成token 3. 记录token值,保存下来。 4. 替换node节点/etc/kubernetes/ bootstrap-kubelet.conf中token （所有node节点） 5. 删除/etc/kubernetes/kubelet.conf （所有node节点） rm -rf /etc/kubernetes/kubelet.conf 6. 重启kubelet （所有node节点） systemctl restart kubelet 7. 查看节点状态： kubectl get node 验证集群状态 ","date":"2019-08-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/add-public-ip-to-kubernetes-apiserver-operation-guide/","section":"post","tags":["K8s","Certificate"],"title":"添加公网ip到kubernetes的apiserver操作指南"},{"categories":null,"contents":"Shell-operator是用于在Kubernetes集群中运行事件驱动脚本工具。Shell-operator通过脚本作为事件触发的钩子（hook），在Kubernetes集群事件和Shell脚本之间提供了一个转化层。触发钩子包含add, update和delete。以pod add为例通俗的话说，当新创建了一个pod时，会自动触发脚本中else部分。\nShell-operator特点：  轻松管理Kubernetes集群：可以是bash，python和kubectl。 Kubernetes对象事件：钩子触发包含add, update或delete事件。 对象选择器和属性过滤器：可以监视一组特定的对象并检测其属性的变化。 配置简单：钩子绑定语法格式为yaml/json输出。  1、环境准备（只要Kubernetes环境即可） Kubernetes: v1.18.3\n2、快速开始（包含bash和python实例） 目录结构\n[root@node1 shell-operator]# tree . ├── Dockerfile ├── hooks │ └── pods-hook.sh └── shell-operator-pod.yaml 2.1、Shell-operator最简设置步骤为：  用钩子（脚本）构建的镜像。 在Kubernetes集群中创建必要的RBAC对象。 使用构建的镜像运行一个pod/deployment。  2.2、用钩子脚本构建镜像  钩子是一个脚本，当执行\u0026ndash;config选项时，配置将以yaml/json格式输出。 以下创建一个简单的operator将来监视所有namespaces下的所有pod，并记录新pod的名字。 包含pods-hook.sh和Dockerfile  以bash脚本为例\n pods-hook.sh  #!/usr/bin/env bash if [[ $1 == \u0026quot;--config\u0026quot; ]] ; then cat \u0026lt;\u0026lt;EOF configVersion: v1 kubernetes: - apiVersion: v1 kind: Pod executeHookOnEvent: [\u0026quot;Added\u0026quot;] EOF else podName=$(jq -r .[0].object.metadata.name $BINDING_CONTEXT_PATH) echo \u0026quot;Pod '${podName}' added\u0026quot; fi  添加执行权限。  chmod +x pods-hook.sh\n 基于flant/shell-operator:latest的基础镜像构建新的Dockerfile。  FROM flant/shell-operator:latest ADD pods-hook.sh /hooks  构建一个新的镜像。  docker build -t lilinlinlin/shell-operator:monitor-pods\n 推镜像至dockerhub, 仓库名根据自身的情况而定，也可以不操作此步骤。  docker push lilinlinlin/shell-operator:monitor-pods\n2.3、创建RBAC对象 需要监视所有namespaces下的pods，意味着我们需要shell-operator的特定RBAC定义。\nkubectl create namespace example-monitor-pods kubectl create serviceaccount monitor-pods-acc --namespace example-monitor-pods kubectl create clusterrole monitor-pods --verb=get,watch,list --resource=pods kubectl create clusterrolebinding monitor-pods --clusterrole=monitor-pods --serviceaccount=example-monitor-pods:monitor-pods-acc 2.4、在集群中部署shell-operator shell-operator-pod.yaml文件为\napiVersion: v1 kind: Pod metadata: name: shell-operator spec: containers: - name: shell-operator image: lilinlinlin/shell-operator:monitor-pods imagePullPolicy: IfNotPresent serviceAccountName: monitor-pods-acc  部署shell-operator时，pods-hook.sh脚本中if的部分就会被执行。新创建pod时，else部分就会被执行。  kubectl -n example-monitor-pods apply -f shell-operator-pod.yaml\n3、测试验证效果 部署一个nginx服务,查看日志。会出现Pod nginx-****** added字样，说明监视生效了。\nkubectl run nginx --image=nginx kubectl -n example-monitor-pods logs pod/shell-operator -f ... INFO[0027] queue task HookRun:main operator.component=handleEvents queue=main INFO[0030] Execute hook binding=kubernetes hook=pods-hook.sh operator.component=taskRunner queue=main task=HookRun INFO[0030] Pod 'nginx-775dd7f59c-hr7kj' added binding=kubernetes hook=pods-hook.sh output=stdout queue=main task=HookRun INFO[0030] Hook executed successfully binding=kubernetes hook=pods-hook.sh operator.component=taskRunner queue=main task=HookRun ... 4、python实例 实例中直接运行else部分。\n1、hooks/00-hook.py #!/usr/bin/env python import sys if __name__ == \u0026quot;__main__\u0026quot;: if len(sys.argv)\u0026gt;1 and sys.argv[1] == \u0026quot;--config\u0026quot;: print '{\u0026quot;configVersion\u0026quot;:\u0026quot;v1\u0026quot;, \u0026quot;onStartup\u0026quot;: 10}' else: print \u0026quot;OnStartup Python powered hook\u0026quot; 2、Dockerfile FROM flant/shell-operator:latest-alpine3.11 RUN apk --no-cache add python ADD hooks /hooks 3、shell-operator-pod.yaml --- apiVersion: v1 kind: Pod metadata: name: shell-operator spec: containers: - name: shell-operator image: registry.mycompany.com/shell-operator:startup-python imagePullPolicy: IfNotPresent 4、运行 docker build -t \u0026quot;registry.mycompany.com/shell-operator:startup-python\u0026quot; . kubectl create ns example-startup-python kubectl -n example-startup-python apply -f shell-operator-pod.yaml kubectl -n example-startup-python logs -f po/shell-operator 5、清理环境 kubectl delete ns example-monitor-pods kubectl delete clusterrole monitor-pods kubectl delete clusterrolebinding monitor-pods 6、参考 shell-operator\n","date":"2019-08-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/scripting-the-tool-shell-operator-that-triggers-kubernetes-cluster-events/","section":"post","tags":["iptables","Linux tools"],"title":"脚本化触发Kubernetes集群事件的工具-Shell-operator"},{"categories":["storage"],"contents":"ceph介绍 ceph为块存储，以下通过cephadm安装ceph集群，ceph主要有三个基本角色：\n OSD 用于集群中所有数据与对象的存储，处理集群数据的复制、恢复、回填、再均衡，并向其他osd守护进程发送心跳，然后向 Monitor 提供一些监控信息。 Monitor 监控整个集群的状态，维护集群的 cluster MAP 二进制表，保证集群数据的一致性。 MDS (可选) 为 Ceph 文件系统提供元数据计算、缓存与同步。MDS 进程并不是必须的进程，只有需要使用 CephFS 时，才需要配置 MDS 节点。  1、环境准备 1.1、节点规划    节点  os  IP  磁盘 角色     ceph1 centos 192.168.0.13 vdc cephadm，mgr, mon,osd   ceph2 centos 192.168.0.14 vdc mon,osd   ceph3 centos 192.168.0.16 vdc mon,osd    1.2、修改hosts文件  hostnamectl set-hostname ceph1 #节点1上执行 hostnamectl set-hostname ceph2 #节点2上执行 hostnamectl set-hostname ceph3 #节点3上执行  1.3、配置hosts  三个节点都执行  cat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt;EOF 192.168.0.13 ceph1 192.168.0.14 ceph2 192.168.0.16 ceph3 EOF 1.4、关闭防火墙  三个节点都要执行  systemctl disable --now firewalld setenforce 0 sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config 1.5、时间同步 yum install -y chrony systemctl enable --now chronyd 以ceph1作为时间参考，以下在三个节点都操作，添加server ceph1 prefer vi /etc/chrony.conf #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst server ceph1 prefer 重启chrony systemctl restart chronyd 1.6、docker、lvm2和python3安装 三个节点都需要执行： curl -fsSL get.docker.com -o get-docker.sh sh get-docker.sh systemctl enable docker systemctl restart docker yum install -y python3 yum -y install lvm2 2、cephadm安装(ceph1)  官方下载方法  curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm\n git下载  git clone --single-branch --branch octopus https://github.com/ceph/ceph.git\ncp ceph/src/cephadm/cephadm ./\n cephadm安装(ceph1)  ./cephadm add-repo --release octopus ./cephadm install mkdir -p /etc/ceph cephadm bootstrap --mon-ip 192.168.0.13 安装完成提示： URL: https://ceph1:8443/ User: admin Password: 86yvswzdzd INFO:cephadm:You can access the Ceph CLI with: sudo /usr/sbin/cephadm shell --fsid 3861dbf4-fd47-11ea-a373-5254228af870 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring INFO:cephadm:Please consider enabling telemetry to help improve Ceph: ceph telemetry on For more information see: https://docs.ceph.com/docs/master/mgr/telemetry/ INFO:cephadm:Bootstrap complete.   配置ceph.pub 将ceph1的/etc/ceph/ceph.pub放入到ceph2和ceph3的.ssh/authorized_keys,ceph2和ceph3下root目录下没有.ssh，得先创建，最低权限为700，authorized_keys最低权限600。\n  ceph指令生效\n  cephadm shell\n 添加节点  ceph orch host add ceph2\nceph orch host add ceph3\n3、OSD部署  cephadm shell下执行 若块设备没显示，可加\u0026ndash;refresh参数  [ceph: root@ceph1 /]# ceph orch device ls HOST PATH TYPE SIZE DEVICE_ID MODEL VENDOR ROTATIONAL AVAIL REJECT REASONS ceph1 /dev/vdc hdd 50.0G vol-e801hi3v n/a 0x1af4 1 True ceph1 /dev/vda hdd 20.0G i-2iwis9yr n/a 0x1af4 1 False locked ceph1 /dev/vdb hdd 4096M n/a 0x1af4 1 False Insufficient space (\u0026lt;5GB), locked ceph2 /dev/vdc hdd 50.0G vol-lqcchpox n/a 0x1af4 1 True ceph2 /dev/vda hdd 20.0G i-r53b2flp n/a 0x1af4 1 False locked ceph2 /dev/vdb hdd 4096M n/a 0x1af4 1 False locked, Insufficient space (\u0026lt;5GB) ceph3 /dev/vdc hdd 100G vol-p0eksa5m n/a 0x1af4 1 True ceph3 /dev/vda hdd 20.0G i-f6njdmtq n/a 0x1af4 1 False locked ceph3 /dev/vdb hdd 4096M n/a 0x1af4 1 False Insufficient space (\u0026lt;5GB), locked [ceph: root@ceph1 /]# devcice确认true后，执行 [ceph: root@ceph1 /]# ceph orch apply osd --all-available-devices Scheduled osd.all-available-devices update...  查看集群状态,ceph方式及运行容器方式  ceph方式： [ceph: root@ceph1 /]# ceph -s cluster: id: af1f33f0-fd69-11ea-8530-5254228af870 health: HEALTH_OK services: mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 66s) mgr: ceph1.rbarkj(active, since 12m), standbys: ceph2.uzdnki osd: 3 osds: 3 up (since 14s), 3 in (since 14s) data: pools: 1 pools, 1 pgs objects: 1 objects, 0 B usage: 3.0 GiB used, 197 GiB / 200 GiB avail pgs: 1 active+clean 容器方式： [root@ceph1 ceph]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b417841153a9 ceph/ceph:v15 \u0026quot;/usr/bin/ceph-osd -…\u0026quot; 5 minutes ago Up 5 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-osd.2 9dbd720f4a6f prom/prometheus:v2.18.1 \u0026quot;/bin/prometheus --c…\u0026quot; 6 minutes ago Up 5 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-prometheus.ceph1 f860282adf85 prom/alertmanager:v0.20.0 \u0026quot;/bin/alertmanager -…\u0026quot; 6 minutes ago Up 6 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-alertmanager.ceph1 a6cb4b354f46 ceph/ceph-grafana:6.6.2 \u0026quot;/bin/sh -c 'grafana…\u0026quot; 16 minutes ago Up 16 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-grafana.ceph1 b745f57895f3 prom/node-exporter:v0.18.1 \u0026quot;/bin/node_exporter …\u0026quot; 17 minutes ago Up 17 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-node-exporter.ceph1 b00c1a2c7ef6 ceph/ceph:v15 \u0026quot;/usr/bin/ceph-crash…\u0026quot; 17 minutes ago Up 17 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-crash.ceph1 99c703f18f67 ceph/ceph:v15 \u0026quot;/usr/bin/ceph-mgr -…\u0026quot; 18 minutes ago Up 18 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-mgr.ceph1.rbarkj e972d3d3f13a ceph/ceph:v15 \u0026quot;/usr/bin/ceph-mon -…\u0026quot; 18 minutes ago Up 18 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-mon.ceph1 参考  https://docs.ceph.com/en/latest/cephadm/install/ ","date":"2019-08-07T00:00:00Z","permalink":"https://Forest-L.github.io/post/k8s-openebs-install/","section":"post","tags":["openebs","k8s"],"title":"openebs基于k8s安装"},{"categories":["storage"],"contents":"rook介绍 Rook是目前开源中比较流行的云原生的存储编排系统，专注于如何实现把ceph运行在Kubernetes平台上。将之前手工执行部署、初始化、配置、扩展、升级、迁移、灾难恢复、监控以及资源管理等转变为自动触发。比如集群增加一块磁盘，rook能自动初始化为一个OSD，并自动加入到合适的故障中，这个osd在kubernetes中是以pod形式运行的。\n1、部署  Rook-Ceph底层存储可以是：卷、分区、block模式的pv。主要包括三部分：CRD、Operator、Cluster。 下载rook安装文件并部署  git clone --single-branch --branch release-1.3 https://github.com/rook/rook.git cd cluster/examples/kubernetes/ceph 注意：若是重装需清空node节点的/var/lib/rook下的文件， 并且要保证挂载的卷或者分区是没有文件系统的 部署： (1) CRD: kubectl create -f common.yaml (2) Operator: kubectl create -f operator.yaml (3) Cluster: kubectl create -f cluster.yaml [root@node1 ~]# kubectl get pod -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-l5rw5 3/3 Running 0 8m33s csi-cephfsplugin-lrzfj 3/3 Running 0 22m csi-cephfsplugin-mqbg8 3/3 Running 0 53m csi-cephfsplugin-provisioner-58888b54f5-fj8tf 5/5 Running 5 53m csi-cephfsplugin-provisioner-58888b54f5-p2vpt 5/5 Running 9 53m csi-rbdplugin-5mmsl 3/3 Running 0 8m34s csi-rbdplugin-d4966 3/3 Running 0 53m csi-rbdplugin-mcsxw 3/3 Running 0 22m csi-rbdplugin-provisioner-6ddbf76966-498gn 6/6 Running 11 53m csi-rbdplugin-provisioner-6ddbf76966-z2vfq 6/6 Running 6 53m rook-ceph-crashcollector-node1-69666f444d-5bgh9 1/1 Running 0 5m41s rook-ceph-crashcollector-node2-6c5b88dcf5-th4xk 1/1 Running 0 6m4s rook-ceph-crashcollector-node3-54b5c58544-hz6m8 1/1 Running 0 22m rook-ceph-mgr-a-5d85bd689f-g2dfh 1/1 Running 0 5m41s rook-ceph-mon-a-76c84f876b-m62d7 1/1 Running 0 6m38s rook-ceph-mon-b-6dc744d5b8-bspvh 1/1 Running 0 6m22s rook-ceph-mon-c-67f5987779-4l8vf 1/1 Running 0 6m4s rook-ceph-operator-6659fb4ddd-wdxnp 1/1 Running 3 55m rook-ceph-osd-0-57954bcb4f-8b48j 1/1 Running 0 4m59s rook-ceph-osd-1-7c59f96f47-xnfpc 1/1 Running 0 4m48s rook-ceph-osd-prepare-node1-hrzg6 1/1 Running 0 5m38s rook-ceph-osd-prepare-node2-c5wcl 0/1 Completed 0 5m38s rook-ceph-osd-prepare-node3-gkt7g 0/1 Completed 0 5m38s rook-discover-6ll64 1/1 Running 0 8m34s rook-discover-bvbsh 1/1 Running 0 22m rook-discover-cm5ls 1/1 Running 0 54m 说明 (1) csi-cephfsplugin-*, csi-rbdplugin-* : ceph-FS 和ceph-rbd CSI (2) rook-ceph-crashcollector-*: crash 收集器 (3) rook-ceph-mgr-*: 管理后台 (4) root-ceph-mon-*: Mon监视器，维护集群中的各种状态 (5) rook-ceph-osd-*: ceph-OSD，主要功能是数据的存储，本例中每个盘会起一个OSD (6) rook-discorer-*: 检测符合要求的存储设备 1.2、修改hosts文件  hostnamectl set-hostname ceph1 #节点1上执行 hostnamectl set-hostname ceph2 #节点2上执行 hostnamectl set-hostname ceph3 #节点3上执行  1.3、配置hosts  三个节点都执行  cat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt;EOF 192.168.0.13 ceph1 192.168.0.14 ceph2 192.168.0.16 ceph3 EOF 1.4、关闭防火墙  三个节点都要执行  systemctl disable --now firewalld setenforce 0 sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config 1.5、时间同步 yum install -y chrony systemctl enable --now chronyd 以ceph1作为时间参考，以下在三个节点都操作，添加server ceph1 prefer vi /etc/chrony.conf #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst server ceph1 prefer 重启chrony systemctl restart chronyd 1.6、docker、lvm2和python3安装 三个节点都需要执行： curl -fsSL get.docker.com -o get-docker.sh sh get-docker.sh systemctl enable docker systemctl restart docker yum install -y python3 yum -y install lvm2 2、cephadm安装(ceph1)  官方下载方法  curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm\n git下载  git clone --single-branch --branch octopus https://github.com/ceph/ceph.git\ncp ceph/src/cephadm/cephadm ./\n cephadm安装(ceph1)  ./cephadm add-repo --release octopus ./cephadm install mkdir -p /etc/ceph cephadm bootstrap --mon-ip 192.168.0.13 安装完成提示： URL: https://ceph1:8443/ User: admin Password: 86yvswzdzd INFO:cephadm:You can access the Ceph CLI with: sudo /usr/sbin/cephadm shell --fsid 3861dbf4-fd47-11ea-a373-5254228af870 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring INFO:cephadm:Please consider enabling telemetry to help improve Ceph: ceph telemetry on For more information see: https://docs.ceph.com/docs/master/mgr/telemetry/ INFO:cephadm:Bootstrap complete.   配置ceph.pub 将ceph1的/etc/ceph/ceph.pub放入到ceph2和ceph3的.ssh/authorized_keys,ceph2和ceph3下root目录下没有.ssh，得先创建，最低权限为700，authorized_keys最低权限600。\n  ceph指令生效\n  cephadm shell\n 添加节点  ceph orch host add ceph2\nceph orch host add ceph3\n3、OSD部署  cephadm shell下执行 若块设备没显示，可加\u0026ndash;refresh参数  [ceph: root@ceph1 /]# ceph orch device ls HOST PATH TYPE SIZE DEVICE_ID MODEL VENDOR ROTATIONAL AVAIL REJECT REASONS ceph1 /dev/vdc hdd 50.0G vol-e801hi3v n/a 0x1af4 1 True ceph1 /dev/vda hdd 20.0G i-2iwis9yr n/a 0x1af4 1 False locked ceph1 /dev/vdb hdd 4096M n/a 0x1af4 1 False Insufficient space (\u0026lt;5GB), locked ceph2 /dev/vdc hdd 50.0G vol-lqcchpox n/a 0x1af4 1 True ceph2 /dev/vda hdd 20.0G i-r53b2flp n/a 0x1af4 1 False locked ceph2 /dev/vdb hdd 4096M n/a 0x1af4 1 False locked, Insufficient space (\u0026lt;5GB) ceph3 /dev/vdc hdd 100G vol-p0eksa5m n/a 0x1af4 1 True ceph3 /dev/vda hdd 20.0G i-f6njdmtq n/a 0x1af4 1 False locked ceph3 /dev/vdb hdd 4096M n/a 0x1af4 1 False Insufficient space (\u0026lt;5GB), locked [ceph: root@ceph1 /]# devcice确认true后，执行 [ceph: root@ceph1 /]# ceph orch apply osd --all-available-devices Scheduled osd.all-available-devices update...  查看集群状态,ceph方式及运行容器方式  ceph方式： [ceph: root@ceph1 /]# ceph -s cluster: id: af1f33f0-fd69-11ea-8530-5254228af870 health: HEALTH_OK services: mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 66s) mgr: ceph1.rbarkj(active, since 12m), standbys: ceph2.uzdnki osd: 3 osds: 3 up (since 14s), 3 in (since 14s) data: pools: 1 pools, 1 pgs objects: 1 objects, 0 B usage: 3.0 GiB used, 197 GiB / 200 GiB avail pgs: 1 active+clean 容器方式： [root@ceph1 ceph]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b417841153a9 ceph/ceph:v15 \u0026quot;/usr/bin/ceph-osd -…\u0026quot; 5 minutes ago Up 5 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-osd.2 9dbd720f4a6f prom/prometheus:v2.18.1 \u0026quot;/bin/prometheus --c…\u0026quot; 6 minutes ago Up 5 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-prometheus.ceph1 f860282adf85 prom/alertmanager:v0.20.0 \u0026quot;/bin/alertmanager -…\u0026quot; 6 minutes ago Up 6 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-alertmanager.ceph1 a6cb4b354f46 ceph/ceph-grafana:6.6.2 \u0026quot;/bin/sh -c 'grafana…\u0026quot; 16 minutes ago Up 16 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-grafana.ceph1 b745f57895f3 prom/node-exporter:v0.18.1 \u0026quot;/bin/node_exporter …\u0026quot; 17 minutes ago Up 17 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-node-exporter.ceph1 b00c1a2c7ef6 ceph/ceph:v15 \u0026quot;/usr/bin/ceph-crash…\u0026quot; 17 minutes ago Up 17 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-crash.ceph1 99c703f18f67 ceph/ceph:v15 \u0026quot;/usr/bin/ceph-mgr -…\u0026quot; 18 minutes ago Up 18 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-mgr.ceph1.rbarkj e972d3d3f13a ceph/ceph:v15 \u0026quot;/usr/bin/ceph-mon -…\u0026quot; 18 minutes ago Up 18 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-mon.ceph1 参考  https://docs.ceph.com/en/latest/cephadm/install/ ","date":"2019-08-07T00:00:00Z","permalink":"https://Forest-L.github.io/post/k8s-rook-install/","section":"post","tags":["rook","k8s"],"title":"rook基于k8s安装"},{"categories":["storage"],"contents":"ceph介绍 ceph为块存储，以下通过cephadm安装ceph集群，ceph主要有三个基本角色：\n OSD 用于集群中所有数据与对象的存储，处理集群数据的复制、恢复、回填、再均衡，并向其他osd守护进程发送心跳，然后向 Monitor 提供一些监控信息。 Monitor 监控整个集群的状态，维护集群的 cluster MAP 二进制表，保证集群数据的一致性。 MDS (可选) 为 Ceph 文件系统提供元数据计算、缓存与同步。MDS 进程并不是必须的进程，只有需要使用 CephFS 时，才需要配置 MDS 节点。  1、环境准备 1.1、节点规划    节点  os  IP  磁盘 角色     ceph1 centos 192.168.0.13 vdc cephadm，mgr, mon,osd   ceph2 centos 192.168.0.14 vdc mon,osd   ceph3 centos 192.168.0.16 vdc mon,osd    1.2、修改hosts文件  hostnamectl set-hostname ceph1 #节点1上执行 hostnamectl set-hostname ceph2 #节点2上执行 hostnamectl set-hostname ceph3 #节点3上执行  1.3、配置hosts  三个节点都执行  cat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt;EOF 192.168.0.13 ceph1 192.168.0.14 ceph2 192.168.0.16 ceph3 EOF 1.4、关闭防火墙  三个节点都要执行  systemctl disable --now firewalld setenforce 0 sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config 1.5、时间同步 yum install -y chrony systemctl enable --now chronyd 以ceph1作为时间参考，以下在三个节点都操作，添加server ceph1 prefer vi /etc/chrony.conf #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst server ceph1 prefer 重启chrony systemctl restart chronyd 1.6、docker、lvm2和python3安装 三个节点都需要执行： curl -fsSL get.docker.com -o get-docker.sh sh get-docker.sh systemctl enable docker systemctl restart docker yum install -y python3 yum -y install lvm2 2、cephadm安装(ceph1)  官方下载方法  curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm\n git下载  git clone --single-branch --branch octopus https://github.com/ceph/ceph.git\ncp ceph/src/cephadm/cephadm ./\n cephadm安装(ceph1)  ./cephadm add-repo --release octopus ./cephadm install mkdir -p /etc/ceph cephadm bootstrap --mon-ip 192.168.0.13 安装完成提示： URL: https://ceph1:8443/ User: admin Password: 86yvswzdzd INFO:cephadm:You can access the Ceph CLI with: sudo /usr/sbin/cephadm shell --fsid 3861dbf4-fd47-11ea-a373-5254228af870 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring INFO:cephadm:Please consider enabling telemetry to help improve Ceph: ceph telemetry on For more information see: https://docs.ceph.com/docs/master/mgr/telemetry/ INFO:cephadm:Bootstrap complete.   配置ceph.pub 将ceph1的/etc/ceph/ceph.pub放入到ceph2和ceph3的.ssh/authorized_keys,ceph2和ceph3下root目录下没有.ssh，得先创建，最低权限为700，authorized_keys最低权限600。\n  ceph指令生效\n  cephadm shell\n 添加节点  ceph orch host add ceph2\nceph orch host add ceph3\n3、OSD部署  cephadm shell下执行 若块设备没显示，可加\u0026ndash;refresh参数  [ceph: root@ceph1 /]# ceph orch device ls HOST PATH TYPE SIZE DEVICE_ID MODEL VENDOR ROTATIONAL AVAIL REJECT REASONS ceph1 /dev/vdc hdd 50.0G vol-e801hi3v n/a 0x1af4 1 True ceph1 /dev/vda hdd 20.0G i-2iwis9yr n/a 0x1af4 1 False locked ceph1 /dev/vdb hdd 4096M n/a 0x1af4 1 False Insufficient space (\u0026lt;5GB), locked ceph2 /dev/vdc hdd 50.0G vol-lqcchpox n/a 0x1af4 1 True ceph2 /dev/vda hdd 20.0G i-r53b2flp n/a 0x1af4 1 False locked ceph2 /dev/vdb hdd 4096M n/a 0x1af4 1 False locked, Insufficient space (\u0026lt;5GB) ceph3 /dev/vdc hdd 100G vol-p0eksa5m n/a 0x1af4 1 True ceph3 /dev/vda hdd 20.0G i-f6njdmtq n/a 0x1af4 1 False locked ceph3 /dev/vdb hdd 4096M n/a 0x1af4 1 False Insufficient space (\u0026lt;5GB), locked [ceph: root@ceph1 /]# devcice确认true后，执行 [ceph: root@ceph1 /]# ceph orch apply osd --all-available-devices Scheduled osd.all-available-devices update...  查看集群状态,ceph方式及运行容器方式  ceph方式： [ceph: root@ceph1 /]# ceph -s cluster: id: af1f33f0-fd69-11ea-8530-5254228af870 health: HEALTH_OK services: mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 66s) mgr: ceph1.rbarkj(active, since 12m), standbys: ceph2.uzdnki osd: 3 osds: 3 up (since 14s), 3 in (since 14s) data: pools: 1 pools, 1 pgs objects: 1 objects, 0 B usage: 3.0 GiB used, 197 GiB / 200 GiB avail pgs: 1 active+clean 容器方式： [root@ceph1 ceph]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b417841153a9 ceph/ceph:v15 \u0026quot;/usr/bin/ceph-osd -…\u0026quot; 5 minutes ago Up 5 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-osd.2 9dbd720f4a6f prom/prometheus:v2.18.1 \u0026quot;/bin/prometheus --c…\u0026quot; 6 minutes ago Up 5 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-prometheus.ceph1 f860282adf85 prom/alertmanager:v0.20.0 \u0026quot;/bin/alertmanager -…\u0026quot; 6 minutes ago Up 6 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-alertmanager.ceph1 a6cb4b354f46 ceph/ceph-grafana:6.6.2 \u0026quot;/bin/sh -c 'grafana…\u0026quot; 16 minutes ago Up 16 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-grafana.ceph1 b745f57895f3 prom/node-exporter:v0.18.1 \u0026quot;/bin/node_exporter …\u0026quot; 17 minutes ago Up 17 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-node-exporter.ceph1 b00c1a2c7ef6 ceph/ceph:v15 \u0026quot;/usr/bin/ceph-crash…\u0026quot; 17 minutes ago Up 17 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-crash.ceph1 99c703f18f67 ceph/ceph:v15 \u0026quot;/usr/bin/ceph-mgr -…\u0026quot; 18 minutes ago Up 18 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-mgr.ceph1.rbarkj e972d3d3f13a ceph/ceph:v15 \u0026quot;/usr/bin/ceph-mon -…\u0026quot; 18 minutes ago Up 18 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-mon.ceph1 参考  https://docs.ceph.com/en/latest/cephadm/install/ ","date":"2019-08-04T00:00:00Z","permalink":"https://Forest-L.github.io/post/k8s-ceph-install/","section":"post","tags":["ceph","cephadm","osd","linux-tools"],"title":"基于cephadm安装ceph"},{"categories":["storage"],"contents":"k8s default Storage Class搭建 在k8s中，StorageClass为动态存储，存储大小设置不确定，对存储并发要求高和读写速度要求高等方面有很大优势；pv为静态存储，存储大小要确定。而default Storage Class的作用为pvc文件没有标识任何和storageclass相关联的信息，但通过annotations属性关联起来。\n1. 创建storageClass 要使用 StorageClass，我们就得安装对应的自动配置程序，比如我们这里存储后端使用的是 nfs，那么我们就需要使用到一个 nfs-client 的自动配置程序，我们也叫它 Provisioner，这个程序使用我们已经配置好的 nfs 服务器，来自动创建持久卷，也就是自动帮我们创建 PV。 nfs服务器参考博客nfs服务器搭建\n1.1 nfs-client-provisioner 前提：\n Kubernetes 1.9+ Existing NFS Share helm  1.1.1 安装nfs-client-provisioner指令： helm install --name nfs-client --set nfs.server=192.168.0.9 --set nfs.path=/nfsdatas stable/nfs-client-provisioner 如果安装报错，显示没有该helm的stable，在机器上添加helm 源 helm repo add stable http://mirror.azure.cn/kubernetes/charts/\n1.1.2 卸载指令： helm delete nfs-client\n1.1.3 验证storageClass是否存在 在相应安装nfs-client-provisioner机器上执行：kubectl get sc即可，如下所示：\n[root@i-cj1r8a8m ~]# kubectl get sc NAME PROVISIONER AGE nfs-client cluster.local/my-release-nfs-client-provisioner 1h 2. DefaultStorageClass 在定义StorageClass时，可以在Annotation中添加一个键值对：storageclass.kubernetes.io/is-default-class: true，那么此StorageClass就变成默认的StorageClass了。\n2.1 第一种方法 在这个PVC对象中添加一个声明StorageClass对象的标识，这里我们可以利用一个annotations属性来标识，如下\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvctest annotations: volume.beta.kubernetes.io/storage-class: \u0026quot;nfs-client\u0026quot; spec: accessModes: - ReadWriteMany resources: requests: storage: 10Mi 2.2 第二种方法 用 kubectl patch 命令来更新： kubectl patch storageclass nfs-client -p '{\u0026quot;metadata\u0026quot;: {\u0026quot;annotations\u0026quot;:{\u0026quot;storageclass.kubernetes.io/is-default-class\u0026quot;:\u0026quot;true\u0026quot;}}}' 最后结果中包含default为：\n[root@i-cj1r8a8m ~]# kubectl get sc NAME PROVISIONER AGE nfs-client (default)cluster.local/my-release-nfs-client-provisioner 2h ","date":"2019-08-01T00:00:00Z","permalink":"https://Forest-L.github.io/post/k8s-default-storage-class-installer/","section":"post","tags":["nfs","nfs-client-provisioner","k8s"],"title":"k8s-default-Storage-Class搭建"},{"categories":null,"contents":"","date":"2019-07-29T00:00:00Z","permalink":"https://Forest-L.github.io/post/k8s-service-cognize/","section":"post","tags":["iptables","Linux tools"],"title":"k8s-service-认知"},{"categories":null,"contents":"k8s configMap 认知 许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息，而ConfigMap作用是保存配置信息，格式为键值对，可以单独一个key/value使用，也可以多个key/value构成的文件使用。数据不包含敏感信息的字符串。ConfigMap必须在Pod引用它之前创建;Pod只能使用同一个命名空间内的ConfigMap。\n1 常见configMap创建方式 1.1 从key-value字符串创建ConfigMap kubectl create configmap config1 --from-literal=config.test=good\n1.2 从目录创建,/root/configmap/test1和/root/configmap/test2 中 vi test1 a:a1 vi test2 b:b1 kubectl create configmap special-config --from-file=/root/configmap/\n1.3 通过pod形式创建,最常用的方式configtest.yaml内容如下： kind: ConfigMap apiVersion: v1 metadata: name: configtest namespace: default data: test.property.1: a1 test.property.2: b2 test.property.file: |- property.1=a1 property.2=b2 property.3=c3 kubectl create -f configtest.yaml\n2. 常见查看configmap方式 以上面所示的第三种方式创建的configmap为例，名为：configtest。configmap可以简称cm。\n2.1 \u0026ldquo;-o json\u0026quot;格式， kubectl get cm configtest -o json\n2.2 go模板的格式 -o go-template='{{.data}}'格式 kubectl get configmap configtest -o go-template='{{.data}}' 3. 常见使用configmap场景 通过多种方式在Pod中使用，比如设置环境变量、设置容器命令行参数、在Volume中创建配置文件等。 说 明 :  镜像以：buysbox镜像为例，如果能连国外的网，选择gcr.io/google_containers/busybox，不能连国外的网但能国内的外网使用：busybox:1.28.4这个镜像。 下面所有command里面都加了sleep,方便大家进容器查看配置是否起作用。\n3.1 configmap先创建好,以下创建两种类型。 kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm kubectl create configmap env-config --from-literal=log_level=INFO\n3.2 环境参数，注意busybox镜像选择及sleep时间,test-pod.yaml。 apiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - name: test-container image: gcr.io/google_containers/busybox command: [ \u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;sleep 60 ;env\u0026quot; ] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: special.how - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-config key: special.type envFrom: - configMapRef: name: env-config restartPolicy: Never kubectl create -f test-pod.yaml 进入对应的容器里，输入ENV则会包含如下结果：\nSPECIAL_LEVEL_KEY=very SPECIAL_TYPE_KEY=charm log_level=INFO 3.3 命令行参数，注意busybox镜像选择及sleep时间,dapi-test-pod.yaml。 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: gcr.io/google_containers/busybox command: [ \u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;sleep 60 ;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)\u0026quot; ] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: special.how - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-config key: special.type restartPolicy: Never kubectl create -f dapi-test-pod.yaml 进入容器中，执行echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)指令得到结果： very charm\n3.4 volume将ConfigMap作为文件或目录直接挂载，注意busybox镜像选择及sleep时间,当存在同名文件时，直接覆盖掉，vol-test-pod.yaml。 apiVersion: v1 kind: Pod metadata: name: vol-test-pod spec: containers: - name: test-container image: gcr.io/google_containers/busybox command: [ \u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;sleep 60 ;cat /etc/config/special.how\u0026quot; ] volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: special-config restartPolicy: Never kubectl create -f vol-test-pod.yaml 进入容器，cat的结果：very\n","date":"2019-07-26T00:00:00Z","permalink":"https://Forest-L.github.io/post/k8s-configmap-cognize/","section":"post","tags":["iptables","Linux tools"],"title":"k8s-ConfigMap认知"},{"categories":null,"contents":"名为凌，姓李，寓意为生机勃勃。英文名为Forest。2017年3月份华中科技大学研究生毕业。在南京中兴工作过两年，负责容器开发与运维的工作。目前就职于北京青云科技股份有限公司武汉分公司,在KubeSphere团队中负责K8s安装开发与K8s运维的工作。\n本博客主要内容包括为：不同版本K8s快速安装，K8s概念认知，K8s配置参数的调优，不同存储的安装，不同存储对接K8s使用，存储的备份及恢复，Chart包开发指南，Linux排查工具安装及使用，类似模块性能对比分析和生产实践中K8s遇到坑等。\n若内容有误或者内容对你有帮助的地方，欢迎邮箱交流。邮箱地址为：lilin13297@gmail.com\n","date":"2019-07-25T00:00:00Z","permalink":"https://Forest-L.github.io/about/","section":"","tags":null,"title":"关于"},{"categories":["storage"],"contents":"ceph介绍 ceph为块存储，以下通过cephadm安装ceph集群，ceph主要有三个基本角色：\n OSD 用于集群中所有数据与对象的存储，处理集群数据的复制、恢复、回填、再均衡，并向其他osd守护进程发送心跳，然后向 Monitor 提供一些监控信息。 Monitor 监控整个集群的状态，维护集群的 cluster MAP 二进制表，保证集群数据的一致性。 MDS (可选) 为 Ceph 文件系统提供元数据计算、缓存与同步。MDS 进程并不是必须的进程，只有需要使用 CephFS 时，才需要配置 MDS 节点。  1、环境准备 1.1、节点规划    节点  os  IP  磁盘 角色     ceph1 centos 192.168.0.13 vdc cephadm，mgr, mon,osd   ceph2 centos 192.168.0.14 vdc mon,osd   ceph3 centos 192.168.0.16 vdc mon,osd    1.2、修改hosts文件  hostnamectl set-hostname ceph1 #节点1上执行 hostnamectl set-hostname ceph2 #节点2上执行 hostnamectl set-hostname ceph3 #节点3上执行  1.3、配置hosts  三个节点都执行  cat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt;EOF 192.168.0.13 ceph1 192.168.0.14 ceph2 192.168.0.16 ceph3 EOF 1.4、关闭防火墙  三个节点都要执行  systemctl disable --now firewalld setenforce 0 sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config 1.5、时间同步 yum install -y chrony systemctl enable --now chronyd 以ceph1作为时间参考，以下在三个节点都操作，添加server ceph1 prefer vi /etc/chrony.conf #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst server ceph1 prefer 重启chrony systemctl restart chronyd 1.6、docker、lvm2和python3安装 三个节点都需要执行： curl -fsSL get.docker.com -o get-docker.sh sh get-docker.sh systemctl enable docker systemctl restart docker yum install -y python3 yum -y install lvm2 2、cephadm安装(ceph1)  官方下载方法  curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm\n git下载  git clone --single-branch --branch octopus https://github.com/ceph/ceph.git\ncp ceph/src/cephadm/cephadm ./\n cephadm安装(ceph1)  ./cephadm add-repo --release octopus ./cephadm install mkdir -p /etc/ceph cephadm bootstrap --mon-ip 192.168.0.13 安装完成提示： URL: https://ceph1:8443/ User: admin Password: 86yvswzdzd INFO:cephadm:You can access the Ceph CLI with: sudo /usr/sbin/cephadm shell --fsid 3861dbf4-fd47-11ea-a373-5254228af870 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring INFO:cephadm:Please consider enabling telemetry to help improve Ceph: ceph telemetry on For more information see: https://docs.ceph.com/docs/master/mgr/telemetry/ INFO:cephadm:Bootstrap complete.   配置ceph.pub 将ceph1的/etc/ceph/ceph.pub放入到ceph2和ceph3的.ssh/authorized_keys,ceph2和ceph3下root目录下没有.ssh，得先创建，最低权限为700，authorized_keys最低权限600。\n  ceph指令生效\n  cephadm shell\n 添加节点  ceph orch host add ceph2\nceph orch host add ceph3\n3、OSD部署  cephadm shell下执行 若块设备没显示，可加\u0026ndash;refresh参数  [ceph: root@ceph1 /]# ceph orch device ls HOST PATH TYPE SIZE DEVICE_ID MODEL VENDOR ROTATIONAL AVAIL REJECT REASONS ceph1 /dev/vdc hdd 50.0G vol-e801hi3v n/a 0x1af4 1 True ceph1 /dev/vda hdd 20.0G i-2iwis9yr n/a 0x1af4 1 False locked ceph1 /dev/vdb hdd 4096M n/a 0x1af4 1 False Insufficient space (\u0026lt;5GB), locked ceph2 /dev/vdc hdd 50.0G vol-lqcchpox n/a 0x1af4 1 True ceph2 /dev/vda hdd 20.0G i-r53b2flp n/a 0x1af4 1 False locked ceph2 /dev/vdb hdd 4096M n/a 0x1af4 1 False locked, Insufficient space (\u0026lt;5GB) ceph3 /dev/vdc hdd 100G vol-p0eksa5m n/a 0x1af4 1 True ceph3 /dev/vda hdd 20.0G i-f6njdmtq n/a 0x1af4 1 False locked ceph3 /dev/vdb hdd 4096M n/a 0x1af4 1 False Insufficient space (\u0026lt;5GB), locked [ceph: root@ceph1 /]# devcice确认true后，执行 [ceph: root@ceph1 /]# ceph orch apply osd --all-available-devices Scheduled osd.all-available-devices update...  查看集群状态,ceph方式及运行容器方式  ceph方式： [ceph: root@ceph1 /]# ceph -s cluster: id: af1f33f0-fd69-11ea-8530-5254228af870 health: HEALTH_OK services: mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 66s) mgr: ceph1.rbarkj(active, since 12m), standbys: ceph2.uzdnki osd: 3 osds: 3 up (since 14s), 3 in (since 14s) data: pools: 1 pools, 1 pgs objects: 1 objects, 0 B usage: 3.0 GiB used, 197 GiB / 200 GiB avail pgs: 1 active+clean 容器方式： [root@ceph1 ceph]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b417841153a9 ceph/ceph:v15 \u0026quot;/usr/bin/ceph-osd -…\u0026quot; 5 minutes ago Up 5 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-osd.2 9dbd720f4a6f prom/prometheus:v2.18.1 \u0026quot;/bin/prometheus --c…\u0026quot; 6 minutes ago Up 5 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-prometheus.ceph1 f860282adf85 prom/alertmanager:v0.20.0 \u0026quot;/bin/alertmanager -…\u0026quot; 6 minutes ago Up 6 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-alertmanager.ceph1 a6cb4b354f46 ceph/ceph-grafana:6.6.2 \u0026quot;/bin/sh -c 'grafana…\u0026quot; 16 minutes ago Up 16 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-grafana.ceph1 b745f57895f3 prom/node-exporter:v0.18.1 \u0026quot;/bin/node_exporter …\u0026quot; 17 minutes ago Up 17 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-node-exporter.ceph1 b00c1a2c7ef6 ceph/ceph:v15 \u0026quot;/usr/bin/ceph-crash…\u0026quot; 17 minutes ago Up 17 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-crash.ceph1 99c703f18f67 ceph/ceph:v15 \u0026quot;/usr/bin/ceph-mgr -…\u0026quot; 18 minutes ago Up 18 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-mgr.ceph1.rbarkj e972d3d3f13a ceph/ceph:v15 \u0026quot;/usr/bin/ceph-mon -…\u0026quot; 18 minutes ago Up 18 minutes ceph-af1f33f0-fd69-11ea-8530-5254228af870-mon.ceph1 参考  https://docs.ceph.com/en/latest/cephadm/install/ ","date":"2019-07-14T00:00:00Z","permalink":"https://Forest-L.github.io/post/k8s-glusterfs-install/","section":"post","tags":["glusterfs","linux-tools"],"title":"glusterfs安装"},{"categories":["storage"],"contents":"Linux 下NFS 服务器的搭建及配置 nfs是网络存储文件系统，客户端通过网络访问不同主机上磁盘的数据，用于unix系统。\n演示nfs机器信息，分为服务端和客户端介绍，以下服务端的ip请根据自己环境来替换。 服务端：192.168.0.9，客户端：192.168.0.10\n1.1服务端安装（192.168.0.9） centos: sudo yum install nfs-utils -y\nubuntu16.04: sudo apt install nfs-kernel-server\n1.2服务端配置 centos: rpcbind服务的开机自启和启动： sudo systemctl enable rpcbind;sudo systemctl restart rpcbind\nnfs服务的开机自启和启动： sudo systemctl enable nfs;sudo systemctl restart nfs\nubuntu16.04: sudo systemctl enable nfs-kernel-server;sudo systemctl restart nfs-kernel-server\n1.3配置共享目录 目录为服务端目录，后续存储的数据在该目录下 sudo mkdir -p /nfsdatas sudo chmod 755 /nfsdatas 重要: 根据上面创建的目录，相应配置导出目录 sudo vi /etc/exports 添加如下配置再保存，重启nfs服务即可： /nfsdatas/ 192.168.0.0/16(rw,sync,no_root_squash,no_all_squash)\n /nfsdatas:共享目录位置。 192.168.0.0/24：客户端IP范围，*代表所有。 rw：权限设置，可读可写。 sync：同步共享目录。 no_root_squash: 可以使用root授权。 no_all_squash: 可以使用普通用户授权  centos: systemctl restart nfs ubuntu16.04: sudo systemctl restart nfs-kernel-server\n2.1客户端安装（192.168.0.10） centos: sudo yum install nfs-utils -y\nubuntu16.04: sudo apt install nfs-common\n2.2客户端开机自启和启动即可 centos: sudo systemctl enable rpcbind sudo systemctl start rpcbind\nubuntu16.04: sudo systemctl enable nfs-common;sudo systemctl restart nfs-common\n2.3客户端验证及测试 检查服务端的共享目录： showmount -e 192.168.0.9 客户端创建目录 sudo mkdir -p /tmp/nfsdata 挂载指令： sudo mount -t nfs 192.168.0.9:/nfsdatas /tmp/nfsdata 然后进入/tmp/nfsdata目录下，新建文件 sudo touch test 之后在nfs服务端192.168.0.9的/nfsdatas目录查看是否有test文件 卸载指令：不要在/tmp/nfsdata目录下执行卸载指令。 umount /tmp/nfsdata\ncenots和ubuntu脚本部署 下面内容添加：vi nfs-install.sh 加权限和执行：chmod +x nfs-install.sh \u0026amp;\u0026amp; ./nfs-install.sh 以下脚本安装的服务端目录为：/nfsdatas,如果需要修改的话，脚本内容需要修改。\n#!/bin/bash function centostest(){ yum clean all;yum makecache yum install nfs-utils -y systemctl enable rpcbind;sudo systemctl restart rpcbind systemctl enable nfs;sudo systemctl restart nfs mkdir -p /nfsdatas chmod 755 /nfsdatas echo \u0026quot;/nfsdatas/ 192.168.0.0/16(rw,sync,no_root_squash,no_all_squash)\u0026quot; \u0026gt; /etc/exports systemctl restart nfs showmount -e localhost if [[ $? -eq 0 ]]; then #statements str=\u0026quot;successsful!\u0026quot; echo -e \u0026quot;\\033[30;47m$str\\033[0m\u0026quot; else str=\u0026quot;failed!\u0026quot; echo -e \u0026quot;\\033[31;47m$str\\033[0m\u0026quot; exit fi } function ubuntutest(){ apt-get update sudo apt install nfs-kernel-server sudo systemctl enable nfs-kernel-server;sudo systemctl restart nfs-kernel-server mkdir -p /nfsdatas chmod 755 /nfsdatas echo \u0026quot;/nfsdatas/ 192.168.0.0/16(rw,sync,no_root_squash,no_all_squash)\u0026quot; \u0026gt; /etc/exports sudo systemctl restart nfs-kernel-server showmount -e localhost if [[ $? -eq 0 ]]; then #statements str=\u0026quot;successsful!\u0026quot; echo -e \u0026quot;\\033[30;47m$str\\033[0m\u0026quot; else str=\u0026quot;failed!\u0026quot; echo -e \u0026quot;\\033[31;47m$str\\033[0m\u0026quot; exit fi } cat /etc/redhat-release if [[ $? -eq 0 ]]; then str=\u0026quot;centos!\u0026quot; echo -e \u0026quot;\\033[30;47m$str\\033[0m\u0026quot; centostest else str=\u0026quot;ubuntu!\u0026quot; echo -e \u0026quot;\\033[31;47m$str\\033[0m\u0026quot; ubuntutest fi ","date":"2019-07-13T00:00:00Z","permalink":"https://Forest-L.github.io/post/linux-nfs-install/","section":"post","tags":["nfs","linux-tools"],"title":"Linux中nfs搭建"},{"categories":null,"contents":"","date":"2019-05-28T00:00:00Z","permalink":"https://Forest-L.github.io/archives/","section":"","tags":null,"title":""},{"categories":null,"contents":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution  Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\n Blockquote with attribution  Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\n Tables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\n   Name Age     Bob 27   Alice 23    Inline Markdown within tables    Inline  Markdown  In  Table     italics bold strikethrough  code    Code Blocks Code block with backticks html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  Code block with Hugo\u0026rsquo;s internal highlight shortcode \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; List Types Ordered List  First item Second item Third item  Unordered List  List item Another item And another item  Nested list  Item   First Sub-item Second Sub-item  Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\n  The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015. \u0026#x21a9;\u0026#xfe0e;\n  ","date":"2019-03-11T00:00:00Z","permalink":"https://Forest-L.github.io/post/markdown-syntax/","section":"post","tags":["markdown","css","html","themes"],"title":"Markdown语法入门"},{"categories":null,"contents":"1. asciinema 终端录屏 asciinema是一个在终端下非常棒的录制分享软件，基于文本的录屏工具，对终端输入输出进行捕捉， 然后以文本的形式来记录和回放！对多种系统都支持。\n2. asciinema安装 各种安装方法如连接，以下按centos为例进行： https://asciinema.org/docs/installation\nyum install -y epel-release \nyum install -y asciinema\n检查是否成功，看asciinema版本\nasciinema --version\n3. 指令及常见用法 [root@i-cvswezkk ~]# asciinema --help usage: asciinema [-h] [--version] {rec,play,upload,auth} ... Record and share your terminal sessions, the right way. positional arguments: {rec,play,upload,auth} rec Record terminal session # 记录终端会话 play Replay terminal session # 播放重播终端会话 upload Upload locally saved terminal session to asciinema.org #上传本地保存的终端会话到asciinema.org auth Manage recordings on asciinema.org account # 管理asciinema.org帐户上的记录 optional arguments: -h, --help show this help message and exit --version show program's version number and exit 3.1 各个指令具体含义： .cast和.json文件是一样的，注册asciinema就需要邮箱即可，然后邮箱认证即可，可能收到邮箱的时间长点。 记录终端并将其上传到asciinema.org\nasciinema rec\n将终端记录到本地文件\nasciinema rec demo.cast\n记录终端并将其上传到asciinema.org，指定标题：\u0026ldquo;my aslog1\u0026rdquo;\nasciinema rec -t \u0026quot;my aslog1\u0026quot;\n将终端记录到本地文件，将空闲时间限制到最大2.5秒\nasciinema rec -i 2.5 demo.cast\n从本地文件重放终端记录\nasciinema play demo.cast\nasciinema还提供了一个可以管理asciinema个人账户所拥有的会话文件的功能, 命令为\nasciinema auth\n执行命令\u0026quot;asciinema auth\u0026quot;命令后, 会返回一个网络地址, 点击这个地址就会打开asciinema个人账号注册界面 本地修改记录文件\nvi demo.cast\n本地修改记录文件再重新上传至asciinema.org，\n# asciinema upload /root/260132.json View the recording at: https://asciinema.org/a/M1K9rrUHl5D0q3TOVDhtRcJIn 4.浏览器访问效果 双击对应的一个录屏，有share download settings，且settings可以设置参数 ","date":"2019-03-05T00:00:00Z","permalink":"https://Forest-L.github.io/post/asciinema/","section":"post","tags":["asciinema","Linux tools"],"title":"终端录屏软件入门"},{"categories":null,"contents":"title = \u0026ldquo;terraform对接qingcloud安装K8s\u0026rdquo; date = \u0026ldquo;2019-08-13\u0026rdquo; tags = [ \u0026ldquo;iptables\u0026rdquo;, \u0026ldquo;Linux tools\u0026rdquo; ] +++\n 使用Terraform作用是不需要用户在iaas平台上单独创建机器，配置好参数之后由Terraform自动创建，实现一键自动化部署。 单节点的安装主要分为三部分，var.tf、kubesphere.tf和install.sh，以下文件内容只需要修改API密钥值就可以部署Ks集群。  执行步骤说明  安装terraform工具 创建一个目录，把var.tf、kubesphere.tf和install.sh三个文件放到该目录下。 修改iaas的API密码 进入目录下执行terraform init指令，显示成功。 init成功之后，然后执行terraform apply即可就开始创建机器，安装Ks。 删除机器指令操作为terraform destroy  1、terraform安装（单独一台机器）  以centos操作系统为例来安装，需要执行以下指令即可。 其余操作系统，参考terraform  sudo yum install -y yum-utils sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo sudo yum -y install terraform  是否安装成功及版本输出  terraform version\n2、var.tf文件说明  access_key和secret_key为必填项。点击iaas用户\u0026ndash;》API密钥\u0026ndash;》创建即可，然后把两个参数填入到下面的配置文件中。zone可以根据自己需求来修改，默认pek3a。  terraform { required_providers { qingcloud = { source = \u0026quot;shaowenchen/qingcloud\u0026quot; version = \u0026quot;1.2.6\u0026quot; } } } variable \u0026quot;access_key\u0026quot; { default = \u0026quot;***\u0026quot; } variable \u0026quot;secret_key\u0026quot; { default = \u0026quot;***\u0026quot; } variable \u0026quot;zone\u0026quot; { default = \u0026quot;pek3a\u0026quot; } provider \u0026quot;qingcloud\u0026quot; { access_key = \u0026quot;${var.access_key}\u0026quot; secret_key = \u0026quot;${var.secret_key}\u0026quot; zone = \u0026quot;${var.zone}\u0026quot; } 3、kubesphere.tf文件说明  resource qingcloud_eip为创建外网ip，也可以用已存在外网IP，如果用已存在外网ip，就不需要qingcloud_eip resource模块。 qingcloud_security_group为创建防火墙，也可以使用已存在防火墙，同理。 qingcloud_security_group_rule为创建防火墙开放的端口。 qingcloud_keypair为密钥创建，此处注释掉，用密码形式。 qingcloud_instance创建机器，包含名字，操作系统，内存，cpu，磁盘，密码，绑定外网IP，防火墙，子网和类型等。 null_resource 和install_kubesphere包含文件拷贝及执行命令。  resource \u0026quot;qingcloud_eip\u0026quot; \u0026quot;init\u0026quot;{ name = \u0026quot;tf_eip\u0026quot; description = \u0026quot;\u0026quot; billing_mode = \u0026quot;traffic\u0026quot; bandwidth = 20 need_icp = 0 } resource \u0026quot;qingcloud_security_group\u0026quot; \u0026quot;basic\u0026quot;{ name = \u0026quot;防火墙\u0026quot; description = \u0026quot;这是第一个防火墙\u0026quot; } resource \u0026quot;qingcloud_security_group_rule\u0026quot; \u0026quot;openport\u0026quot; { security_group_id = \u0026quot;${qingcloud_security_group.basic.id}\u0026quot; protocol = \u0026quot;tcp\u0026quot; priority = 0 action = \u0026quot;accept\u0026quot; direction = 0 from_port = 22 to_port = 40000 } # qingcloud_keypair upload an SSH public key # In this example, upload ~/.ssh/id_rsa.pub content. # You may not have this file in your system, you will need to create your own SSH key. #resource \u0026quot;qingcloud_keypair\u0026quot; \u0026quot;arthur\u0026quot;{ # name = \u0026quot;arthur\u0026quot; # public_key = \u0026quot;${file(\u0026quot;~/.ssh/id_rsa.pub\u0026quot;)}\u0026quot; #} resource \u0026quot;qingcloud_instance\u0026quot; \u0026quot;init\u0026quot;{ count = 1 name = \u0026quot;master-${count.index}\u0026quot; image_id = \u0026quot;centos76x64a\u0026quot; cpu = \u0026quot;16\u0026quot; memory = \u0026quot;32768\u0026quot; instance_class = \u0026quot;0\u0026quot; managed_vxnet_id=\u0026quot;vxnet-0\u0026quot; # keypair_ids = [\u0026quot;${qingcloud_keypair.arthur.id}\u0026quot;] login_passwd = \u0026quot;Qcloud@123\u0026quot; security_group_id =\u0026quot;${qingcloud_security_group.basic.id}\u0026quot; eip_id = \u0026quot;${qingcloud_eip.init.id}\u0026quot; } resource \u0026quot;null_resource\u0026quot; \u0026quot;install_kubesphere\u0026quot; { provisioner \u0026quot;file\u0026quot; { destination = \u0026quot;./install.sh\u0026quot; source = \u0026quot;./install.sh\u0026quot; connection { type = \u0026quot;ssh\u0026quot; user = \u0026quot;root\u0026quot; host = \u0026quot;${qingcloud_eip.init.addr}\u0026quot; password = \u0026quot;Qcloud@123\u0026quot; # private_key = \u0026quot;${file(\u0026quot;~/.ssh/id_rsa\u0026quot;)}\u0026quot; port = \u0026quot;22\u0026quot; } } provisioner \u0026quot;remote-exec\u0026quot; { inline = [ \u0026quot;sh install.sh\u0026quot; ] connection { type = \u0026quot;ssh\u0026quot; user = \u0026quot;root\u0026quot; host = \u0026quot;${qingcloud_eip.init.addr}\u0026quot; password = \u0026quot;Qcloud@123\u0026quot; # private_key = \u0026quot;${file(\u0026quot;~/.ssh/id_rsa\u0026quot;)}\u0026quot; port = \u0026quot;22\u0026quot; } } } 4、install.sh文件说明  具体执行命令  curl -O -k https://kubernetes.pek3b.qingstor.com/tools/kubekey/kk chmod +x kk yum install -y vim openssl socat conntrack ipset echo -e 'yes\\n' | /root/kk create cluster --with-kubesphere 5、执行及删除  进入目录，执行init，出现successfully字样。 terraform apply, 输入yes开始安装。 terraform destroy  [root@node4 ~]# cd terraform [root@node4 terraform]# ls install.sh kubesphere.tf var.tf [root@node4 terraform]# terraform init Initializing the backend... Initializing provider plugins... Terraform has been successfully initialized! [root@node4 terraform]# terraform apply An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Enter a value: yes 6、参考 https://github.com/yunify/terraform-provider-qingcloud\n","date":"0001-01-01T00:00:00Z","permalink":"https://Forest-L.github.io/post/terraform-docking-qingcloud-installation-k8s/","section":"post","tags":null,"title":""}]