<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on 李林博客</title>
    <link>https://Forest-L.github.io/categories/kubernetes/</link>
    <description>Recent content in Kubernetes on 李林博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <copyright>Copyright © 2008–2020</copyright>
    <lastBuildDate>Wed, 30 Dec 2020 20:32:48 +0800</lastBuildDate>
    
	<atom:link href="https://Forest-L.github.io/categories/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>K8s中生命周期管理设计模式</title>
      <link>https://Forest-L.github.io/post/managed-lifecycle/</link>
      <pubDate>Wed, 30 Dec 2020 20:32:48 +0800</pubDate>
      
      <guid>https://Forest-L.github.io/post/managed-lifecycle/</guid>
      <description>&lt;p&gt;由云原生平台管理的容器化应用对其生命周期没有控制权，要想成为优秀的云原生化 ，它们必须监听管理平台发出的事件，并相应地调整其生命周期。托管生命周期模式描述了应用程序如何能够并且应该对这些生命周期事件做出反应。&lt;/p&gt;
&lt;h2 id=&#34;存在问题&#34;&gt;存在问题&lt;/h2&gt;
&lt;p&gt;健康探针我们解释了为什么容器要为不同的健康检查提供API。健康检查API是平台不断探查以获取应用洞察力的只读endpoints。它是平台从应用中提取信息的一种机制。&lt;/p&gt;
&lt;p&gt;除了监控容器的状态外，平台有时可能会发出命令，并期望应用程序对此做出反应。在策略和外部因素的驱动下，云原生平台可能会在任何时刻决定启动或停止其管理的应用程序。容器化应用要决定哪些事件是重要的，要做出反应以及如何反应。但实际上，这是一个API，平台是用来和应用进行通信和发送命令的。另外，如果应用程序不需要这项服务，他们可以自由地从生命周期管理中获益，或者忽略它。&lt;/p&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;p&gt;我们看到，只检查进程状态并不能很好地显示应用程序的健康状况。这就是为什么有不同的API来监控容器的健康状况。同样，只使用进程模型来运行和停止一个进程是不够好的。现实世界中的应用需要更多的细粒度交互和生命周期管理能力。有些应用需要帮助预热，有些应用需要优雅而不带缓存的关闭程序。对于这种和其他用例，一些事件，如图1-1所示，由平台发出，容器可以监听并在需要时做出反应。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/006bbiLEgy1gm1kg6aoq6j309i03bmx8.jpg&#34; alt=&#34;生命周期.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;应用程序的部署单元是一个Pod。你已经知道，一个Pod由一个或多个容器组成。在Pod层面，还有其他的构造，比如init容器、init Container（以及defer-containers，截止到目前还在提案阶段），可以帮助管理容器的生命周期。我们在本章中描述的事件和钩子都是应用在单个容器级别而不是Pod级别。&lt;/p&gt;
&lt;h4 id=&#34;sigterm-signal&#34;&gt;SIGTERM Signal&lt;/h4&gt;
&lt;p&gt;每当Kubernetes决定关闭一个容器时，无论是因为它所属的Pod正在关闭，还是仅仅是一个失败的liveness探测导致容器重新启动，容器都会收到一个SIGTERM信号。SIGTERM是在Kubernetes发出更突然的SIGKILL信号之前，温柔地戳一下容器，让它干净利落地关闭。一旦收到SIGTERM信号，应用程序应该尽快关闭。对于一些应用来说，这可能是一个快速的终止，而其他一些应用可能必须完成其飞行中的请求，释放开放的连接，并清理临时文件，这可能需要稍长的时间。在所有的情况下，对SIGTERM做出反应是以正确时刻和干净的方式关闭容器。&lt;/p&gt;
&lt;h4 id=&#34;sigkill-signal&#34;&gt;SIGKILL Signal&lt;/h4&gt;
&lt;p&gt;如果容器进程在发出SIGTERM信号后还没有关闭，则会被下面的SIGKILL信号强制关闭。Kubernetes不会立即发送SIGKILL信号，而是在发出SIGTERM信号后默认等待30秒的宽限期。这个宽限期可以使用.spec.terminalGracePeriodSeconds字段为每个Pod定义，但不能保证，因为它可以在向Kubernetes发出命令时被覆盖。这里的目的应该是设计和实现容器化应用，使其具有短暂性的快速启动和关闭进程。&lt;/p&gt;
&lt;h4 id=&#34;poststart-hook&#34;&gt;Poststart Hook&lt;/h4&gt;
&lt;p&gt;只使用过程信号来管理生命周期是有一定局限性的。这就是为什么Kubernetes提供了额外的生命周期钩子，如postStart和preStop。包含postStart钩子的Pod清单看起来像例5-1中的那个。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;实例1-1，容器中配置poststart hook
apiVersion: v1
kind: Pod
metadata:
  name: post-start-hook
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    lifecycle:
      postStart:
        exec:
          command:
          - sh
          - -c
          - sleep 30 &amp;amp;&amp;amp; echo &amp;quot;Wake up!&amp;quot; &amp;gt; /tmp/postStart_done
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;postStart命令在这里等待30秒。sleep只是模拟任何可能在这里运行的冗长启动代码。另外，它在这里使用一个触发器文件与主应用程序同步，主应用程序是并行启动的。&lt;/p&gt;
&lt;p&gt;postStart 命令在容器创建后与主容器的进程异步执行。即使许多应用程序的初始化和预热逻辑可以作为容器启动步骤的一部分来实现，postStart仍然涵盖了一些用例。postStart动作是一个阻塞调用，容器状态保持为Waiting，直到postStart处理程序完成，这又使Pod状态保持为Pending状态。postStart的这种性质可以用来延迟容器的启动状态，同时给容器主进程初始化的时间。&lt;/p&gt;
&lt;p&gt;postStart的另一个用途是当Pod不满足某些前提条件时，防止容器启动。例如，当postStart钩子通过返回一个非零的退出代码来指示错误时，主容器进程会被Kubernetes杀死。&lt;/p&gt;
&lt;p&gt;postStart和preStophook调用机制类似于所述的健康探针，并支持这些处理程序类型。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;exec  直接在容器中运行指令&lt;/li&gt;
&lt;li&gt;httpGet  对一个Pod容器监听的端口执行HTTP GET请求。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;你必须非常小心地执行postStart钩子中的关键逻辑，因为它的执行没有保证。由于钩子是与容器进程并行运行的，所以钩子有可能在容器启动之前就被执行了。另外，钩子的目的是至少有一次语义，所以实现必须照顾到重复的执行。另一个需要注意的方面是，平台不会对没有到达处理程序的HTTP请求失败执行任何重试尝。&lt;/p&gt;
&lt;h4 id=&#34;prestop-hook&#34;&gt;Prestop Hook&lt;/h4&gt;
&lt;p&gt;preStop 钩子是在容器被终止之前向其发送的阻塞调用，它与 SIGTERM 信号具有相同的语义，在无法对 SIGTERM 作出反应时，应使用它来优雅关闭容器。它与SIGTERM信号具有相同的语义，当无法对SIGTERM作出反应时，它应该被用来启动容器的优雅关闭。例 1-2 中的 preStop 动作必须在删除容器的调用被发送到容器运行时之前完成，后者会触发 SIGTERM 通知。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1-2. 容器中配置preStop Hook
apiVersion: v1
kind: Pod
metadata:
  name: pre-stop-hook
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    lifecycle:
      preStop:
        httpGet:
          port: 8080
          path: /shutdown
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;即使preStop正在阻塞，按住它或返回一个不成功的结果并不能阻止容器被删除和进程被杀死。preStop只是一个方便的替代SIGTERM信号的优雅应用，仅此而已。它还提供了与我们之前介绍的postStart钩子相同的处理程序类型和保证。&lt;/p&gt;
&lt;h4 id=&#34;other-lifecycle-controls&#34;&gt;Other Lifecycle Controls&lt;/h4&gt;
&lt;p&gt;在本章中，到目前为止，我们已经关注了当容器生命周期事件发生时允许执行命令的钩子。但另一种机制不是在容器层面，而是在Pod层面，允许执行初始化指令。&lt;/p&gt;
&lt;p&gt;Init容器，深入浅出，但这里我们简单介绍一下，将其与生命周期钩子进行比较。与普通的应用容器不同，init容器按顺序运行，一直运行到完成，并且在Pod中任何一个应用容器启动之前运行。这些保证允许使用init容器进行Pod级初始化任务。生命周期钩子和init容器都以不同的粒度（分别在容器级和Pod级）运行，可以在某些情况下交替使用，或者在其他情况下相互补充。表1-1总结了两者的主要区别。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/006bbiLEgy1gm1mmjpp7zj30gj05omy0.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;除了当你需要特定的时间保证时，使用哪种机制没有严格的规定。我们可以完全跳过生命周期钩子和初始化容器，使用bash脚本来执行特定的操作，作为容器启动或关闭命令的一部分。这是有可能的，但它会将容器与脚本紧密耦合，并将其变成维护的噩梦。&lt;/p&gt;
&lt;p&gt;我们还可以使用 Kubernetes 生命周期钩子来执行本章所述的一些动作。另外，我们还可以更进一步，运行使用init容器执行各个动作的容器。在这个序列中，这些选项越来越需要更多的提升，但同时也提供了更强的保障，并实现了重用。&lt;/p&gt;
&lt;p&gt;理解容器和Pod生命周期的阶段和可用钩子对于创建Kubernetes管理的应用来说是至关重要的。&lt;/p&gt;
&lt;h2 id=&#34;讨论&#34;&gt;讨论&lt;/h2&gt;
&lt;p&gt;云原生平台提供的主要好处之一是能够在潜在的不可靠的云基础设施之上可靠和可预测地运行和扩展应用程序。这些平台为在其上运行的应用程序提供了一系列限制和共识。为了有利于应用程序，云原生平台提供的所有功能都遵守这些机制。处理和响应这些事件可以确保您的应用程序可以优雅地启动和关闭，对消费服务的影响最小。目前，在其基本形式下，这意味着容器应该像任何设计良好的POSIX进程一样行为。在未来，可能会有更多的事件给应用程序提示，当它即将被放大，或要求释放资源以防止被关闭。重要的是要进入这样的思维模式：应用程序的生命周期不再由人控制，而是由平台完全自动化。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>K8s中健康检查设计模式</title>
      <link>https://Forest-L.github.io/post/health-probe/</link>
      <pubDate>Tue, 29 Dec 2020 13:34:44 +0800</pubDate>
      
      <guid>https://Forest-L.github.io/post/health-probe/</guid>
      <description>&lt;p&gt;健康探针模式是关于应用程序如何将其健康状态传达给Kubernetes。为了实现完全自动化，云原生应用必须具有高度的可观察性，允许推断其状态，以便Kubernetes能够检测应用是否已经启动，是否准备好服务的请求。这些观察结果会影响Pods的生命周期管理以及流量被路由到应用程序的方式。&lt;/p&gt;
&lt;h2 id=&#34;存在问题&#34;&gt;存在问题&lt;/h2&gt;
&lt;p&gt;Kubernetes会定期检查容器进程状态，如果发现问题就会重新启动。然而，从实践中我们知道，检查进程状态并不足以决定应用程序的健康状况。在很多情况下，一个应用程序挂起了，但它的进程仍然在运行。例如，一个Java应用程序可能会抛出一个OutOfMemoryError，但JVM进程仍在运行。或者，一个应用程序可能会因为运行到一个无限循环、死锁或一些冲击（缓存、堆、进程）而冻结。为了检测这类情况，Kubernetes需要一种可靠的方法来检查应用程序的健康状况。也就是说，并不是要了解应用的内部工作情况，而是一种检查，表明应用是否按照预期运行，是否能够为消费者提供服务。&lt;/p&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;p&gt;软件业已经接受了这样一个事实，即不可能写出无错误的代码。此外，在使用分布式应用程序时，发生故障的机会就更多了。因此，处理故障的重点已经从避免故障转移到检测故障和恢复上。检测故障并不是一个简单的任务，不能对所有的应用统一执行，因为所有的应用对故障的定义都不同。而且，各种类型的故障需要不同的纠正措施。只要有足够的时间，暂时性的故障可能会自我恢复，而其他一些故障可能需要重新启动应用程序。让我们看看Kubernetes用来检测和纠正故障的检查。&lt;/p&gt;
&lt;h4 id=&#34;进程健康检查&#34;&gt;进程健康检查&lt;/h4&gt;
&lt;p&gt;进程健康检查是Kubelet不断对容器进程进行的最简单的健康检查。如果容器进程没有运行，就会重新启动探测。因此，即使没有任何其他的健康检查，应用程序也会因为这个通用检查而变得更加健壮。如果你的应用程序能够检测到任何类型的故障并关闭自己，那么进程健康检查就是你所需要的全部内容.然而，对于大多数情况下，这还不够，其他类型的健康检查也是必要的。&lt;/p&gt;
&lt;h4 id=&#34;liveness-probes&#34;&gt;Liveness Probes&lt;/h4&gt;
&lt;p&gt;如果你的应用程序运行到一些死锁，从进程健康检查的角度来看，它仍然被认为是健康的。为了根据你的==应用业务逻辑来检测==这种问题和任何其他类型的故障，Kubernetes有==liveness probes==&amp;ndash;由Kubelet代理定期执行检查，询问你的容器确认它仍然是健康的。重要的是要从外部而不是应用程序本身执行健康检查，因为一些故障可能会阻止应用程序看门狗报告其故障。关于纠正措施，这种健康检查类似于进程健康检查，因为如果检测到故障，容器就会重新启动。然而，在使用什么方法检查应用程序健康状况方面，它提供了更多的灵活性，如下所示。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HTTP探针通过容器IP地址执行HTTP GET请求，并期望得到一个介于200和399之间的成功的HTTP响应代码。&lt;/li&gt;
&lt;li&gt;TCP Socket探针假设TCP连接成功。&lt;/li&gt;
&lt;li&gt;Exec探针在容器内核命名空间中执行一个任意命令，并期望有一个成功的退出代码（0）。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;1.1 容器中配置liveness probe
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-liveness-check
spec:
  containers:
  - image: k8spatterns/random-generator:1.0 
    name: random-generator
    env:
    - name: DELAY_STARTUP
      value:&amp;quot;20&amp;quot; 
    ports:
    - containerPort: 8080
    protocol: TCP
    livenessProbe:
      httpGet:
      path: /actuator/health
      port: 8080
    initialDelaySeconds: 30
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;根据您的应用程序的性质，您可以选择最适合您的方法。由您的实现来决定您的应用程序何时被认为是健康的或不健康的。然而，请记住，没有通过健康检查的结果是重启你的容器。如果重启你的容器没有帮助，那么健康检查失败没有任何好处，因为Kubernetes会重启你的容器而不解决根本问题。&lt;/p&gt;
&lt;h4 id=&#34;readiness-probes&#34;&gt;Readiness Probes&lt;/h4&gt;
&lt;p&gt;Liveness检查对于保持应用程序的健康非常有用，它可以杀死不健康的容器，并用新的容器替换它们。但有时一个容器可能并不健康，重启它可能也无济于事。最常见的例子是当一个容器还在启动，还没有准备好处理任何请求。或者是一个容器超载了，它的延迟在增加，你希望它暂时屏蔽掉额外的负载。&lt;/p&gt;
&lt;p&gt;对于这种场景，Kubernetes有Readiness探针。执行就绪性Readiness检查的方法与有效性检查（HTTP、TCP、Exec）相同，但纠正措施不同。失败的Readiness探针不是重启容器，而是导致容器从服务端点中移除，并且不接收任何新的流量。当容器准备就绪时，Readiness针会发出信号，以便它在受到服务请求的冲击之前有一段时间进行热身。它对于在后期阶段屏蔽服务的流量也很有用，因为Readiness探测会定期执行，类似于Liveness检查。例1-2展示了如何通过探测已运行的应用的内部文件是否存在来实现Readiness探测。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1-2 容器中配置readiness probe
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-readiness-check
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    readinessProbe:
      exec:  command: [ &amp;quot;stat&amp;quot;, &amp;quot;/var/run/random-generator-ready&amp;quot; ]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;同样，由你对健康检查的实现来决定你的应用程序什么时候准备好做它的工作，什么时候应该让它自己去做。进程健康检查和liveness检查的目的是通过重启容器从故障中恢复，而readiness检查则是为你的应用程序争取时间，并期望它自己恢复。请记住，Kubernetes试图阻止你的容器接收新的请求（例如，当它正在关闭时），无论readiness检查是否在收到SIGTERM信号后仍然通过。&lt;/p&gt;
&lt;p&gt;在许多情况下，您可以同时执行liveness和readiness探针检查。然而，readiness探针的存在为您的容器提供了启动时间。只有通过了readiness检查，部署才被视为成功，因此，例如，使用旧版本的Pods可以作为滚动更新的一部分被终止。&lt;/p&gt;
&lt;p&gt;liveness和readiness探针是云原生应用程序自动化的基本构件。应用框架，如Spring执行器、WildFly Swarm健康检查、Karaf健康检查或Java的MicroProfile规范都提供了健康探针的实现。&lt;/p&gt;
&lt;h2 id=&#34;讨论&#34;&gt;讨论&lt;/h2&gt;
&lt;p&gt;为了实现完全自动化，云原生应用必须具有高度可观察性，为管理平台提供读取和解释应用健康状况的方法，并在必要时采取纠正措施。健康检查在部署、自愈、扩展等活动的自动化中起着基础作用。然而，对于应用健康，您的应用程序还可以通过其他手段提供更多可见性。&lt;/p&gt;
&lt;p&gt;显而易见的、老的方法是通过日志记录来实现这一目的。对于容器来说，一个好的做法是记录任何重大的系统出错和系统错误事件，并将这些日志收集到一个中心位置以便进一步分析。日志通常不是用来采取自动行动的，而是用来提出告警和进一步调查。日志更有用的方面是对故障的事后分析和检测不明显的错误。&lt;/p&gt;
&lt;p&gt;除了记录到标准流中，将退出容器的原因记录到/dev/termination-log也是一个好的做法。这个位置是容器在永久消失之前陈述其最后意愿的地方。图1-1显示了容器如何与运行时平台通信的可能选项。
&lt;img src=&#34;http://ww1.sinaimg.cn/large/006bbiLEgy1gm1a8dibrjj30ah03dwel.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;容器通过将其视为黑盒，为包装和运行应用程序提供了一种统一的方式。然而，任何旨在成为云原生产品的容器都必须为运行时环境提供API，以观察容器的健康状况并采取相应行动。这种支持是以统一的方式实现容器更新和生命周期自动化的基本前提，从而提高系统的弹性和用户体验。在实际操作中，这意味着，作为最起码的要求，你的容器化应用程序必须为不同类型的健康检查（liveness和readiness）提供API。&lt;/p&gt;
&lt;p&gt;即使是表现更好的应用程序也必须提供其他手段，让管理平台通过集成跟踪和度量收集库（如OpenTracing或Prometheus）来观察容器化应用程序的状态。把你的应用当作一个黑盒子，但要实现所有必要的API，以帮助平台以最好的方式观察和管理你的应用。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>K8s中声明式的部署设计模式</title>
      <link>https://Forest-L.github.io/post/declarative-deployment/</link>
      <pubDate>Sun, 27 Dec 2020 19:08:06 +0800</pubDate>
      
      <guid>https://Forest-L.github.io/post/declarative-deployment/</guid>
      <description>&lt;p&gt;声明式Deployment模式的核心是Kubernetes 的Deployment资源。这个抽象封装了一组容器的升级和回滚过程，并使其执行成为一种可重复和自动化的活动。&lt;/p&gt;
&lt;h2 id=&#34;存在问题&#34;&gt;存在问题&lt;/h2&gt;
&lt;p&gt;我们可以以自助服务的方式将隔离的环境作为命名空间进行调配，并通过调度器将服务调度在这些环境中，只需最少的人工干预。但是随着微服务的数量越来越多，不断地用新的版本更新和更换也成了越来越大的负担。&lt;/p&gt;
&lt;p&gt;将服务升级到下一个版本涉及的活动包括启动新版本的Pod，优雅地停止旧版本的Pod，等待并验证它已经成功启动，有时在失败的情况下将其全部回滚到以前的版本。这些活动的执行方式有两种，一种是允许有一定的停机时间，但不允许同时运行并发的服务版本，另一种是没有停机时间，但在更新过程中由于两个版本的服务都在运行，导致资源使用量增加。手动执行这些步骤可能会导致人为错误，而正确地编写脚本则需要花费大量的精力，这两点都会使发布过程迅速变成瓶颈。&lt;/p&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;p&gt;幸运的是，Kubernetes也已经自动完成了这项活动。使用Deployment的概念，我们可以描述我们的应用程序应该如何更新，使用不同的策略，并调整更新过程的各个方面。如果您考虑到每次发布周期都要为每个微服务实例进行多次部署的话（根据团队和项目的不同，可能从几分钟到几个月），这是Kubernetes的另一个省力的自动化。&lt;/p&gt;
&lt;p&gt;在第2章中，我们已经看到，为了有效地完成工作，调度器需要主机上有足够的资源、适当的调度策略以及容器充分定义了资源配置文件。同样，为了使部署正确地完成其工作，它希望容器成为良好的云原生。部署的核心是可预测地启动和停止一组Pod的能力。为了达到预期的工作效果，容器本身通常会监听和符合生命周期事件（如SIGTERM；请参见第5章，托管生命周期），并且还提供第4章云原生中所述的健康检查endpoints，即健康探针，以指示它们是否成功启动。&lt;/p&gt;
&lt;p&gt;如果一个容器准确地覆盖了这两个领域，平台就可以干净利落地关闭旧的容器，并通过启动更新的实例来替换它们。然后，更新过程的所有剩余方面都可以以声明的方式定义，并作为一个原子动作执行，具有预定义的步骤和预期的结果。让我们看看容器更新行为的选项吧&lt;/p&gt;
&lt;p&gt;==注意：==&lt;/p&gt;
&lt;h4 id=&#34;使用-kubectl-进行命令式-rolling-updates已被废弃&#34;&gt;使用 kubectl 进行命令式 Rolling Updates已被废弃&lt;/h4&gt;
&lt;p&gt;Kubernetes从一开始就支持滚动更新。最早的实现是势在必行的，客户端kubectl告诉服务器每个更新步骤要做什么。&lt;/p&gt;
&lt;p&gt;虽然kubectl rolling-update 命令仍然存在，但由于这种命令式的方法存在以下缺点，所以它已经被高度废弃。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kubectl rolling-update 不是描述预期的最终状态，而是发布命令让系统进入预期状态。&lt;/li&gt;
&lt;li&gt;替换容器和ReplicationControllers的整个协调逻辑由kubectl执行，它在发生更新过程时，会监控并与API服务器交互，将固有服务器端的责任转移到客户端。&lt;/li&gt;
&lt;li&gt;您可能需要不止一条命令来使系统进入期望状态。这些命令必须是自动的，并且在不同的环境中可以重复使用。&lt;/li&gt;
&lt;li&gt;随着时间的推移，别人可能会覆盖你的修改。&lt;/li&gt;
&lt;li&gt;更新过程必须记录下来，并在服务推进的同时保持更新。&lt;/li&gt;
&lt;li&gt;要想知道我们部署了什么，唯一的方法就是检查系统的状态。有时候，当前系统的状态可能并不是理想的状态，在这种情况下，我们必须使部署文档相互关联。&lt;/li&gt;
&lt;li&gt;取而代之的是，引入Deployment资源对象来支持声明式更新，完全由Kubernetes后端管理。由于声明式更新有如此多的优势，而命令式更新支持终将消失，我们在这个模式中只关注声明式更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;rolling-deployment&#34;&gt;Rolling Deployment&lt;/h4&gt;
&lt;p&gt;Kubernetes中更新应用的声明方式是通过Deployment的概念。在幕后，Deployment创建了一个ReplicaSet，支持基于集合的标签选择器。同时，Deployment抽象允许通过RollingUpdate（默认）和Recreate等策略来塑造更新过程行为。例1-1显示了Deployment配置的滚动更新策略重要部分。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;例1-1，Deployment for a rolling update
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: random-generator
spec: 
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    selector:
      matchLabels:
        app: random-generator
    template:
      metadata:
        labels:
          app: random-generator
      spec: 
        containers:
        - image: k8spatterns/random-generator:1.0
          name: random-generator 
          readinessProbe:
            exec:
              command: [&amp;quot;stat&amp;quot;,&amp;quot;/random-generator-ready&amp;quot; ]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;滚动更新（RollingUpdate）策略行为确保了更新过程中没有停机时间。在幕后，Deployment实现类似这样的动作来执行，通过创建新的ReplicaSets和用新容器替换旧容器。通过Deployment，这里有一个增强是可以控制新容器滚动的速度。Deployment对象允许你通过maxSurge和maxUnavailable字段来控制可用和超出Pod的范围。图1-1展示了滚动更新过程。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/006bbiLEgy1glyrd6sisrj30eh04s3z1.jpg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;要触发声明式更新，你有三个选项:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用 kubectl replace 将整个部署替换为新版本的部署。&lt;/li&gt;
&lt;li&gt;补丁(kubectl patch)或交互式编辑(kubectl edit)部署，以设置新版本的新容器镜像。&lt;/li&gt;
&lt;li&gt;使用 kubectl set image 来设置部署中的新镜像。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;请参阅我们的示例仓库中的完整示例，其中演示了这些命令的用法，并展示了如何使用kubectl rollout监控或回滚升级。&lt;/p&gt;
&lt;p&gt;除了解决前面提到的命令部署服务方式的弊端外，Deployment还带来了以下好处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deployment是一个Kubernetes资源对象，其状态完全由Kubernetes内部管理。整个更新过程在服务器端进行，无需客户端交互。&lt;/li&gt;
&lt;li&gt;Deployment的声明性使您可以看到已部署的状态应该是怎样的，而不是到达那里的必要步骤。&lt;/li&gt;
&lt;li&gt;Deployment定义是一个可执行的对象，在上生产之前会在多个环境中进行测试。&lt;/li&gt;
&lt;li&gt;更新过程也会被完全记录下来，版本上有暂停、继续和回滚到以前版本的选项。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;fixed-deployment&#34;&gt;Fixed Deployment&lt;/h4&gt;
&lt;p&gt;滚动更新（RollingUpdate）策略对于在更新过程中确保零停机时间非常有用。然而，这种方法的副作用是，在更新过程中，容器的两个版本同时运行。这可能会给服务消费者带来问题，特别是当更新过程在服务API中引入了向后不兼容的变化，而客户端又无法处理这些变化时。对于这种情况，有Recreate策略，如图1-2所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/006bbiLEgy1glys496y3lj30eg052t9b.jpg&#34; alt=&#34;fixed Deployment.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Recreate策略的效果是将maxUnavailable设置为已声明的复制数。这意味着它首先杀死当前版本的所有容器，然后在旧容器被驱逐时同时启动所有新容器。这意味着它首先杀死当前版本的所有容器，然后在旧容器被驱逐时同时启动所有新容器。这一系列操作的结果是，当所有拥有旧版本的容器被停止时，会有一些停机时间，而且没有新的容器准备好处理传入的请求。从积极的一面来看，不会有两个版本的容器同时运行，简化了服务消费者的生活，一次只处理一个版本。&lt;/p&gt;
&lt;h4 id=&#34;蓝绿发布&#34;&gt;蓝绿发布&lt;/h4&gt;
&lt;p&gt;蓝绿部署是一种用于在最小化停机时间和降低风险的生产环境中部署软件的发布策略。Kubernetes&amp;rsquo;Deployment抽象是一个基本概念，它可以让你定义Kubernetes如何将不可变容器从一个版本过渡到另一个版本。我们可以将Deployment基元作为一个构件，与其他Kubernetes基元一起，实现这种更高级的蓝绿部署的发布策略。&lt;/p&gt;
&lt;p&gt;如果没有使用Service Mesh或Knative等扩展，则需要手动完成蓝绿部署。技术上，它的工作原理是通过创建第二个Deployment，容器的最新版本（我们称它为绿色）还没有服务于任何请求。在这一阶段，原始Deployment中的旧Pod副本（称为蓝色）仍在运行并服务于实时请求。&lt;/p&gt;
&lt;p&gt;一旦我们确信新版本的Pods是健康的，并且准备好处理实时请求，我们就会将流量从旧的Pod副本切换到新的副本。Kubernetes中的这个活动可以通过更新服务选择器来匹配新容器（标记为绿色）来完成。如图1-3所示，一旦绿色容器处理了所有的流量，就可以删除蓝色容器，释放资源用于未来的蓝绿部署。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/006bbiLEgy1glysteayffj30f105bmxr.jpg&#34; alt=&#34;蓝绿发布.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;蓝绿方式的一个好处是，只有一个版本的应用在服务请求，这降低了Service消费者处理多个并发版本的复杂性。缺点是它需要两倍的应用容量，同时蓝色和绿色容器都在运行。另外，在过渡期间，可能会出现长时间运行的进程和数据库状态漂移的重大并发症。&lt;/p&gt;
&lt;h4 id=&#34;金丝雀发布&#34;&gt;金丝雀发布&lt;/h4&gt;
&lt;p&gt;金丝雀发布是一种通过用新的实例替换一小部分旧的实例来软性地将一个应用程序的新版本部署到生产中的方法。这种技术通过只让部分消费者达到更新的版本来降低将新版本引入生产的风险。当我们对新版本的服务以及它在小样本用户中的表现感到满意时，我们就用新版本替换所有的旧实例。图1-4显示了一个金丝雀版本的运行情况。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/006bbiLEgy1glyt4zxuo9j30ey0580t8.jpg&#34; alt=&#34;金丝雀发布.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;在Kubernetes中，可以通过为新的容器版本（最好使用Deployment）创建一个新的ReplicaSet来实现这一技术，该ReplicaSet的副本数量较少，可以作为Canary实例使用。在这个阶段，服务应该将一些消费者引导到更新的Pod实例上。一旦我们确信使用新 ReplicaSet 的一切都能按预期工作，我们就会将新的 ReplicaSet 规模化，旧的 ReplicaSet 则降为零。从某种程度上来说，我们是在进行一个可控的、经过用户测试的增量式推广。&lt;/p&gt;
&lt;h2 id=&#34;讨论&#34;&gt;讨论&lt;/h2&gt;
&lt;p&gt;Deoloyment本质是 Kubernetes 将手动更新应用程序的繁琐过程转化为可重复和自动化的声明式活动的一个例子。开箱即用的部署策略（rolling和recreate）控制用新容器替换旧容器，而发布策略（bluegreen和金丝雀）则控制新版本如何提供给服务消费者。后两种发布策略是基于人对过渡触发器的决定，因此不是完全自动化的，而是需要人的交互。图1-5显示了部署和发布策略的摘要，显示了过渡期间的实例数。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/006bbiLEgy1glyw6xng8fj30d70a1dgn.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;每个软件都是不同的，部署复杂的系统通常需要额外的步骤和检查。本章讨论的技术涵盖了Pod更新过程，但不包括更新和回滚其他Pod依赖项，如ConfigMaps、Secrete或其他依赖服务。&lt;/p&gt;
&lt;p&gt;截至目前，Kubernetes有一个建议，允许在部署过程中使用钩子。Pre和Post钩子将允许在Kubernetes执行部署策略之前和之后执行自定义命令。这些命令可以在部署进行时执行额外的操作，另外还可以中止、重试或继续部署。这些命令是向新的自动化部署和发布策略迈出的良好一步。目前，一种行之有效的方法是用脚本化去更高层次上编写更新过程，本节中讨论的Deployment和其他属性来管理服务及其依赖关系的更新过程。&lt;/p&gt;
&lt;p&gt;无论你使用何种部署策略，Kubernetes都必须知道你的应用Pod何时启动并运行，以执行所需的步骤序列达到定义的目标部署状态。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>K8s中可预测的需求设计模式</title>
      <link>https://Forest-L.github.io/post/predictable-demands/</link>
      <pubDate>Sat, 26 Dec 2020 15:38:59 +0800</pubDate>
      
      <guid>https://Forest-L.github.io/post/predictable-demands/</guid>
      <description>&lt;p&gt;在共享云环境中成功部署、管理和共存应用的基础，取决于识别和声明应用资源需求和运行时依赖性。这个Predictable Demands模式是关于你应该如何声明应用需求，无论是硬性的运行时依赖还是资源需求。声明你的需求对于Kubernetes在集群中为你的应用找到合适的位置至关重要。&lt;/p&gt;
&lt;h2 id=&#34;存在问题&#34;&gt;存在问题&lt;/h2&gt;
&lt;p&gt;Kubernetes可以管理用不同编程语言编写的应用，只要该应用可以在容器中运行。然而，不同的语言有不同的资源需求。通常情况下，编译后的语言运行速度更快，而且经常是
与即时运行时或解释语言相比，需要更少的内存。考虑到很多同类别的现代编程语言对资源的要求都差不多，从资源消耗的角度来看，更重要的是领域、应用的业务逻辑和实际实现细节。&lt;/p&gt;
&lt;p&gt;很难预测容器可能需要多少资源才能发挥最佳功能，而知道服务运行的预期资源是开发人员（通过测试发现）。有些服务的CPU和内存消耗情况是固定的，有些服务则是瞬间的。有些服务需要持久性存储来存储数据；有些传统服务需要在主机上固定端口号才能正常工作。定义所有这些应用特性并将其传递给管理平台是云原生应用的基本前提。&lt;/p&gt;
&lt;p&gt;除了资源需求外，应用运行时还对平台管理的能力有依赖性，如数据存储或应用配置。&lt;/p&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;p&gt;了解容器的运行时要求很重要，主要有两个原因。首先，在定义了所有的运行时依赖和资源需求设想后，Kubernetes可以智能地决定在集群上的哪里运行容器以获得最有效的硬件利用率。在大量优先级不同的进程共享资源的环境中，要想成功共存，唯一的办法就是提前了解每个进程的需求。然而，智能投放只是硬币的一面。&lt;/p&gt;
&lt;p&gt;容器资源配置文件必不可少的第二个原因是容量规划。根据具体的服务需求和服务总量，我们可以针对不同的环境做一些容量规划，得出性价比最高的主机配置文件，来满足整个集群的需求。服务资源配置文件和容量规划相辅相成，才能长期成功地进行集群管理。&lt;/p&gt;
&lt;p&gt;在深入研究资源配置文件之前，我们先来看看如何声明运行时依赖关系。&lt;/p&gt;
&lt;h4 id=&#34;运行时依赖&#34;&gt;运行时依赖&lt;/h4&gt;
&lt;p&gt;最常见的运行时依赖之一是用于保存应用程序状态的文件存储。容器文件系统是短暂的，当容器关闭时就会丢失。Kubernetes提供了volume作为Pod级的存储实用程序，可以在容器重启后幸存。&lt;/p&gt;
&lt;p&gt;最直接的卷类型是emptyDir，只要Pod存活，它就会存活，当Pod被删除时，它的内容也会丢失。卷需要有其他类型的存储机制支持，才能有一个在Pod重启后仍能存活的卷。如果你的应用程序需要向这种长时间的存储设备读写文件，你必须在容器定义中使用volumes明确声明这种依赖性。
如例1-1所示。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;如例1-1，依赖于PV
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    volumeMounts:
    - mountPath:&amp;quot;/logs&amp;quot;
      name: log-volume
  volumes:
  - name: log-volume
    persistentVolumeClaim:
      claimName: random-generator-log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;调度器会评估Pod所需要的卷类型，这将影响Pod的调度位置。如果Pod需要的卷不是由集群上的任何节点提供的，那么Pod根本不会被调度。卷是运行时依赖性的一个例子，它影响Pod可以运行什么样的基础设施，以及Pod是否可以被调度。&lt;/p&gt;
&lt;p&gt;当你要求Kubernetes通过hostPort方式暴露容器端口为主机上特定端口时，也会发生类似的依赖关系。hostPort的使用在节点上创建了另一个运行时依赖性，并限制了Pod的调度位置。 hostPort在集群中的每个节点上保留了端口，并限制每个节点最多调度一个Pod。由于端口冲突，你可以扩展到Kubernetes集群中有多少节点就有多少Pod。&lt;/p&gt;
&lt;p&gt;另一种类型的依赖是配置。几乎每个应用程序都需要一些配置信息，Kubernetes提供的推荐解决方案是通过ConfigMaps。你的服务需要有一个消耗设置的策略&amp;ndash;无论是通过环境变量还是文件系统。无论是哪种情况，这都会引入你的容器对名为ConfigMaps的运行时依赖性。如果没有创建所有预期的 ConfigMaps，则容器被调度在节点上，但它们不会启动。ConfigMaps和Secrets在第19章Configuratio资源中进行了更详细的解释，例1-2展示了如何将这些资源用作运行时依赖。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;实例1-2所示，依赖于ConfigMap
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    env:
    - name: PATTERN
      valueFrom:
        configMapKeyRef:
          name: random-generator-config
          key: pattern
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;与ConfigMaps类似的概念是Secrets，它提供了一种略微更安全的方式将特定环境的配置分发到容器中。使用Secret的方式与使用ConfigMap的方式相同，它引入了从容器到namespace的相同依赖性。&lt;/p&gt;
&lt;p&gt;虽然ConfigMap和Secret对象的创建是我们必须执行的简单管理任务，但集群节点提供了存储和端口。其中一些依赖性限制了Pod被调度的位置（如果有的话），而其他依赖性则限制了Pod的运行。
可能会阻止Pod的启动。在设计带有这种依赖关系的容器化应用程序时，一定要考虑它们创建之后运行时的约束。&lt;/p&gt;
&lt;h4 id=&#34;资源配置文件&#34;&gt;资源配置文件&lt;/h4&gt;
&lt;p&gt;指定容器的依赖性，如ConfigMap、Secret和卷，是很直接的。我们需要更多的思考和实验来确定容器的资源需求。在Kubernetes的上下文中，计算资源被定义为可以被容器请求、分配给容器并从容器中获取的东西。资源分为可压缩的（即可以节制的，如CPU，或网络带宽）和不可压缩的（即不能节制的，如内存）。&lt;/p&gt;
&lt;p&gt;区分可压缩资源和不可压缩资源很重要。如果你的容器消耗了太多的可压缩资源（如CPU），它们就会被节流，但如果它们使用了太多的不可压缩资源（如内存），它们就会被杀死（因为没有其他方法可以要求应用程序释放分配的内存）。&lt;/p&gt;
&lt;p&gt;根据你的应用程序的性质和实现细节，你必须指定所需资源的最小量（称为请求）和它可以增长到的最大量（限制）。每个容器定义都可以以请求和限制的形式指定它所需要的CPU和内存量。在一个高层次上，请求/限制的概念类似于软/硬限制。例如，同样地，我们通过使用-Xms和-Xmx命令行选项来定义Java应用程序的堆大小。&lt;/p&gt;
&lt;p&gt;调度器将Pod调度到节点时，使用的是请求量（但不是限制）。对于一个给定的Pod，调度器只考虑那些仍有足够能力容纳Pod及其所有请求资源量相加容器的节点。从这个意义上说，每个容器的请求字段会影响到Pod可以被调度或不被调度的位置。例1-3显示了如何为Pod指定这种限制。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;实例1-3，资源限制
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec: 
  containers: 
  - image: k8spatterns/random-generator:1.0   name: random-generator 
    resources:
      requests:  
        cpu: 100m 
        memory: 100Mi 
      limits:  
        cpu: 200m 
        memory: 200Mi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;根据您是指定请求、限制，还是两者都指定，平台提供不同的服务质量（QoS）。&lt;/p&gt;
&lt;h5 id=&#34;best-effort&#34;&gt;Best-Effort&lt;/h5&gt;
&lt;p&gt;没有为其容器设置任何请求和限制的Pod。这样的Pod被认为是最低优先级的，当Pod的节点用完不可压缩资源时，会首先被干掉。&lt;/p&gt;
&lt;h5 id=&#34;burstable&#34;&gt;Burstable&lt;/h5&gt;
&lt;p&gt;已定义请求和限制的Pod，但它们并不相等（而且限制比预期的请求大）。这样的Pod有最小的资源保证，但也愿意在可用的情况下消耗更多的资源，直至其极限。当节点面临不可压缩的资源压力时，如果没有Best-Effort Pods剩余，这些Pod很可能被干掉。&lt;/p&gt;
&lt;h5 id=&#34;guaranteed&#34;&gt;Guaranteed&lt;/h5&gt;
&lt;p&gt;拥有同等数量请求和限制资源的Pod。这些是优先级最高的Pod，保证不会在Best-Effort和Burstable Pods之前被干掉。&lt;/p&gt;
&lt;p&gt;所以你为容器定义的资源特性或省略资源特性会直接影响到它的QoS，并定义了Pod在资源不足时的相对重要性。在定义你的Pod资源需求时，要考虑到这个后果。&lt;/p&gt;
&lt;h4 id=&#34;pod优先级&#34;&gt;Pod优先级&lt;/h4&gt;
&lt;p&gt;我们解释了容器资源声明如何也定义了Pod的QoS，并影响Kubelet在资源不足时干掉Pod中容器的顺序。另一个相关的功能，在写这篇文章的时候还在测试阶段，就是Pod优先和优先权。Pod优先级允许表明一个Pod相对于其他Pod的重要性，这影响了Pod的调度顺序。让我们在例子1-4中看到它的作用。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;实例1-4，pod优先级
apiVersion: scheduling.k8s.io/v1beta1
kind: PriorityClass
metadata: 
  name: high-priority 
value: 1000 
globalDefault: false
description: This is a very high priority Pod class
---
apiVersion: v1
kind: Pod
metadata: 
  name: random-generator 
  labels: 
    env: random-generator
spec: 
  containers: 
  - image: k8spatterns/random-generator:1.0 
    name: random-generator   
  priorityClassName: high-priority
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们创建了一个PriorityClass，这是一个非命名空间的对象，用于定义一个基于整数的优先级。我们的PriorityClass被命名为high-priority，优先级为1,000。现在我们可以通过它的名字将这个优先级分配给Pods，如priorityClassName：high-riority。PriorityClass是一种表示Pods相对重要性的机制，数值越高表示Pods越重要。&lt;/p&gt;
&lt;p&gt;启用Pod Priority功能后，它会影响调度器将Pod调度在节点上的顺序。首先，优先权进入许可控制器使用priorityClass Name字段来填充新Pod的优先权值。当有多个Pod等待调度时，调度器按最高优先级对待放Pod队列进行排序。在调度队列中，任何待定的Pod都会被选在其他优先级较低的待定Pod之前，如果没有阻止其调度的约束条件，该Pod就会被调度。&lt;/p&gt;
&lt;p&gt;下面是关键部分。如果没有足够容量的节点来调度Pod，调度器可以从节点上抢占（移除）优先级较低的Pod，以释放资源，调度优先级较高的Pod。因此，如果满足其他所有调度要求，优先级较高的Pod可能比优先级较低的Pod更早被调度。这种算法有效地使集群管理员能够控制哪些Pod是更关键的工作负载，并通过允许调度器驱逐优先级较低的Pod，以便在工作节点上为优先级较高的Pod腾出空间，将它们放在第一位。如果一个Pod不能被调度，调度器就会继续调度其他优先级较低的Pod。&lt;/p&gt;
&lt;p&gt;Pod QoS（前面已经讨论过了）和Pod优先级是两个正交的特性，它们之间没有联系，只有一点点重叠。QoS主要被Kubelet用来在可用计算资源较少时保持节点稳定性。==Kubelet在驱逐前首先考虑QoS，然后考虑Pods的PriorityClass。另一方面，调度器驱逐逻辑在选择抢占目标时完全忽略了Pods的QoS==。调度器试图挑选一组优先级最低的Pod，满足优先级较高的Pod等待调度的需求。&lt;/p&gt;
&lt;p&gt;当Pod具有指定的优先级时，它可能会对其他被驱逐的Pod产生不良影响。例如，当一个Pod的优雅终止策略受到重视，第10章中讨论的PodDisruptionBudget，单服务没有得到保证，这可能会打破一个依赖多数Pod数的较低优先级集群应用。&lt;/p&gt;
&lt;p&gt;另一个问题是恶意或不知情的用户创建了优先级最高的Pods，并驱逐了所有其他Pods。为了防止这种情况发生，ResourceQuota已经扩展到支持PriorityClass，较大的优先级数字被保留给通常不应该被抢占或驱逐的关键系统Pods。&lt;/p&gt;
&lt;p&gt;总而言之，Pod优先级应谨慎使用，因为用户指定的数字优先级，指导调度器和Kubelet调度或干掉哪些Pod，会受到用户的影响。任何改变都可能影响许多Pod，并可能阻止平台提供可预测的服务级别协议。&lt;/p&gt;
&lt;h4 id=&#34;项目资源&#34;&gt;项目资源&lt;/h4&gt;
&lt;p&gt;Kubernetes是一个自助服务平台，开发者可以在指定的隔离环境上运行他们认为合适的应用。然而，在一个共享的多租户平台中工作，也需要存在特定的边界和控制单元，以防止一些用户消耗平台的所有资源。其中一个这样的工具是ResourceQuota，它为限制命名空间中的聚合资源消耗提供了约束。通过ResourceQuotas，集群管理员可以限制消耗的计算资源（CPU、内存）和存储的总和。它还可以限制命名空间中创建的对象（如ConfigMaps、Secrets、Pods或Services）的总数。&lt;/p&gt;
&lt;p&gt;这方面的另一个有用的工具是LimitRange，它允许为每种类型的资源设置资源使用限制。除了指定不同资源类型的最小和最大允许量以及这些资源的默认值外，还可以控制请求和限制之间的比例，也就是所谓的超额承诺水平。表1-1给出了如何选择请求和限额的可能值的例子。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;表1-1. Limit和request ranges
Type       Resource  Min    Max  Default limit  Default request  Lim/req ratio  
Container  CPU       500m   2    500m           250m             4
Container  Memory    250Mi  2Gi  500Mi          250Mi            4
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;LimitRanges对于控制容器资源配置非常有用，这样就不会出现需要的资源超过集群节点所能提供的资源的容器。它还可以防止集群用户创建消耗大量资源的容器，使节点不能为其他容器分配资源。考虑到请求(而不是限制)是调度器用来调度的主要容器特性，LimitRequestRatio允许你控制容器的请求和限制之间的差距有多大。在请求和限制之间有很大的综合差距，会增加节点上超负荷的机会，并且当许多容器同时需要比最初请求更多的资源时，可能会降低应用性能。&lt;/p&gt;
&lt;h4 id=&#34;容量规划&#34;&gt;容量规划&lt;/h4&gt;
&lt;p&gt;考虑到容器在不同的环境中可能会有不同的资源情况，以及不同数量的实例，显然，多用途环境的容量规划并不简单。例如，为了获得最佳的硬件利用率，在一个非生产集群上，你可能主要拥有Best-Effort和Burstable容器。在这样的动态环境中，很多容器都是同时启动和关闭的，即使有容器在资源不足的时候被平台干掉，也不会致命。在生产集群上，我们希望事情更加稳定和可预测，容器可能主要是Guaranteed类型，还有一些Burstable。如果一个容器被杀死，那很可能是一个信号，说明集群的容量应该增加。&lt;/p&gt;
&lt;p&gt;当然，在现实生活中，你使用Kubernetes这样的平台，更可能的原因是还有很多服务需要管理，有些服务即将退出，有些服务还在设计开发阶段。即使是一个不断移动的目标，根据前面描述的类似方法，我们可以计算出每个环境中所有服务所需要的资源总量。&lt;/p&gt;
&lt;h2 id=&#34;讨论&#34;&gt;讨论&lt;/h2&gt;
&lt;p&gt;容器不仅对隔离进程和作为打包方式有用。在确定了资源概况后，它们也是成功进行产能规划的基石。进行一些早期测试，以发现每个容器的资源需求，并将该信息作为未来产能规划和预测的基础。&lt;/p&gt;
&lt;p&gt;然而，更重要的是，资源配置文件是应用程序与Kubernetes沟通的方式，以协助调度和管理决策。如果你的应用不提供任何请求或限制，Kubernetes能做的就是把你的容器当作不透明的盒子，当集群满了的时候就会丢掉。所以，每一个应用或多或少都要考虑和提供这些资源声明。&lt;/p&gt;
&lt;p&gt;现在你已经知道了如何确定我们应用的大小，在第3章 &amp;ldquo;声明式部署 &amp;ldquo;中，你将学习多种策略来让我们的应用在Kubernetes上安装和更新。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>