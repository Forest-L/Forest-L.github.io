<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 一切皆有可能</title>
    <link>https://Forest-L.github.io/post/</link>
    <description>Recent content in Posts on 一切皆有可能</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <copyright>Copyright © 2008–2020</copyright>
    <lastBuildDate>Mon, 05 Oct 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://Forest-L.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>k8s分布式存储总结</title>
      <link>https://Forest-L.github.io/post/k8s-storage-summary/</link>
      <pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s-storage-summary/</guid>
      <description>&lt;h2 id=&#34;1pvpvc和sc来源&#34;&gt;1、pv、pvc和sc来源&lt;/h2&gt;
&lt;p&gt;pv引入解耦了pod与底层存储；pvc引入分离声明与消费，分离开发与运维责任，存储由运维系统人员管理，开发人员只需要通过pvc声明需要存储的类型、大小和访问模式即可；sc引入使pv自动创建或删除，开发人员定义的pvc中声明stroageclassname以及大小等需求自动创建pv；运维人员只需要声明好sc以及quota配额即可，无需维护pv。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;早期pod使用volume方式，每次pod都需要配置存储，volume都需要配置存储插件的一堆配置，如果是第三方存储，配置非常复杂；强制开发人员需要了解底层存储类型和配置。从而引入了pv。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    volumeMounts:
    - mountPath: /data
      name: data
  volumes:
  - name: data
    capacity:
      storage: 10Gi
    cephfs:
      monitors:
      - 192.168.0.1:6789
      - 192.168.0.2:6789
      - 192.168.0.3:6789
      path: /opt/eshop_dir/eshop
      user: admin
      secretRef:
        name: ceph-secret

&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;pv的yaml文件，pv其实就是把volume的配置声明从pod中分离出来。存储系统由运维人员管理，开发人员不知道底层配置，所以引入了pvc。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: cephfs
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  cephfs:
    monitors:
    - 192.168.0.1:6789
    - 192.168.0.2:6789
    - 192.168.0.3:6789
    path: /opt/eshop_dir/eshop
    user: admin
    secretRef:
      name: ceph-secret
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;pvc的yaml文件，pvc会根据声明的大小、存储类型和accessMode等关键字查找pv，如果找到了匹配的pv，则会与之关联,而pod直接关联pvc。运维人员需要维护一堆pv，如果pv不够还需要手工创建新的pv，pv空闲还需要手动回收，所以引入了sc。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: cephfs
spec:
  accessModes:
      - ReadWriteMany
  resources:
      requests:
        storage: 8Gi
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;storageclass类似声明了一个非常大的存储池，其中一个最重要参数是provisioner，这个provisioner可以aws-ebs，ceph和nfs等。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: aws-gp2
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  fsType: ext4
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2存储发展过程&#34;&gt;2、存储发展过程&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;最初通过volume plugin实现，又称in-tree。&lt;/li&gt;
&lt;li&gt;1.8开始，新的插件形式支持外部存储系统，即FlexVolume,通过外部脚本集成外部存储接口。&lt;/li&gt;
&lt;li&gt;1.9开始，csi接入，存储厂商需要实现三个服务接口Identity Service、Controller Service、Node Service。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Identity service用于返回一些插件信息。
Controller Service实现Volume的curd操作。
Node Service运行在所有的Node节点，用于实现把volume挂载在当前Node节点的指定目录，该服务会监听一个socket，controller通过这个socket进行通信。
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;云原生分布式存储（Container Attached Storage）CAS&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;1、重新设计一个分布式存储，像openebs/longhorn/PortWorx/StorageOS。
2、已有的分布式存储包装管理，像Rook。
3、CAS：每个volume都由一个轻量级的controller来管理，这个controller可以是一个单独的pod；这个controller与使用该volume的应用pod在同一个node；不同的volume的数据使用多个独立的controller pod进行管理。
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3存储分类&#34;&gt;3、存储分类&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;块存储&lt;/li&gt;
&lt;li&gt;共享存储&lt;/li&gt;
&lt;li&gt;对象存储&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4各个存储fio性能测试供参考&#34;&gt;4、各个存储fio性能测试，供参考。&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://Forest-L.github.io/img/storage-fio.png&#34; alt=&#34;存储性能对比&#34;&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Pmem与块存储性能对比</title>
      <link>https://Forest-L.github.io/post/pmem-versus-block-storage-performance/</link>
      <pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/pmem-versus-block-storage-performance/</guid>
      <description>&lt;h2 id=&#34;1pmem介绍&#34;&gt;1、Pmem介绍&lt;/h2&gt;
&lt;p&gt;PMEM是硬件产品，Intel Optane DC持久存储模块，是一种具有大容量和数据持久性的创新存储技术。有2种运行模式。两级内存模式无需软件更改，DCPMM被视为更大的内存，并使用DRAM作为其缓存层。AppDirect模式将设备暴露为持久内存，支持软件栈，可用于加速不同的应用程序。在本文中，我们将使用AppDirect模式。&lt;/p&gt;
&lt;h2 id=&#34;2环境信息&#34;&gt;2、环境信息&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;一台master和一台node构成的K8s1.18.6集群&lt;/li&gt;
&lt;li&gt;redis镜像为根据源码编译为pmem-redis:4.0.0&lt;/li&gt;
&lt;li&gt;ceph/neonsan块存储&lt;/li&gt;
&lt;li&gt;centos7.7/内核5.8.7&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3测试两种模式&#34;&gt;3、测试两种模式&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;测试目的是体验PMEM对REDIS的加速效果。先在没有PMEM加速AOF模式，再有PMEM加速PBA模式。&lt;/li&gt;
&lt;li&gt;AOF模式是append only file的意思，通常REDIS是一种内存数据库，数据掉电就丢失了。AOF模式可以把数据库记录随时备份到分布式存储里，这样可以使得REDIS具有掉电恢复的功能。&lt;/li&gt;
&lt;li&gt;PBA模式是pointer based AOF模式，它是使用PMEM对AOF做了加速，原理是备份写盘时只把指针写到磁盘里，数据还在内存或PMEM里，使用PMEM作为缓存。这样既可以掉电恢复，又提升了性能。充分发挥了PMEM AD模式的优势。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4pmem-redis镜像构建&#34;&gt;4、pmem-redis镜像构建&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;git clone &lt;a href=&#34;https://github.com/clayding/opencloud_benchmark.git&#34;&gt;https://github.com/clayding/opencloud_benchmark.git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;cd opencloud_benchmark/k8s/redis/docker&lt;/li&gt;
&lt;li&gt;docker build -t pmem-redis:latest &amp;ndash;network host .&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;5pba模式的设置&#34;&gt;5、PBA模式的设置&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ipmctl安装&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;cd /etc/yum.repos.d/
wget https://copr.fedorainfracloud.org/coprs/jhli/ipmctl/repo/epel-7/jhli-ipmctl-epel-7.repo
wget https://copr.fedorainfracloud.org/coprs/jhli/safeclib/repo/epel-7/jhli-safeclib-epel-7.repo
yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
el-ha-for-rhel-*-server-rpms&amp;quot;
yum install ndctl ndctl-libs ndctl-devel libsafec rubygem-asciidoctor
yum install ipmctl
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;app direct模式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sudo ipmctl delete -goal
sudo ipmctl create -goal PersistentMemoryType=AppDirect

A reboot is required to process new memory allocation goals:
sudo reboot
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;检测Pmem能正常工作且为ad模式,ad是否有值&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;ipmctl show -memoryresources
 MemoryType   | DDR         | PMemModule  | Total
========================================================
 Volatile     | 191.000 GiB | 0.000 GiB   | 191.000 GiB
 AppDirect    | -           | 504.000 GiB | 504.000 GiB
 Cache        | 0.000 GiB   | -           | 0.000 GiB
 Inaccessible | 1.000 GiB   | 1.689 GiB   | 2.689 GiB
 Physical     | 192.000 GiB | 505.689 GiB | 697.689 GiB

 当ad没有值时，ipmctl start -diagnostic诊断是否有错误消息
 刚开始遇到这样的一个问题： 
 The platform configuration check detected that PMem module 0x0001 is not configured.
 分析为：新版Ipmctl有问题，用1.x的版本把pcd delete以后重新provision就可以了
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;6pmem-csi安装&#34;&gt;6、Pmem-csi安装&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;下载， https://github.com/intel/pmem-csi/blob/devel/docs/install.md#install-pmem-csi-driver
cd pmem-CSI

Setting up certificates for securities
# curl -L https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o _work/bin/cfssl --create-dirs
# curl -L https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o _work/bin/cfssljson --create-dirs
# chmod a+x _work/bin/cfssl _work/bin/cfssljson
# export PATH=$PATH:$PWD/_work/bin
# ./test/setup-ca-kubernetes.sh

Deploying the driver to K8s using LVM mode, please choose yaml files corresponding to your kubernetes version
# kubectl create -f deploy/kubernetes-1.18/pmem-csi-lvm.yaml
Applying a storage class
# kubectl apply -f deploy/kubernetes-1.18/pmem-storageclass-ext4.yaml
pod状态
kubectl get pod
NAME                    READY   STATUS        RESTARTS   AGE
pmem-csi-controller-0   2/2     Running       0          22s
pmem-csi-node-tw4mw     2/2     Running       2          33h
sc状态
kubectl get sc
NAME               PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
pmem-csi-sc-ext4   pmem-csi.intel.com         Delete          Immediate           false                  3d2h
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;7benchmark安装&#34;&gt;7、benchmark安装&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;yum install autoconf automake make gcc-c++&lt;/li&gt;
&lt;li&gt;yum install pcre-devel zlib-devel libmemcached-devel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Remove system libevent and install new version:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sudo yum remove libevent&lt;/li&gt;
&lt;li&gt;wget &lt;a href=&#34;https://github.com/downloads/libevent/libevent/libevent-2.0.21-stable.tar.gz&#34;&gt;https://github.com/downloads/libevent/libevent/libevent-2.0.21-stable.tar.gz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;tar xfz libevent-2.0.21-stable.tar.gz&lt;/li&gt;
&lt;li&gt;pushd libevent-2.0.21-stable&lt;/li&gt;
&lt;li&gt;./configure&lt;/li&gt;
&lt;li&gt;make&lt;/li&gt;
&lt;li&gt;sudo make install&lt;/li&gt;
&lt;li&gt;popd&lt;/li&gt;
&lt;li&gt;export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:${PKG_CONFIG_PATH}&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Build and install memtier:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;git clone &lt;a href=&#34;https://github.com/RedisLabs/memtier_benchmark&#34;&gt;https://github.com/RedisLabs/memtier_benchmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;cd memtier_benchmark&lt;/li&gt;
&lt;li&gt;autoreconf -ivf&lt;/li&gt;
&lt;li&gt;./configure &amp;ndash;disable-tls&lt;/li&gt;
&lt;li&gt;make&lt;/li&gt;
&lt;li&gt;sudo make install&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;8测试对比&#34;&gt;8、测试对比&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;aof的yanl文件&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language----&#34; data-lang=&#34;---&#34;&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 2Gi
  storageClassName: csi-neonsan
---
kind: Pod
apiVersion: v1
metadata:
  name: redis-aof
  labels:
    app: redis-aof
spec:
  containers:
    - name: redis-aof
      image: pmem-redis:latest
      imagePullPolicy: IfNotPresent
      args: [&amp;quot;no&amp;quot;]
      ports:
      - containerPort: 6379
      resources:
        limits:
          cpu: &amp;quot;1000m&amp;quot;
          memory: &amp;quot;16Gi&amp;quot;
        requests:
          cpu: &amp;quot;1000m&amp;quot;
          memory: &amp;quot;16Gi&amp;quot;
      volumeMounts:
      - mountPath: &amp;quot;/ceph&amp;quot;
        name: ceph-csi-volume
  volumes:
  - name: ceph-csi-volume
    persistentVolumeClaim:
      claimName: rbd-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: redis
  labels:
    app: redis-aof
spec:
  type: NodePort
  ports:
  - name: redis
    port: 6379
    nodePort: 30379
    targetPort: 6379
  selector:
    app: redis-aof
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;pba的yanl文件&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pmem-csi-pvc-ext4
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  storageClassName: pmem-csi-sc-ext4 # defined in pmem-storageclass-ext4.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 2Gi
  storageClassName: csi-neonsan
---
kind: Pod
apiVersion: v1
metadata:
  name: redis-with-pba
  labels:
    app: redis-with-pba
spec:
  containers:
    - name: redis-with-pba
      image: pmem-redis:latest
      imagePullPolicy: IfNotPresent
      args: [&amp;quot;yes&amp;quot;]
      ports:
      - containerPort: 6379
      resources:
        limits:
          cpu: &amp;quot;1000m&amp;quot;
          memory: &amp;quot;16Gi&amp;quot;
        requests:
          cpu: &amp;quot;1000m&amp;quot;
          memory: &amp;quot;16Gi&amp;quot;
      volumeMounts:
      - mountPath: &amp;quot;/data&amp;quot;
        name: my-csi-volume
      - mountPath: &amp;quot;/ceph&amp;quot;
        name: ceph-csi-volume
  volumes:
  - name: ceph-csi-volume
    persistentVolumeClaim:
      claimName: rbd-pvc
  - name: my-csi-volume
    persistentVolumeClaim:
      claimName: pmem-csi-pvc-ext4
---
apiVersion: v1
kind: Service
metadata:
  name: redis-pba
  labels:
    app: redis-with-pba
spec:
  type: NodePort
  ports:
  - name: redis
    port: 6379
    nodePort: 31379
    targetPort: 6379
  selector:
    app: redis-with-pba
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;先执行aof的yaml文件，kubectl apply -f aof.yaml，然后再执行memtier_benchmark&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;memtier_benchmark -s 172.30.35.9  -p 30379 -R -d 1024 --key-maximum=1000000 -n 10000  --ratio=1:9 | grep Totals&amp;gt;&amp;gt;aof_1024
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 100%,  37 secs]  0 threads:     2000000 ops,   53289 (avg:   53544) ops/sec, 7.23MB/sec (avg: 7.29MB/sec),  3.75 (avg:  3.73) msec latency



[root@neonsan-10 scripts]# cat aof_1024
Totals      53508.29        26.75     48130.71         3.73350         3.27900         8.31900        18.43100      7456.87
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;先执行pba的yaml文件，kubectl apply -f pba.yaml，然后再执行memtier_benchmark,注意yaml文件里面同时挂载不同存储。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;memtier_benchmark -s 172.30.35.9  -p 31379 -R -d 1024 --key-maximum=1000000 -n 10000  --ratio=1:9 | grep Totals&amp;gt;&amp;gt;pba_1024
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 100%,  24 secs]  0 threads:     2000000 ops,   81619 (avg:   81294) ops/sec, 11.07MB/sec (avg: 11.10MB/sec),  2.45 (avg:  2.46) msec latency


[root@neonsan-10 scripts]# cat pba_1024
Totals          0.00         0.00         0.00            -nan         0.00000         0.00000         0.00000         0.00
Totals      82856.35        82.86     74487.86         2.45929         2.38300         4.79900         6.30300     11588.37
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;通过速度和延迟性比较两种存储的性能。&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>CkA考试经验</title>
      <link>https://Forest-L.github.io/post/cka-test-experience/</link>
      <pubDate>Sat, 11 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/cka-test-experience/</guid>
      <description>&lt;h1 id=&#34;cka考试经验&#34;&gt;CKA考试经验&lt;/h1&gt;
&lt;p&gt;CKA: Kubernetes管理员认证（CKA）旨在确保认证持有者具备履行Kubernetes管理员职责的技能，知识和能力。如果企业想要申请 KCSP ，条件之一是：至少需要三名员工拥有CKA认证。
&lt;img src=&#34;https://Forest-L.github.io/img/cka.jpg&#34; alt=&#34;CKA图片.jpg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-考试报名&#34;&gt;1. 考试报名&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;%5Bhttps://www.cncf.io/certification/cka/%5D&#34;&gt;CKA报名地址&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;注意事项：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;1、报名成功之后，可在12个月之内进行考试，考试不过有一次补考机会。&lt;/p&gt;
&lt;p&gt;2、CKA：74分或以上可以获得证书。&lt;/p&gt;
&lt;p&gt;3、每年Cyber Monday（网络星期一，也就是黑五后的第一个星期一）有优惠或者某些辅导架构有优惠券。&lt;/p&gt;
&lt;h2 id=&#34;2-备考&#34;&gt;2. 备考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;考试时你可以打开两个浏览器Tab，一个是考试窗口，一个用来查阅官方文档（仅允许访问https://kubernetes.io/docs/、https://github.com/kubernetes/ 和https://kubernetes.io/blog/ ）&lt;/li&gt;
&lt;li&gt;查询文档的浏览器Tab可以弄成标签。&lt;/li&gt;
&lt;li&gt;考试前一周看下官网k8s版本，然后部署一个同样版本的K8s练习。&lt;/li&gt;
&lt;li&gt;可以参考考试大纲复习。&lt;/li&gt;
&lt;li&gt;怎么复习，可以在自己搭建的环境下，操作指令，生成对应的。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3-考前检查及考试环境&#34;&gt;3. 考前检查及考试环境&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;考试形式: 在线监控，需要共享桌面和摄像头&lt;/li&gt;
&lt;li&gt;考试环境: 在一个密闭空间，例如书房、卧室、会议室等，电脑屏幕不能对着窗户，房间里除了考生不能存在第二个人，考试的桌面不能放其它东西，水杯也不行&lt;/li&gt;
&lt;li&gt;考试时间及题目: CKA-3小时-24道题&lt;/li&gt;
&lt;li&gt;选择考试时间: 报名成功之后可以在12个月之内进行考试，考试之前需要选择考试时间，选择考试时间的时候记得先选择北京时区，默认是0时区时间。&lt;/li&gt;
&lt;li&gt;电脑要求: 可以在这里WebDelivery Compatibility Check检测自己的电脑环境和网络速度等&lt;/li&gt;
&lt;li&gt;选择的是Linux-Foundation&amp;mdash;&amp;gt;CKA-English&lt;/li&gt;
&lt;li&gt;考试前考官检查:
考试可以提前15分钟进入考试界面
考官会以发消息的方式和你交流（没有语音交流）
看不懂考官发的英文怎么办：可以在chrome浏览器右键翻译
考官会让你共享摄像头，共享桌面 考官会让你出示能确认你身份ID的证件，我当时用的是罗技C310摄像头，无法对焦，护照看上去模糊到不行，后来考官又叫我给护照打光还是不行，后面又叫我打开我的手机，用手机相机当作放大镜用，这样才能看清楚。（我考CKAD的时候，我护照还没举稳，考官就说可以了，应该是考过CKA，他们系统里面已经有我的信息了，就随便瞄了一眼而已）
考官会让你用摄像头环视房间一周，确认你的考试环境（当时我房间门开了一个小缝也要求我去把门关好，还是比较严格）
考官会让你用摄像头看你的整个桌面和桌子底下
考官会让你打开任务管理器，点击左下角简略信息，是否已关闭了其它后台服务。
考官会让你再次点一下桌面共享，然后你叫你点击取消，然后就开始进入考试了&lt;/li&gt;
&lt;li&gt;考试的界面:
左边是题目
右边是终端
终端上面是共享摄像头、共享屏幕、考试信息等按钮（可以唤出记事本）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-考试心得&#34;&gt;4. 考试心得&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;有Notepad记事本，可以记录下自己环境信息，哪道题还没做，题目中的信息等。&lt;/li&gt;
&lt;li&gt;一定要记得用鼠标，拷贝和粘贴特别方便。&lt;/li&gt;
&lt;li&gt;尽量在官网中拷贝yaml文件到答题环境中。&lt;/li&gt;
&lt;li&gt;很多指令记得不清楚，请使用-h，比如etcdctl,node不能调度等。&lt;/li&gt;
&lt;li&gt;特别重要，根据个人考试，然后在浏览器中收藏的记录为：
&lt;img src=&#34;https://Forest-L.github.io/img/content.png&#34; alt=&#34;CKA考试相关内容.png&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>iptables指令将容器内部端口映射到外部宿主机端口指南</title>
      <link>https://Forest-L.github.io/post/the-iptables-directive-is-a-guide-to-mapping-ports-inside-containers-to-ports-on-external-hosts/</link>
      <pubDate>Tue, 07 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/the-iptables-directive-is-a-guide-to-mapping-ports-inside-containers-to-ports-on-external-hosts/</guid>
      <description>&lt;h4 id=&#34;背景&#34;&gt;背景：&lt;/h4&gt;
&lt;p&gt;docker run 某个容器，忘记了-p/-P 映射端口操作时，怎么把容器端口映射到主机上呢？以下描述的是如何通过iptables指令把容器端口映射到外部宿主机端口操作，防止容器重新创建。
宿主机docker启了一个容器，在容器里面又部署了一个pod，而部署pod这个服务是后续操作的，宿主机docker启容器时没有把端口映射出来，如何通过宿主机去访问pod服务。&lt;/p&gt;
&lt;h2 id=&#34;1相关联的认知&#34;&gt;1、相关联的认知&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;docker run -p: 指令中的小p为具体的宿主机端口映射到容器内部开放的网络端口上。&lt;/li&gt;
&lt;li&gt;docker run -P: 指令中的大P为随机选择一个宿主机端口映射到容器内部开放的网络端口上。&lt;/li&gt;
&lt;li&gt;docker run -p: 可以绑定多IP和端口（跟多个-p）。&lt;/li&gt;
&lt;li&gt;kubectl expose &amp;ndash;type=nodePort: 指令将容器内部端口映射到主机上(宿主机为随机端口，范围30000-32767/在service中编辑修改为具体端口)。&lt;/li&gt;
&lt;li&gt;kubectl expose &amp;ndash;type=lb: 指令，为直接将容器服务暴露出去。&lt;/li&gt;
&lt;li&gt;kubectl ingress: 它允许你基于路径或者子域名来路由流量到后端服务,7层协议http/https。&lt;/li&gt;
&lt;li&gt;kubectl port-forward: 将容器端口转发至本地端口，也可以转发TCP流量。&lt;/li&gt;
&lt;li&gt;kubectl kube-proxy: 只能转发HTTP流量。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2docker与iptables关系&#34;&gt;2、docker与iptables关系&lt;/h2&gt;
&lt;p&gt;源地址变换规则、目标地址变换规则、自定义限制外部ip规则、docker容器间通信iptables规则、docker网络与ip-forward和具体的用iptables指令将容器内部端口映射到外部宿主机端口操作指令。&lt;/p&gt;
&lt;h3 id=&#34;21源ip地址变换规则&#34;&gt;2.1、源ip地址变换规则&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Docker安装完成后，将默认在宿主机系统上增加一些iptables规则，以用于Docker容器和容器之间以及和外界的通信，可以使用iptables-save命令查看。&lt;/li&gt;
&lt;li&gt;其中nat表中的POSTROUTING链有这么一条规则&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE

参数说明：
-s ：源地址172.17.0.0/16
-o：指定数据报文流出接口为docker0
-j ：动作为MASQUERADE（地址伪装）
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;上面这条规则关系着Docker容器和外界的通信，含义是：
判断源地址为172.17.0.0/16的数据包（即Docker容器发出的数据），当不是从docker0网卡发出时做SNAT（源地址转换）。
这样一来，从Docker容器访问外网的流量，在外部看来就是从宿主机上发出的，外部感觉不到Docker容器的存在。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;22目标地址变换规则&#34;&gt;2.2、目标地址变换规则&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;从Docker容器访问外网的流量，在外部看来就是从宿主机上发出的，外部感觉不到Docker容器的存在。那么，外界想到访问Docker容器的服务时，同样需要相应的iptables规则.&lt;/li&gt;
&lt;li&gt;以启动tomcat容器，将其8080端口映射到宿主机上的8080端口为例,然后通过iptables-save查看：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;docker run -itd --name  tomcat -p 8080:8080 tomcat:latest
#iptables-save
*nat
-A POSTROUTING -s 172.18.0.2/32 -d 172.18.0.2/32 -p tcp -m tcp --dport 8080 -j MASQUERADE
...
*filter
-A DOCKER -d 172.18.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 8080 -j ACCEPT
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;可以看到，在nat、filter的Docker链中分别增加了一条规则&lt;/li&gt;
&lt;li&gt;这两条规则将访问宿主机8080端口的流量转发到了172.17.0.4的8080端口上（即真正提供服务的Docker容器IP和端口）。所以外界访问Docker容器是通过iptables做DNAT（目的地址转换）实现的。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;23自定义限制外部ip规则&#34;&gt;2.3、自定义限制外部ip规则&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Docker的forward规则默认允许所有的外部IP访问容器&lt;/li&gt;
&lt;li&gt;可以通过在filter的DOCKER链上添加规则来对外部的IP访问做出限制&lt;/li&gt;
&lt;li&gt;只允许源IP192.168.0.0/16的数据包访问容器，需要添加如下规则：
&lt;code&gt;iptables -I DOCKER -i docker0 ! -s 192.168.0.0/16 -j DROP&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;24docker容器间通信iptables规则&#34;&gt;2.4、docker容器间通信iptables规则&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;不仅仅是与外界间通信，Docker容器之间互个通信也受到iptables规则限制。&lt;/li&gt;
&lt;li&gt;同一台宿主机上的Docker容器默认都连在docker0网桥上，它们属于一个子网，这是满足相互通信的第一步。&lt;/li&gt;
&lt;li&gt;Docker daemon启动参数&amp;ndash;icc(icc参数表示是否允许容器间相互通信)设置为false时会在filter的FORWARD链中增加一条ACCEPT的规则（&amp;ndash;icc=true）：
&lt;code&gt;-A FORWARD -i docker0 -o docker0 -j ACCEPT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;当Docker datemon启动参数&amp;ndash;icc设置为false时，以上规则会被设置为DROP，Docker容器间的相互通信就被禁止,默认是ACCEPT。&lt;/li&gt;
&lt;li&gt;这种情况下，想让两个容器通信就需要在docker run时使用&amp;ndash;link选项。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;25docker网络与ip-forward&#34;&gt;2.5、docker网络与ip-forward&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;在Docker容器和外界通信的过程中，还涉及了数据包在多个网卡间的转发，如从docker0网卡转发到宿主机ens160网卡，这需要内核将ip-forward功能打开&lt;/li&gt;
&lt;li&gt;即将ip_forward系统参数设1：echo 1 &amp;gt; /proc/sys/net/ipv4/ip_forward&lt;/li&gt;
&lt;li&gt;Docker daemon启动的时候默认会将其设为1（&amp;ndash;ip-forward=true）&lt;/li&gt;
&lt;li&gt;永久生效的ip转发
&lt;code&gt;vim /etc/sysctl.conf&lt;/code&gt;
&lt;code&gt;net.ipv4.ip_forward = 1&lt;/code&gt;
&lt;code&gt;sysctl -p /etc/sysctl.conf&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;26iptables指令映射&#34;&gt;2.6、iptables指令映射&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;需要执行三条指令,其中就修改两个参数:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;iptables -t nat -A DOCKER -p tcp --dport ${YOURPORT} -j DNAT --to-destination ${CONTAINERIP}:${YOURPORT}

iptables -t nat -A POSTROUTING -j MASQUERADE -p tcp --source ${CONTAINERIP} --destination ${CONTAINERIP} --dport ${YOURPORT}

iptables -A DOCKER -j ACCEPT -p tcp --destination ${CONTAINERIP} --dport ${YOURPORT}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;${CONTAINERIP} 就是对应容器的ip地址，比如我的容器ip地址是 172.18.0.2 ，（容器的IP可以通过如下方式查看：a.在容器中：ip addr;b.在宿主机中: docker inspect 容器名 |grep IPAddress ）所以我就把上述的参数换成我的IP地址。&lt;/li&gt;
&lt;li&gt;${YOURPORT} 就是要映射出来的端口，我配置的是一个console平台，其端口是30880&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3注意地方及参考&#34;&gt;3、注意地方及参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如果容器是pod形式启的，上面iptables指令映射不适合，其中有对docker链的操作。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/19335444/how-do-i-assign-a-port-mapping-to-an-existing-docker-container&#34;&gt;映射port至存在的docker容器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/zh/docs/concepts/services-networking/service/&#34;&gt;k8s如何访问&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>chart包实践开发指南</title>
      <link>https://Forest-L.github.io/post/chart-package-practice-development-guide/</link>
      <pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/chart-package-practice-development-guide/</guid>
      <description>&lt;h2 id=&#34;chart包文件结构以wordpress包为例&#34;&gt;chart包文件结构，以wordpress包为例&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;wordpress/
  Chart.yaml          # 包含有关chart信息的YAML文件
  LICENSE             # OPTIONAL: 包含chart许可证的纯文本文件
  README.md           # OPTIONAL: 一个可读的README文件
  requirements.yaml   # OPTIONAL: 一个YAML文件，列出了chart的依赖关系
  values.yaml         # 该chart的默认配置值
  charts/             # OPTIONAL: 包含此chart所依赖的任何chart的目录。
  templates/          # OPTIONAL: 一个模板目录，当与values相结合时，
                      # 将生成有效的Kubernetes清单文件
  templates/NOTES.txt # OPTIONAL: 包含简短使用说明的纯文本文件
  templates/_helpers.tpl # OPTIONAL:通过define 函数定义命名模板
  crds/               # OPTIONAL: 自定义资源
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;以下说明chart包内容&#34;&gt;以下说明chart包内容&lt;/h3&gt;
&lt;h2 id=&#34;chartyaml文件内容格式&#34;&gt;Chart.yaml文件内容格式&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;name: chart的名称 (required)
version: 一个SemVer 2(语义化版本)版本(required)
description: 这个项目的单句描述 (optional)
keywords:
  - 关于此项目的关键字列表 (optional)
home: 该项目主页的URL(optional)
sources:
  - 此项目的源代码URL列表 (optional)
dependencies: chart依赖关系 (optional)
  - name: chart名字 (nginx)
    version: chart版本 (&amp;quot;1.2.3&amp;quot;)
    repository: url仓库 (&amp;quot;https://example.com/charts&amp;quot;) 
    condition: (optional) 布尔值的yaml路径，用于启用/禁用图表 
maintainers: # (optional)
  - name: 维护者的名称 (每个维护者都需要)
    email: 维护者的email (optional for each maintainer)
    url: 维护者的url (optional for each maintainer)
engine: gotpl＃模板引擎的名称（可选，默认为gotpl）
icon: 要用作图标的SVG或PNG图像的URL (optional)
appVersion: 包含的应用程序版本（可选）这个不一定是SemVer
deprecated: 此chart是否已被弃用（可选，布尔型）
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;readmemd内容&#34;&gt;README.md内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Introduction&lt;/li&gt;
&lt;li&gt;Prerequisites&lt;/li&gt;
&lt;li&gt;Installing the Chart&lt;/li&gt;
&lt;li&gt;Uninstalling the Chart&lt;/li&gt;
&lt;li&gt;Configuration&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;requirementsyaml介绍&#34;&gt;requirements.yaml介绍&lt;/h2&gt;
&lt;p&gt;在Helm中，一个chart可能取决于任何数量的其他chart。 这些依赖关系可以通过requirements.yaml文件动态链接，或者引入charts/目录并手动管理。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dependencies:
  - name: apache
    version: 1.2.3
    repository: http://example.com/charts
    alias: new-subchart-1
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Name: 你想要的chart的名称。&lt;/li&gt;
&lt;li&gt;Version: 你想要的chart的版本。&lt;/li&gt;
&lt;li&gt;repository字段是图表存储库的完整URL。 请注意，您还必须使用helm repo add在本地添加该repository。&lt;/li&gt;
&lt;li&gt;alias：别名。
一旦有一个依赖关系文件，可以运行helm dependency update，它会使用你的依赖关系文件为你下载所有指定的chart到你的charts/目录中。
可以在values.yaml定义true/false判断依赖包是否被启用，如&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apache:
  enabled: true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;依赖关系可以是chart压缩包（foo-1.2.3.tgz），也可以是未打包的chart目录。
依赖执行顺序：参考k8s负载自启动原理，所以我们可以不关心执行顺利。实际上交叉执行。&lt;/p&gt;
&lt;h4 id=&#34;说明helm2是通过requirementsyaml文件描述依赖关系helm3直接在chartyaml描述&#34;&gt;说明：helm2是通过requirements.yaml文件描述依赖关系，helm3直接在Chart.yaml描述。&lt;/h4&gt;
&lt;h2 id=&#34;templatesk8s资源&#34;&gt;templates/k8s资源&lt;/h2&gt;
&lt;p&gt;templates下有多个deployment对象，可以命名不同名字。
执行顺序：参考k8s负载自启动原理，所以我们可以不关心执行顺利。
实际执行顺序为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var InstallOrder KindSortOrder = []string{
    &amp;quot;Namespace&amp;quot;,
    &amp;quot;NetworkPolicy&amp;quot;,
    &amp;quot;ResourceQuota&amp;quot;,
    &amp;quot;LimitRange&amp;quot;,
    &amp;quot;PodSecurityPolicy&amp;quot;,
    &amp;quot;PodDisruptionBudget&amp;quot;,
    &amp;quot;Secret&amp;quot;,
    &amp;quot;ConfigMap&amp;quot;,
    &amp;quot;StorageClass&amp;quot;,
    &amp;quot;PersistentVolume&amp;quot;,
    &amp;quot;PersistentVolumeClaim&amp;quot;,
    &amp;quot;ServiceAccount&amp;quot;,
    &amp;quot;CustomResourceDefinition&amp;quot;,
    &amp;quot;ClusterRole&amp;quot;,
    &amp;quot;ClusterRoleList&amp;quot;,
    &amp;quot;ClusterRoleBinding&amp;quot;,
    &amp;quot;ClusterRoleBindingList&amp;quot;,
    &amp;quot;Role&amp;quot;,
    &amp;quot;RoleList&amp;quot;,
    &amp;quot;RoleBinding&amp;quot;,
    &amp;quot;RoleBindingList&amp;quot;,
    &amp;quot;Service&amp;quot;,
    &amp;quot;DaemonSet&amp;quot;,
    &amp;quot;Pod&amp;quot;,
    &amp;quot;ReplicationController&amp;quot;,
    &amp;quot;ReplicaSet&amp;quot;,
    &amp;quot;Deployment&amp;quot;,
    &amp;quot;HorizontalPodAutoscaler&amp;quot;,
    &amp;quot;StatefulSet&amp;quot;,
    &amp;quot;Job&amp;quot;,
    &amp;quot;CronJob&amp;quot;,
    &amp;quot;Ingress&amp;quot;,
    &amp;quot;APIService&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;两种方式可以提前执行,一种设置pre-install,另一种是设置权重：
pre-install hooks，如：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: foo
  annotations:
    &amp;quot;helm.sh/hook&amp;quot;: &amp;quot;pre-install&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;定义权重，如：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;annotations:
    &amp;quot;helm.sh/hook-weight&amp;quot;: &amp;quot;5&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;valuesyaml&#34;&gt;values.yaml&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Release.Name: release的名称(不是chart的名称！)&lt;/li&gt;
&lt;li&gt;Release.Namespace: chart release的名称空间。&lt;/li&gt;
&lt;li&gt;Release.Service: 进行release的服务。 通常这是Tiller。&lt;/li&gt;
&lt;li&gt;chart版本可以作为Chart.Version获得。Chart：Chart.yaml 的内容。&lt;/li&gt;
&lt;li&gt;templates下有多个deployment对象，可以命名不同名字，然后在values.yaml以不同名字打头定义值。，如以下格式定义：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;mysql:
  name: 
  image:
    repository: 
    tag: 
    pullPolicy:

redis:
  name: 
  image:
    repository: 
    tag: 
    pullPolicy:
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;helm模板&#34;&gt;helm模板&lt;/h2&gt;
&lt;p&gt;helm模板语法嵌套在{{和}}之间，有三个常见的&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.Values.*
从value.yaml文件中读取或者--set获取（--set优先级最大）。
.Release.*
从运行Release的元数据读取,每次安装均会生成一个新的release
template * .
从_helpers.tpl文件中读取，通过define 函数定义命名模板
.Chart.*
从Chart.yaml文件中读取
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;模板函数和管道&#34;&gt;模板函数和管道&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;* | 管道，类似linux下的管道，以下实例效果是一样的。
{{ quote .Values.favorite.drink }}与 {{ .Values.favorite.drink | quote }}
* default制定默认值
drink: {{ .Values.favorite.drink | default “tea” | quote }}
* indent 模板函数，对左空出空格，左边空出两个空格
{{ include &amp;quot;mychart_app&amp;quot; . | indent 2 }}
include 函数，与 template 类似功能
如实例，在_helpers.tpl中define模板，在资源对象中引用。


{{- define &amp;quot;mychart.labels&amp;quot; }}
  labels:
    generator: helm
    date: {{ now | htmlDate }}
{{- end }}

apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-configmap
  {{- template &amp;quot;mychart.labels&amp;quot; }}
data:
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;在模板中使用文件&#34;&gt;在模板中使用文件&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: conf
data:
{{ (.Files.Glob &amp;quot;foo/*&amp;quot;).AsConfig | indent 2 }}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;chart根目录下foo目录的所有文件配置为configmap的内容&lt;/p&gt;
&lt;h2 id=&#34;模板流程控制&#34;&gt;模板流程控制&lt;/h2&gt;
&lt;p&gt;常用的有
if/else 条件控制
with 范围控制
range 循环控制
如：values.yaml中定义变量，ConfigMap中.Values.favorite循环控制参数。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;favorite:
  drink: coffee
  food: pizza

apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-configmap
data:
  myvalue: &amp;quot;Hello World&amp;quot;
  {{- range $key, $val := .Values.favorite }}
  {{ $key }}: {{ $val | quote }}
  {{- end}}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在deployment.yaml文件中使用if/else语法，如：- end结束标志，双括号都有“-”。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{- if .Values.image.repository -}}
image: {.Values.image.repository}
{{- else -}}
image: &amp;quot;***/{{ .Release.Name }}:{{ .Values.image.version }}&amp;quot;
{{- end -}}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>ipv6地址搭建K8s</title>
      <link>https://Forest-L.github.io/post/ipv6-address-setup-k8s/</link>
      <pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/ipv6-address-setup-k8s/</guid>
      <description>&lt;h1 id=&#34;ipv6地址搭建k8s&#34;&gt;ipv6地址搭建K8s&lt;/h1&gt;
&lt;h3 id=&#34;环境&#34;&gt;环境&lt;/h3&gt;
&lt;p&gt;centos: 7.7
k8s: v1.16.0&lt;/p&gt;
&lt;h3 id=&#34;提前准备&#34;&gt;提前准备&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;修改主机名
&lt;code&gt;hostnamectl set-hostname node1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;添加ipv6地址及主机名
&lt;code&gt;vi /etc/hosts&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;添加操作系统的ipv6的参数，且使参数生效&lt;code&gt;sysctl -p&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vi /etc/sysctl.conf
net.ipv6.conf.all.disable_ipv6 = 0
net.ipv6.conf.default.disable_ipv6 = 0
net.ipv6.conf.lo.disable_ipv6 = 0
net.ipv6.conf.all.forwarding=1
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;开启ipv6,添加如下内容&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vi /etc/sysconfig/network
NETWORKING_IPV6=yes
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;开启网卡的ipv6,添加如下内容，最后执行reboot生效。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vi /etc/sysconfig/network-scripts/ifcfg-eth0
IPV6INIT=yes
IPV6_AUTOCONF=yes
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;关闭防火墙&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;systemctl stop firewalld
systemctl disable firewalld
setenforce 0
vi /etc/selinux/config
SELINUX=disabled
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;关闭虚拟内存,添加如下内容，最后通过执行sysctl -p /etc/sysctl.d/k8s.conf生效。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;swapoff -a
vi /etc/sysctl.d/k8s.conf 添加下面一行：
vm.swappiness=0
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;安装docker&#34;&gt;安装docker&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;yum install -y yum-utils device-mapper-persistent-data lvm2
yum install wget -y
wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo
sudo sed -i &#39;s+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+&#39; /etc/yum.repos.d/docker-ce.repo
yum makecache fast
yum install docker-ce -y
systemctl enable docker;systemctl restart docker
docker的配置vi /etc/docker/daemon.json
{
&amp;quot;insecure-registry&amp;quot;:[&amp;quot;0.0.0.0/0&amp;quot;],
&amp;quot;ipv6&amp;quot;: true,
&amp;quot;fixed-cidr-v6&amp;quot;: &amp;quot;2001:db8:1::/64&amp;quot;,
&amp;quot;host&amp;quot;:[&amp;quot;unix:///var/run/docker.sock&amp;quot;,&amp;quot;tcp://:::2375&amp;quot;],
&amp;quot;log-level&amp;quot;:&amp;quot;debug&amp;quot;
}
systemctl restart docker
echo &amp;quot;1&amp;quot; &amp;gt;/proc/sys/net/bridge/bridge-nf-call-iptables
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;安装kubectlkubeadm和kubelet插件&#34;&gt;安装kubectl、kubeadm和kubelet插件&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;添加k8s下载源
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

指定版本的安装
yum install kubelet-1.16.0 kubeadm-1.16.0 kubectl-1.16.0 -y
systemctl enable kubelet &amp;amp;&amp;amp; sudo systemctl start kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;初始化的准备&#34;&gt;初始化的准备&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;查看安装过程需要哪些镜像
&lt;code&gt;kubeadm config images list --kubernetes-version=v1.16.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;通过脚本下载所需的镜像。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vi images.sh 
#!/bin/bash
images=(kube-proxy:v1.16.0 kube-scheduler:v1.16.0 kube-controller-manager:v1.16.0 kube-apiserver:v1.16.0 etcd:3.3.15-0 pause:3.1 coredns:1.6.2)
for imageName in ${images[@]} ; do
docker pull gcr.azk8s.cn/google-containers/$imageName
docker tag gcr.azk8s.cn/google-containers/$imageName k8s.gcr.io/$imageName
docker rmi gcr.azk8s.cn/google-containers/$imageName
done
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;执行如下指令下载：&lt;code&gt;chmod +x images.sh &amp;amp;&amp;amp; ./images.sh&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;拷贝kubeadm.yaml文件，需要注意advertiseAddress参数为本机ipv6地址&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vi kubeadm.yaml 
apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: &amp;quot;2402:e7c0:0:a00:ffff:ffff:fffe:fffb&amp;quot;
  bindPort: 6443
nodeRegistration:
  taints:
  - effect: PreferNoSchedule
    key: node-role.kubernetes.io/master
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.16.0
networking:
  podSubnet: 1100::/52
  serviceSubnet: fd00:4000::/112
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;执行如下安装指令：&lt;code&gt;kubeadm init --config=kubeadm.yaml&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;如果使1.16.0之前版本需要安装指令后面添加如下参数执行：&lt;code&gt;--ignore-preflight-errors=HTTPProxy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;如果以下安装有问题，需要重置，先执行&lt;code&gt;kubeadm reset&lt;/code&gt;,再执行以上&lt;code&gt;kubeadm init&lt;/code&gt;指令，安装成功之后，需要做如下操作：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl taint node node1 node-role.kubernetes.io/master-
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;安装网络插件如calico&#34;&gt;安装网络插件，如calico&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;下载calico.yaml文件&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;curl https://docs.projectcalico.org/v3.11/manifests/calico.yaml -O
需要修改及添加的内容为,总共三处：
 &amp;quot;ipam&amp;quot;: {
     &amp;quot;type&amp;quot;: &amp;quot;calico-ipam&amp;quot;,
     &amp;quot;assign_ipv4&amp;quot;: &amp;quot;false&amp;quot;,
     &amp;quot;assign_ipv6&amp;quot;: &amp;quot;true&amp;quot;,
     &amp;quot;ipv4_pools&amp;quot;: [&amp;quot;172.16.0.0/16&amp;quot;, &amp;quot;default-ipv4-ippool&amp;quot;],
     &amp;quot;ipv6_pools&amp;quot;: [&amp;quot;1100::/52&amp;quot;, &amp;quot;default-ipv6-ippool&amp;quot;]
  },

- name: CALICO_IPV4POOL_CIDR
  value: &amp;quot;172.16.0.0/16&amp;quot;
- name: IP6
  value: &amp;quot;autodetect&amp;quot;
- name: CALICO_IPV6POOL_CIDR
  value: &amp;quot;1100::/52&amp;quot;

# Disable IPv6 on Kubernetes.
- name: FELIX_IPV6SUPPORT
  value: &amp;quot;true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;calico的镜像,可以提前下载
calico/cni:v3.11.1
calico/pod2daemon-flexvol:v3.11.1
calico/node:v3.11.1
calico/kube-controllers:v3.11.1&lt;/li&gt;
&lt;li&gt;执行calico，&lt;code&gt;kubectl apply -f calico.yaml&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;验证&#34;&gt;验证：&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl get pod --all-namespaces -o wide&lt;/code&gt;
&lt;code&gt;kubectl get nodes -o wide&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;部署tomcat应用验证&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl run tomcat  --image=tomcat:8.0  --port=8080
kubectl get pod
kubectl expose deployment tomcat  --port=8080 --target-port=8080 --type=NodePort
# kubectl get svc
NAME         TYPE        CLUSTER-IP        EXTERNAL-IP   PORT(S)          AGE
kubernetes   ClusterIP   fd00:4000::1      &amp;lt;none&amp;gt;        443/TCP          33m
tomcat       NodePort    fd00:4000::bf3e   &amp;lt;none&amp;gt;        8080:30693/TCP   22m

curl -6g [2402:e7c0:0:a00:ffff:ffff:fffe:fffb]:32012
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;参考&#34;&gt;参考&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kubernetes.org.cn/5173.html&#34;&gt;https://www.kubernetes.org.cn/5173.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>netapp存储在kubesphere上的实践</title>
      <link>https://Forest-L.github.io/post/netapp-stored-on-kubesphere-practice/</link>
      <pubDate>Fri, 13 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/netapp-stored-on-kubesphere-practice/</guid>
      <description>&lt;p&gt;&lt;strong&gt;NetApp&lt;/strong&gt;是向目前的数据密集型企业提供统一存储解决方案的居世界最前列的公司，其 Data ONTAP是全球首屈一指的存储操作系统。NetApp 的存储解决方案涵盖了专业化的硬件、软件和服务，为开放网络环境提供了无缝的存储管理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ontap&lt;/strong&gt;数据管理软件支持高速闪存、低成本旋转介质和基于云的对象存储等存储配置，为通过块或文件访问协议读写数据的应用程序提供统一存储。
&lt;strong&gt;Trident&lt;/strong&gt;是一个由NetApp维护的完全支持的开源项目。以帮助您满足容器化应用程序的复杂&lt;strong&gt;持久性&lt;/strong&gt;需求。
&lt;strong&gt;KubeSphere&lt;/strong&gt; 是一款开源项目，在目前主流容器调度平台 Kubernetes 之上构建的企业级分布式多租户&lt;strong&gt;容器管理平台&lt;/strong&gt;，提供简单易用的操作界面以及向导式操作方式，在降低用户使用容器调度平台学习成本的同时，极大降低开发、测试、运维的日常工作的复杂度。&lt;/p&gt;
&lt;h3 id=&#34;1整体方案&#34;&gt;1、整体方案&lt;/h3&gt;
&lt;p&gt;在VMware Workstation环境下安装ONTAP;ONTAP系统上创建SVM(Storage Virtual Machine)且对接nfs协议；在已有k8s环境下部署Trident,Trident将使用ONTAP系统上提供的信息（svm、managementLIF和dataLIF）作为后端来提供卷；在已创建的k8s和StorageClass卷下部署kubesphere。&lt;/p&gt;
&lt;h3 id=&#34;2版本信息&#34;&gt;2、版本信息&lt;/h3&gt;
&lt;p&gt;Ontap: 9.5
Trident: v19.07
k8s: 1.15
kubesphere: 2.0.2&lt;/p&gt;
&lt;h3 id=&#34;3步骤&#34;&gt;3、步骤&lt;/h3&gt;
&lt;p&gt;主要描述ontap搭建及配置、Trident搭建和配置和kubesphere搭建及配置等方面。参考&lt;a href=&#34;https://kubesphereio.com/post/netapp-works-with-k8s-in-kubesphere/&#34;&gt;ontap搭建&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;31-ontap搭建及配置&#34;&gt;3.1 ontap搭建及配置&lt;/h4&gt;
&lt;p&gt;在VMware Workstation上Simulate_ONTAP_9.5P6_Installation_and_Setup_Guide运行，ontap启动之后，按下面操作配置，其中以cluster base license、feature licenses for the non-ESX build配置证书、e0c、ip address：192.168.*.20、netmask:255.255.255.0、集群名:cluster1、密码等信息。
https://IP address,以上设置的iP地址，用户名和密码：
&lt;img src=&#34;https://Forest-L.github.io/img/netapp.png&#34; alt=&#34;netapp&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;32-trident搭建及配置&#34;&gt;3.2 Trident搭建及配置&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;下载安装包trident-installer-19.07.0.tar.gz，解压进入trident-installer目录，执行trident安装指令:
&lt;code&gt;./tridentctl install -n trident&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;结合ontap的提供的参数创建第一个后端vi backend.json。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;{
    &amp;quot;version&amp;quot;: 1,
    &amp;quot;storageDriverName&amp;quot;: &amp;quot;ontap-nas&amp;quot;,
    &amp;quot;backendName&amp;quot;: &amp;quot;customBackendName&amp;quot;,
    &amp;quot;managementLIF&amp;quot;: &amp;quot;10.0.0.1&amp;quot;,
    &amp;quot;dataLIF&amp;quot;: &amp;quot;10.0.0.2&amp;quot;,
    &amp;quot;svm&amp;quot;: &amp;quot;trident_svm&amp;quot;,
    &amp;quot;username&amp;quot;: &amp;quot;cluster-admin&amp;quot;,
    &amp;quot;password&amp;quot;: &amp;quot;password&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;生成后端卷&lt;code&gt;./tridentctl -n trident create backend -f backend.json&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;创建StorageClass,vi storage-class-ontapnas.yaml&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ontapnasudp
provisioner: netapp.io/trident
mountOptions: [&amp;quot;rw&amp;quot;, &amp;quot;nfsvers=3&amp;quot;, &amp;quot;proto=udp&amp;quot;]
parameters:
  backendType: &amp;quot;ontap-nas&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建StorageClass指令&lt;code&gt;kubectl create -f storage-class-ontapnas.yaml&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;33-kubesphere的安装及配置&#34;&gt;3.3 kubesphere的安装及配置&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;在 Kubernetes 集群中创建名为 kubesphere-system 和 kubesphere-monitoring-system 的 namespace。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;$ cat &amp;lt;&amp;lt;EOF | kubectl create -f -
---
apiVersion: v1
kind: Namespace
metadata:
    name: kubesphere-system
---
apiVersion: v1
kind: Namespace
metadata:
    name: kubesphere-monitoring-system
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;创建 Kubernetes 集群 CA 证书的 Secret。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl -n kubesphere-system create secret generic kubesphere-ca  \
--from-file=ca.crt=/etc/kubernetes/pki/ca.crt  \
--from-file=ca.key=/etc/kubernetes/pki/ca.key
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;若 etcd 已经配置过证书，则参考如下创建（以下命令适用于 Kubeadm 创建的 Kubernetes 集群环境）：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl -n kubesphere-monitoring-system create secret generic kube-etcd-client-certs  \
--from-file=etcd-client-ca.crt=/etc/kubernetes/pki/etcd/ca.crt  \
--from-file=etcd-client.crt=/etc/kubernetes/pki/etcd/healthcheck-client.crt  \
--from-file=etcd-client.key=/etc/kubernetes/pki/etcd/healthcheck-client.key
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;修改kubesphere.yaml中存储的设置参数和对应的参数即可
&lt;code&gt;kubectl apply -f kubesphere.yaml&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;访问 KubeSphere UI 界面。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://Forest-L.github.io/img/kubesphere.png&#34; alt=&#34;kubesphere&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;参考文档&#34;&gt;参考文档&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://docs.netapp.com/ontap-9/index.jsp?lang=zh_CN&#34;&gt;http://docs.netapp.com/ontap-9/index.jsp?lang=zh_CN&lt;/a&gt;
&lt;a href=&#34;https://netapp-trident.readthedocs.io/en/stable-v19.07/introduction.html&#34;&gt;https://netapp-trident.readthedocs.io/en/stable-v19.07/introduction.html&lt;/a&gt;
&lt;a href=&#34;https://github.com/kubesphere/ks-installer/blob/master/README_zh.md&#34;&gt;https://github.com/kubesphere/ks-installer/blob/master/README_zh.md&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>netapp联合k8s在kubesphere应用</title>
      <link>https://Forest-L.github.io/post/netapp-works-with-k8s-in-kubesphere/</link>
      <pubDate>Fri, 13 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/netapp-works-with-k8s-in-kubesphere/</guid>
      <description>&lt;h1 id=&#34;netapp联合k8s在kubesphere应用&#34;&gt;netapp联合k8s在kubesphere应用&lt;/h1&gt;
&lt;h4 id=&#34;配置说明&#34;&gt;配置说明&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;k8s: 1.13+&lt;/li&gt;
&lt;li&gt;ontap: 9.5&lt;/li&gt;
&lt;li&gt;trident: v19.07&lt;/li&gt;
&lt;li&gt;kubesphere: 2.0.2&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;前期准备&#34;&gt;前期准备&lt;/h4&gt;
&lt;p&gt;需要准备材料在以下链接上，链接：https://pan.baidu.com/s/1q3KugGrz-XWJzqhgD7Ze9g
提取码：rhyw&lt;/p&gt;
&lt;p&gt;包括ontap的安装说明，Workstation上ontap9.5模拟器的ova，ontap使用文档，对接各种存储的协议证书，&lt;/p&gt;
&lt;h4 id=&#34;1-ontap的安装&#34;&gt;1. ontap的安装&lt;/h4&gt;
&lt;p&gt;本次测试的环境是安装在VMware Workstation，具体参考链接上这个文档Simulate_ONTAP_9.5P6_Installation_and_Setup_Guide，大致流程为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;window机器的资源配置&lt;/li&gt;
&lt;li&gt;开启VT&lt;/li&gt;
&lt;li&gt;为模拟ONTAP配置VMware Workstation&lt;/li&gt;
&lt;li&gt;在VMware Workstation上配置网络适配器，选择bridge网络&lt;/li&gt;
&lt;li&gt;开启模拟ONTAP&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-模拟ontap的配置&#34;&gt;2. 模拟ontap的配置&lt;/h4&gt;
&lt;p&gt;以上ontap启动之后，按下面操作配置，其中以cluster base license、feature licenses for the non-ESX build配置证书、e0c、ip address：192.168.*.20、netmask:255.255.255.0、集群名:cluster1、密码等信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2.1 Press Ctrl-C for Boot 菜单消息显示时，按 Ctrl-C&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;md1.uzip: 39168 x 16384 blocks
md2.uzip: 5760 x 16384 blocks
*******************************
* *
* Press Ctrl-C for Boot Menu. *
* *
*******************************
^C
Boot Menu will be available.
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;2.2 选择4配置&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Please choose one of the following:
(1) Normal Boot.
(2) Boot without /etc/rc.
(3) Change password.
(4) Clean configuration and initialize all disks.
(5) Maintenance mode boot.
(6) Update flash from backup config.
(7) Install new software first.
(8) Reboot node.
Selection (1-8)? 4
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;2.3 确认reset 和 确定&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Zero disks, reset config and install a new file system?: y
This will erase all the data on the disks, are you sure?: y
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;2.4 创建集群,填写参数&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Enter the cluster management interface port [e0d]: e0c
Enter the cluster management interface IP address: 192.168.x.20
Enter the cluster management interface netmask: 255.255.255.0
Enter the cluster management interface default gateway: &amp;lt;Enter&amp;gt;
A cluster management interface on port e0c with IP address 192.168.x.
20 has been created.
You can use this address to connect to and manager the cluster.
Do you want to create a new cluster or join an existing cluster?
{create}:
create
Enter the cluster name: cluster1
login: admin
Password: &amp;lt;password you defined&amp;gt;
Enter the cluster base license key:SMKQROWJNQYQSDAAAAAAAAAAAAAA
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;3-登录ontap界面&#34;&gt;3. 登录ontap界面&lt;/h4&gt;
&lt;p&gt;参考链接中的m_SL10537_gui_nas_basic_concepts_v2.1.0文档，这里需要配置的信息为，对接各个存储的协议证书、子网的设置、聚合的创建、svm创建和导出策略的配置。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;https://IP address,以上设置的iP地址，用户名和密码：
&lt;img src=&#34;https://Forest-L.github.io/img/netapp.png&#34; alt=&#34;netapp&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对接各个存储的协议证书
登录平台之后，配置&amp;ndash;》许可证&amp;ndash;》添加对应的证书，显示为绿色的勾就添加正确。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;子网的设置
登录平台，网络&amp;ndash;》子网&amp;ndash;》创建&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;聚合的创建
登录平台，存储&amp;ndash;》聚合和磁盘&amp;ndash;》聚合&amp;ndash;》创建&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;svm的创建，此处需要选择对应的存储协议、 存储中的聚合和权限、管理(LIF)和数据（LIF）等信息。
登录平台，存储&amp;ndash;》SVM&amp;ndash;》创建&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;导出策略的设置，svm创建之后，点击svm设置&amp;ndash;》导出策略，在规则索引下添加客户端规范0.0.0.0/0，协议和权限。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;4-trident安装部署&#34;&gt;4. trident安装部署&lt;/h4&gt;
&lt;p&gt;介质在链接中，包括所需要的镜像和trident安装包和想要的配置文件。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果多台机器情形，需要在每台机器上执行&lt;code&gt;docker load -i trident.tar&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;解压trident安装包，&lt;code&gt;tar -xf $BASE_FOLDER/trident-installer-19.07.0.tar.gz&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;进入trident-installer目录，执行trident安装指令：&lt;code&gt;./tridentctl install -n trident&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;检查是否安装成功&lt;code&gt;kubectl get pod -n trident&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;创建并验证第一个后端,注意backend.json填写正确的ontap参数，
&lt;code&gt;./tridentctl -n trident create backend -f backend.json&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;验证后端是否生成：&lt;code&gt;./tridentctl -n trident get backend&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;创建storage class：&lt;code&gt;kubectl create -f sample-input/storage-class-basic.yaml&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;5-kubesphere安装部署&#34;&gt;5. kubesphere安装部署&lt;/h4&gt;
&lt;p&gt;参考官方部署文档为：https://github.com/kubesphere/ks-installer&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kubesphere基于Velero做集群的迁移</title>
      <link>https://Forest-L.github.io/post/kubesphere-does-cluster-migration-based-on-velero/</link>
      <pubDate>Wed, 13 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/kubesphere-does-cluster-migration-based-on-velero/</guid>
      <description>&lt;p&gt;#Kubesphere基于Velero做集群的迁移
使用Velero 快速完成云原生应用迁移至备份集群中。&lt;/p&gt;
&lt;h3 id=&#34;环境信息&#34;&gt;环境信息&lt;/h3&gt;
&lt;p&gt;集群A（生产）：
master：192.168.11.6、192.168.11.13、192.168.11.16
lb：192.168.11.252
node：192.168.11.22
nfs：192.168.11.14
集群B（容灾）：
master：192.168.11.8、192.168.11.10、192.168.11.17
lb：192.168.11.253
node：192.168.11.18
nfs：192.168.11.14&lt;/p&gt;
&lt;h3 id=&#34;velero安装部署&#34;&gt;Velero安装部署&lt;/h3&gt;
&lt;p&gt;集群A和集群B都需要安装velero，安装过程参考官方文档&lt;a href=&#34;https://velero.io/docs/v1.2.0/contributions/minio/&#34;&gt;velero安装&lt;/a&gt;,大致流程为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、安装velero客户端安装包，A和B集群都需要。
1.1、wget https://github.com/vmware-tanzu/velero/releases/download/v1.0.0/velero-v1.0.0-linux-amd64.tar.gz
1.2、解压安装包，且将velero拷贝至/usr/local/bin目录下。
2、安装velero服务端，A和B集群都需要。
2.1、本地创建密钥文件，vi credentials-velero
[default]
aws_access_key_id = minio
aws_secret_access_key = minio123
2.2、集群B环境，运下载和运行00-minio-deployment.yaml文件，且需要将其中的ClusterIP改成NodePort，添加nodePort: 31860，集群A环境不需要执行这步。
kubectl apply -f examples/minio/00-minio-deployment.yaml
2.3、集群B环境，启动服务端,需要在密钥文件同级目录下执行：
velero install \
    --provider aws \
    --bucket velero \
    --secret-file ./credentials-velero \
    --use-volume-snapshots=false \
    --backup-location-config region=minio,s3ForcePathStyle=&amp;quot;true&amp;quot;,s3Url=http://minio.velero.svc:9000 \
	--plugins velero/velero-plugin-for-aws:v1.0.0
	
2.4、集群A环境，启动服务端，注意：需要在集群A中获取velero的外部curl：
2.4.1、集群A中，kubectl get svc -n velero,获取9000映射的端口，如：9000:31860，根据情况而定
2.4.2、启动指令：
velero install \
    --provider aws \
    --bucket velero \
    --secret-file ./credentials-velero \
    --use-volume-snapshots=false \
    --backup-location-config region=minio,s3ForcePathStyle=&amp;quot;true&amp;quot;,s3Url=http://192.168.11.8:31860 \
	--plugins velero/velero-plugin-for-aws:v1.0.0

&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;集群a数据的备份及集群b恢复&#34;&gt;集群A数据的备份及集群B恢复&lt;/h3&gt;
&lt;p&gt;具体备份指令，定时备份，参考官方文档&lt;a href=&#34;https://velero.io/docs/v1.2.0/contributions/minio/&#34;&gt;备份指令&lt;/a&gt;
在集群A中模拟了带有持久化的有状态和无状态的应用，备份维度以namespace为基准，为test,将pv的模式改成retain形式。
备份指令为：velero backup create test-backup &amp;ndash;include-namespaces test
集群A所有的机器关机且在机器B恢复验证：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@master1 velero-v1.2.0-linux-amd64]# velero restore create --from-backup test-backup
Restore request &amp;quot;test-backup-20191121141336&amp;quot; submitted successfully.
Run `velero restore describe test-backup-20191121141336` or `velero restore logs test-backup-20191121141336` for more details.
[root@master1 velero-v1.2.0-linux-amd64]# kubectl get ns
NAME                           STATUS   AGE
default                        Active   43h
demo                           Active   42h
kube-node-lease                Active   43h
kube-public                    Active   43h
kube-system                    Active   43h
kubesphere-controls-system     Active   43h
kubesphere-monitoring-system   Active   43h
kubesphere-system              Active   43h
openpitrix-system              Active   23h
test                           Active   12s
velero                         Active   24m
[root@master1 velero-v1.2.0-linux-amd64]# kubectl get pod -n test
NAME                             READY   STATUS    RESTARTS   AGE
mysql-v1-0                       1/1     Running   0          22s
tomcattest-v1-554c8875cd-26fz4   1/1     Running   0          22s
tomcattest-v1-554c8875cd-cmm2z   1/1     Running   0          22s
tomcattest-v1-554c8875cd-dc7mr   1/1     Running   0          22s
tomcattest-v1-554c8875cd-fcgn4   1/1     Running   0          22s
tomcattest-v1-554c8875cd-wqb4t   1/1     Running   0          22s
wordpress-v1-65d58448f8-g5bh8    1/1     Running   0          22s
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>k8s1.15安装</title>
      <link>https://Forest-L.github.io/post/k8s1-15-install/</link>
      <pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s1-15-install/</guid>
      <description>&lt;h1 id=&#34;使用kubeadm安装k8s-115版本&#34;&gt;使用kubeadm安装k8s 1.15版本&lt;/h1&gt;
&lt;p&gt;k8s 1.15版本中，kubeadm对HA集群的配置已经达到了beta可用，这一版本更新主要是针对稳定性的持续改善和可扩展性。其中用到的镜像和rpm包在百度云上，链接如下。
&lt;a href=&#34;https://pan.baidu.com/s/1LoKvv86Fs5ilZ-TYQdN35A&#34;&gt;https://pan.baidu.com/s/1LoKvv86Fs5ilZ-TYQdN35A&lt;/a&gt;
cos3&lt;/p&gt;
&lt;h2 id=&#34;1准备&#34;&gt;1.准备&lt;/h2&gt;
&lt;h3 id=&#34;11系统准备&#34;&gt;1.1系统准备&lt;/h3&gt;
&lt;p&gt;需要将主机ip和主机名放在每台机器的&lt;code&gt;vi /etc/hosts&lt;/code&gt;下
&lt;code&gt;192.168.11.21 i-fahx5c7k&lt;/code&gt;
&lt;code&gt;192.168.11.22 i-ouaaujhz&lt;/code&gt;
如果各个主机启用了防火墙，需要开启各个组件所需要的端口，可以查看https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
这里各个节点禁用防火墙
&lt;code&gt;systemctl stop firewalld&lt;/code&gt;
&lt;code&gt;systemctl disable firewalld&lt;/code&gt;
禁用selinux
&lt;code&gt;setenforce 0&lt;/code&gt;
vi /etc/selinux/config
SELINUX=disabled
创建vi /etc/sysctl.d/k8s.conf文件，添加如下内容：
&lt;code&gt;net.bridge.bridge-nf-call-ip6tables = 1&lt;/code&gt;
&lt;code&gt;net.bridge.bridge-nf-call-iptables = 1&lt;/code&gt;
&lt;code&gt;net.ipv4.ip_forward = 1&lt;/code&gt;
执行命令使修改生效
&lt;code&gt;modprobe br_netfilter&lt;/code&gt;
&lt;code&gt;sysctl -p /etc/sysctl.d/k8s.conf&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;12-kube-proxy开启ipvs的前置条件&#34;&gt;1.2 kube-proxy开启ipvs的前置条件&lt;/h3&gt;
&lt;p&gt;在所有的节点上执行如下脚本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt; /etc/sysconfig/modules/ipvs.modules &amp;lt;&amp;lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; bash /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4

各个节点需要安装 ipset ipvsadm
yum install ipset ipvsadm -y
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;13-docker安装&#34;&gt;1.3 docker安装&lt;/h3&gt;
&lt;p&gt;安装docker的yum源,国内寻找清华源
&lt;code&gt;yum install wget -y&lt;/code&gt;
&lt;code&gt;yum install -y yum-utils device-mapper-persistent-data lvm2&lt;/code&gt;
&lt;code&gt;wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo&lt;/code&gt;
&lt;code&gt;sed -i &#39;s+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+&#39; /etc/yum.repos.d/docker-ce.repo&lt;/code&gt;
&lt;code&gt;yum makecache fast&lt;/code&gt;
&lt;code&gt;yum install docker-ce -y&lt;/code&gt;
重启docker：&lt;code&gt;systemctl enable docker;systemctl restart docker&lt;/code&gt;
修改docker cgroup driver为systemd
创建或修改&lt;code&gt;vi /etc/docker/daemon.json&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启docker:&lt;code&gt;systemctl restart docker&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-使用kubeadm部署kubernetes&#34;&gt;2. 使用kubeadm部署kubernetes&lt;/h2&gt;
&lt;h3 id=&#34;21-安装kubeadm和kubelet&#34;&gt;2.1 安装kubeadm和kubelet&lt;/h3&gt;
&lt;p&gt;下面在各节点安装kubeadm和kubelet和kubectl，请在百度云上面下载rpm包，在rmp目录下执行如下指令：
&lt;code&gt;yum install -y cri-tools-1.13.0-0.x86_64.rpm kubernetes-cni-0.7.5-0.x86_64.rpm kubelet-1.15.1-0.x86_64.rpm kubectl-1.15.1-0.x86_64.rpm kubeadm-1.15.1-0.x86_64.rpm &lt;/code&gt;
k8s 1.8开始要求关闭系统的swap,不关闭，kubelet将无法启动，执行指令方法：
&lt;code&gt;swapoff -a&lt;/code&gt;
修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载，使用free -m确认swap已经关闭。 swappiness参数调整，修改&lt;code&gt;vi /etc/sysctl.d/k8s.conf&lt;/code&gt;添加下面一行：
&lt;code&gt;vm.swappiness=0&lt;/code&gt;
执行&lt;code&gt;sysctl -p /etc/sysctl.d/k8s.conf&lt;/code&gt;使修改生效。
因为这里本次测试主机上还运行其他服务，关闭swap可能会对其他服务产生影响，所以这里修改kubelet的配置去掉这个限制。
修改&lt;code&gt;vi /etc/sysconfig/kubelet&lt;/code&gt;，加入：&lt;code&gt;KUBELET_EXTRA_ARGS=--fail-swap-on=false&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;22-使用kubeadm-init初始化集群&#34;&gt;2.2 使用kubeadm init初始化集群&lt;/h3&gt;
&lt;p&gt;在各节点开机启动kubelet服务：&lt;code&gt;systemctl enable kubelet&lt;/code&gt;
使用&lt;code&gt;kubeadm config print init-defaults&lt;/code&gt;可以打印集群初始化默认的使用的配置:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: node1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: v1.14.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从默认的配置中可以看到，可以使用imageRepository定制在集群初始化时拉取k8s所需镜像的地址。advertiseAddress的ip替换本机，基于默认配置定制出本次使用kubeadm初始化集群所需的配置文件且在11.21机器上&lt;code&gt;vi kubeadm.yaml&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.11.21
  bindPort: 6443
nodeRegistration:
  taints:
  - effect: PreferNoSchedule
    key: node-role.kubernetes.io/master
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.15.0
networking:
  podSubnet: 10.244.0.0/16
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;font color=#DC143C &gt;说 明 : &lt;/font&gt;
使用kubeadm默认配置初始化的集群，会在master节点打上node-role.kubernetes.io/master:NoSchedule的污点，阻止master节点接受调度运行工作负载。这里测试环境只有两个节点，所以将这个taint修改为node-role.kubernetes.io/master:PreferNoSchedule。&lt;/p&gt;
&lt;p&gt;在开始初始化集群之前，需要从百度云上面下载tar包镜像下来，tar通过scp分别传到各个节点执行docker load -i解压，
镜像列表:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;k8s.gcr.io/kube-proxy                v1.15.0             d235b23c3570        5 weeks ago         82.4MB
k8s.gcr.io/kube-apiserver            v1.15.0             201c7a840312        5 weeks ago         207MB
k8s.gcr.io/kube-scheduler            v1.15.0             2d3813851e87        5 weeks ago         81.1MB
k8s.gcr.io/kube-controller-manager   v1.15.0             8328bb49b652        5 weeks ago         159MB
gcr.io/kubernetes-helm/tiller        v2.14.1             ac22eb1f780e        7 weeks ago         94.2MB
quay.io/coreos/flannel               v0.11.0-amd64       ff281650a721        6 months ago        52.6MB
k8s.gcr.io/coredns                   1.3.1               eb516548c180        6 months ago        40.3MB
k8s.gcr.io/etcd                      3.3.10              2c4adeb21b4f        8 months ago        258MB
k8s.gcr.io/pause                     3.1                 da86e6ba6ca1        19 months ago       742kB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;接下来使用kubeadm初始化集群，选择node1作为Master Node，在node1上执行下面的命令：&lt;code&gt;kubeadm init --config kubeadm.yaml --ignore-preflight-errors=Swap&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubeadm join 192.168.11.21:6443 --token k6s0kn.nyao1ulwwmlm2org \ --discovery-token-ca-cert-hash sha256:b721d25356668f921c90bf5f554836cb346b827a62b78797d0bcbcba24214725 &lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;其中关键步骤：
* [kubelet-start] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml”
* [certs]生成相关的各种证书
* [kubeconfig]生成相关的kubeconfig文件
* [control-plane]使用/etc/kubernetes/manifests目录中的yaml文件创建apiserver、controller-manager、scheduler的静态pod
* [bootstraptoken]生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到
* 下面的命令是配置常规用户如何使用kubectl访问集群：
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
* 最后给出了将节点加入集群的命令kubeadm join 192.168.99.11:6443 –token 4qcl2f.gtl3h8e5kjltuo0r \ –discovery-token-ca-cert-hash sha256:7ed5404175cc0bf18dbfe53f19d4a35b1e3d40c19b10924275868ebf2a3bbe6e
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;需要在11.21机器上执行：
&lt;code&gt;mkdir -p $HOME/.kube&lt;/code&gt;
&lt;code&gt;sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config&lt;/code&gt;
&lt;code&gt;sudo chown $(id -u):$(id -g) $HOME/.kube/config&lt;/code&gt;
查看集群状态，确认组件都处于healthy状态：
&lt;code&gt;kubectl get cs&lt;/code&gt;
集群初始化如果遇到问题，可以使用下面的命令进行清理：
&lt;code&gt;kubeadm reset&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;23-安装pod-network&#34;&gt;2.3 安装Pod Network&lt;/h3&gt;
&lt;p&gt;接下来安装flannel network add-on：
&lt;code&gt;mkdir -p ~/k8s/&lt;/code&gt;
&lt;code&gt;cd ~/k8s&lt;/code&gt;
&lt;code&gt;curl -O https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml&lt;/code&gt;
&lt;code&gt;kubectl apply -f  kube-flannel.yml&lt;/code&gt;
这里注意kube-flannel.yml这个文件里的flannel的镜像是0.11.0，quay.io/coreos/flannel:v0.11.0-amd64
如果Node有多个网卡的话，参考https://github.com/kubernetes/kubernetes/issues/39701，
目前需要在kube-flannel.yml中使用–iface参数指定集群主机内网网卡的名称，否则可能会出现dns无法解析。需要将kube-flannel.yml下载到本地，flanneld启动参数加上–iface=&lt;iface-name&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        - --iface=eth1
......
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;使用&lt;code&gt;kubectl get pod –-all-namespaces -o wide&lt;/code&gt;确保所有的Pod都处于Running状态。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kube-flannel.yml
[root@i-fahx5c7k k8s]# kubectl get pod --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP              NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5c98db65d4-nbb4w             1/1     Running   0          6m29s   10.244.0.2      i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-5c98db65d4-wtm58             1/1     Running   0          6m29s   10.244.0.3      i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   etcd-i-fahx5c7k                      1/1     Running   0          5m26s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-apiserver-i-fahx5c7k            1/1     Running   0          5m37s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-controller-manager-i-fahx5c7k   1/1     Running   0          5m45s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-flannel-ds-amd64-bqswg          1/1     Running   0          58s     192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-zhzxj                     1/1     Running   0          6m29s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-scheduler-i-fahx5c7k            1/1     Running   0          5m20s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;24-测试集群dns是否可用&#34;&gt;2.4 测试集群DNS是否可用&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl run curl --image=radial/busyboxplus:curl -it&lt;/code&gt;
进入后执行&lt;code&gt;nslookup kubernetes.default&lt;/code&gt;确认解析正常:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[ root@curl-6bf6db5c4f-f8jjn:/ ]$ nslookup kubernetes.default
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;25-向kubernetes集群中添加node节点&#34;&gt;2.5 向Kubernetes集群中添加Node节点&lt;/h3&gt;
&lt;p&gt;在master上查看添加节点指令：&lt;code&gt;kubeadm token create --print-join-command&lt;/code&gt;
下面将11.22这个主机添加到Kubernetes集群中，在11.22机器上执行:
&lt;code&gt;kubeadm join 192.168.11.21:6443 --token k6s0kn.nyao1ulwwmlm2org \ --discovery-token-ca-cert-hash sha256:b721d25356668f921c90bf5f554836cb346b827a62b78797d0bcbcba24214725&lt;/code&gt;
11.22加入集群很是顺利，下面在master节点上执行命令查看集群中的节点：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@i-fahx5c7k k8s]# kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
i-fahx5c7k   Ready    master   13m   v1.15.1
i-ouaaujhz   Ready    &amp;lt;none&amp;gt;   50s   v1.15.1
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;251-如何从集群中移除node&#34;&gt;2.5.1 如何从集群中移除Node&lt;/h4&gt;
&lt;p&gt;如果需要从集群中移除11.22这个Node执行下面的命令：
在master节点上执行：
&lt;code&gt;kubectl drain i-ouaaujhz --delete-local-data --force --ignore-daemonsets&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete node i-ouaaujhz&lt;/code&gt;
在11.22上执行：
&lt;code&gt;kubeadm reset&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;kube-proxy开启ipvs&#34;&gt;kube-proxy开启ipvs&lt;/h3&gt;
&lt;p&gt;修改ConfigMap的kube-system/kube-proxy中的config.conf，mode: “ipvs”
&lt;code&gt;kubectl edit cm kube-proxy -n kube-system&lt;/code&gt;
之后重启各个节点上的kube-proxy pod：
&lt;code&gt;kubectl get pod -n kube-system | grep kube-proxy | awk &#39;{system(&amp;quot;kubectl delete pod &amp;quot;$1&amp;quot; -n kube-system&amp;quot;)}&#39;&lt;/code&gt;
日志查看：&lt;code&gt;kubectl logs kube-proxy-62ntf  -n kube-system&lt;/code&gt;出现ipvs即开启。&lt;/p&gt;
&lt;h2 id=&#34;3kubernetes常用组件部署&#34;&gt;3.Kubernetes常用组件部署&lt;/h2&gt;
&lt;p&gt;使用Helm这个Kubernetes的包管理器，这里也将使用Helm安装Kubernetes的常用组件。&lt;/p&gt;
&lt;h3 id=&#34;31-helm的安装&#34;&gt;3.1 Helm的安装&lt;/h3&gt;
&lt;p&gt;Helm由客户端命helm令行工具和服务端tiller组成，Helm的安装十分简单。 下载helm命令行工具到master节点node1的/usr/local/bin下，这里下载的2.14.1版本：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;curl -O https://get.helm.sh/helm-v2.14.1-linux-amd64.tar.gz&lt;/code&gt;
&lt;code&gt;tar -zxvf helm-v2.14.1-linux-amd64.tar.gz&lt;/code&gt;
&lt;code&gt;cd linux-amd64/&lt;/code&gt;
&lt;code&gt;cp helm /usr/local/bin/&lt;/code&gt;
为了安装服务端tiller，还需要在这台机器上配置好kubectl工具和kubeconfig文件，确保kubectl工具可以在这台机器上访问apiserver且正常使用。 这里的11.21节点已经配置好了kubectl。
因为Kubernetes APIServer开启了RBAC访问控制，所以需要创建tiller使用的service account: tiller并分配合适的角色给它。这里简单起见直接分配cluster-admin这个集群内置的ClusterRole给它。创建&lt;code&gt;vi helm-rbac.yaml&lt;/code&gt;文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f helm-rbac.yaml&lt;/code&gt;
接下来使用helm部署tiller:
&lt;code&gt;helm init --service-account tiller --skip-refresh&lt;/code&gt;
tiller默认被部署在k8s集群中的kube-system这个namespace下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@i-fahx5c7k centosrepo]# kubectl get pod -n kube-system -l app=helm
NAME                             READY   STATUS    RESTARTS   AGE
tiller-deploy-7bf78cdbf7-46bv5   1/1     Running   0          22s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;helm version&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>centos、ubuntu和pip离线依赖包的制作和使用方法</title>
      <link>https://Forest-L.github.io/post/methods-of-making-and-using-centos-ubuntu-pip-offline-dependent-packages/</link>
      <pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/methods-of-making-and-using-centos-ubuntu-pip-offline-dependent-packages/</guid>
      <description>&lt;p&gt;*介绍centos、ubuntu和pip三大核心系统的离线依赖源的制作方法及制作完成之后如何使用&lt;/p&gt;
&lt;h2 id=&#34;1pip安装离线本地包pip版本1923&#34;&gt;1、pip安装离线本地包,pip版本（19.2.3）&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;导出本地已有的依赖包,需要创建一个空的requirements.txt文件。
&lt;code&gt;pip freeze &amp;gt; requirements.txt&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;下载到某个目录下（提前创建目录/packs），指定pip源。
&lt;code&gt;pip download -r requirements.txt -d /packs -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;安装requirements.txt依赖,可以通过pip &amp;ndash;help获取相关指令。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;在线安装：
pip install -r requirements.txt
离线安装：
将/packs目录下的包拷贝到离线环境的机器某目录上（/packs），
pip install --no-index --find-links=&amp;quot;/packs&amp;quot; -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2centos安装离线本地包&#34;&gt;2、centos安装离线本地包&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;制作离线包&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;新的机器上需要在/etc/yum.conf下打开缓存，keepcache=1;
新建目录存放rpm包，如mkdir -p /root/centos-repo;
安装单个工具，yum install -y iotop --downloaddir=/root/centos-repo;
创建本地源，createrepo /root/centos-repo;
制作iso包：mkisofs -r -o /root/centos-7.5-amd64.iso /root/centos-repo/
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3ubuntu安装离线本地包&#34;&gt;3、ubuntu安装离线本地包&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;制作离线包&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;创建存放目录：mkdir -p /home/ubuntu/packs；
安装软件包dpkg-dev:apt-get install dpkg-dev
拷贝dep包至存放目录：sudo cp -r /var/cache/apt/archives/* /home/ubuntu/packs；
进入packs目录下，生成包的依赖信息：dpkg-scanpackages packs /dev/null |gzip &amp;gt; packs/Packages.gz
制作iso包：mkisofs -r -o /home/ubuntu/ubuntu-16.04.5-amd64.iso /home/ubunut/packs
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;使用离线包,如Ubuntu16.04.5&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;创建目录：mkdir  -p /kubeinstaller/apt_repo/16.04.5/iso
挂载至/etc/fstab下在iso包目录下：sh -c &amp;quot;echo &#39;ubuntu-16.04.5-server-amd64.iso /kubeinstaller/apt_repo/16.04.5/iso iso9660 loop 0  0&#39; &amp;gt;&amp;gt; /etc/fstab&amp;quot;
备份之前源：mv -f /etc/apt/sources.list /etc/apt/sources.list-bak
添加新的源：sh -c &amp;quot;echo &#39;deb [trusted=yes]  file:///kubeinstaller/apt_repo/16.04.5/iso/  /&#39; &amp;gt; /etc/apt/sources.list&amp;quot;
挂载生效：mount -a

# clean the process using apt or dpkg
    apt_process=`ps -aux | grep -E &#39;apt|dpkg&#39; | grep -v &#39;grep&#39; | awk &#39;{print $2}&#39;`
    for process in ${apt_process}
    do
        kill -9 $process
    done
    # remove the apt lock
    sudo rm -f /var/lib/apt/lists/lock
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;参考文献&#34;&gt;参考文献&lt;/h4&gt;
&lt;p&gt;pip: &lt;a href=&#34;https://www.cnblogs.com/zengchunyun/p/9344664.html&#34;&gt;https://www.cnblogs.com/zengchunyun/p/9344664.html&lt;/a&gt;
centos: &lt;a href=&#34;https://blog.csdn.net/hao_rh/article/details/73275071&#34;&gt;https://blog.csdn.net/hao_rh/article/details/73275071&lt;/a&gt;
ubuntu: &lt;a href=&#34;https://www.cnblogs.com/gzxbkk/p/7809296.html&#34;&gt;https://www.cnblogs.com/gzxbkk/p/7809296.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>k8s之etcd数据的备份与恢复</title>
      <link>https://Forest-L.github.io/post/backup-and-restore-etcd-data-of-k8s/</link>
      <pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/backup-and-restore-etcd-data-of-k8s/</guid>
      <description>&lt;h1 id=&#34;k8s之etcd备份与恢复&#34;&gt;k8s之etcd备份与恢复&lt;/h1&gt;
&lt;p&gt;etcd是一款开源的分布式一致性键值存储。目前有版本为V3以上，但是它的API又有v2和v3之分，以至于操作指令也不一样。
查看etcd版本&lt;code&gt;etcdctl --version&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;说 明 : &lt;/font&gt;
若使用 v3 备份数据时存在 v2 的数据则不影响恢复&lt;/p&gt;
&lt;p&gt;若使用 v2 备份数据时存在 v3 的数据则恢复失败&lt;/p&gt;
&lt;h3 id=&#34;1对于api2备份与恢复方法&#34;&gt;1、对于API2备份与恢复方法&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;snap: 存放快照数据,etcd防止WAL文件过多而设置的快照，存储etcd数据状态。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;wal: 存放预写式日志,最大的作用是记录了整个数据变化的全部历程。在etcd中，所有数据的修改在提交前，都要先写入到WAL中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;备份指令：
&lt;code&gt;etcdctl backup --data-dir /home/etcd/ --backup-dir /home/etcd_backup&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;恢复指令：
&lt;code&gt;etcd -data-dir=/home/etcd_backup/ -force-new-cluster&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;恢复时会覆盖 snapshot 的元数据(member ID 和 cluster ID)，所以需要启动一个新的集群。&lt;/p&gt;
&lt;h3 id=&#34;2对于api3备份与恢复方法&#34;&gt;2、对于API3备份与恢复方法&lt;/h3&gt;
&lt;p&gt;在使用 API 3 时需要使用环境变量 ETCDCTL_API 明确指定。
&lt;code&gt;export ETCDCTL_API=3&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;21备份数据&#34;&gt;2.1备份数据&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;etcdctl --endpoints localhost:2379 \
   --cert=/etc/ssl/etcd/ssl/node-master.pem \
   --key=/etc/ssl/etcd/ssl/node-master-key.pem \
   --cacert=/etc/ssl/etcd/ssl/ca.pem \snapshot save \
   /var/backups/kube_etcd/snapshot.db
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;font color=#DC143C &gt;说 明 : &lt;/font&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;/var/backups/kube_etcd这个目录是根宿主机的/var/lib/etcd目录相映射的，所以备份在这个目录在对应的宿主机上也是能看见的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这些证书对应文件可以直接在etcd容器内通过ps aux|more看见
其中–cert-file对应–cert，–key对应–key-file –cacert对应–trusted-ca-file&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;22恢复数据&#34;&gt;2.2恢复数据&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;停止三台master节点的kube-apiserver，指令为：
&lt;code&gt;mv /etc/kubernetes/manifests/kube-apiserver.yaml /etc/kubernetes/kube-apiserver.yaml&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在三个master节点停止 etcd 服务,指令为：
&lt;code&gt;systemctl stop etcd&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在三个master节点转移并备份当前 etcd 集群数据,指令为：
&lt;code&gt;mv /var/lib/etcd /var/lib/etcd.bak&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将最新的etcd备份数据恢复至三个master节点，其中master_ip为不同master主机的IP
指令为：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;export ETCDCTL_API=3 和
etcdctl snapshot restore /var/backups/kube_etcd/etcd-******/snapshot.db \
   --endpoints=master_ip:2379 \
   --cert=/etc/ssl/etcd/ssl/node-master.pem \
   --key=/etc/ssl/etcd/ssl/node-master-key.pem \
   --cacert=/etc/ssl/etcd/ssl/ca.pem \
   --data-dir=/var/lib/etcd
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;执行 etcd 恢复命令,指令为：
&lt;code&gt;systemctl restart etcd&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;重启 kube-apiserver,指令为：
&lt;code&gt;mv /etc/kubernetes/kube-apiserver.yaml /etc/kubernetes/manifests/kube-apiserver.yaml &lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;检查是否正常,指令为：
&lt;code&gt;kubectl get pod  --all-namespaces&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;检查etcd集群状态及成员指令为：
&lt;code&gt;etcdctl --endpoints=https://192.168.0.91:2379 --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-ks-allinone.pem --key=/etc/ssl/etcd/ssl/node-ks-allinone-key.pem member list&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>K8s1.16在centos安装</title>
      <link>https://Forest-L.github.io/post/k8s1.16-installed-on-centos-system/</link>
      <pubDate>Sun, 22 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s1.16-installed-on-centos-system/</guid>
      <description>&lt;h1 id=&#34;centos系统k8s-116版本安装&#34;&gt;centos系统k8s-1.16版本安装&lt;/h1&gt;
&lt;p&gt;k8s1.16版本相对之前版本变化不小，亮点和升级参看&lt;a href=&#34;http://k8smeetup.com/article/N1lqGc0i8v&#34;&gt;v1.16说明&lt;/a&gt;。相关联的镜像和v1.16二进制包上传至百度云上，链接如下&lt;a href=&#34;https://pan.baidu.com/s/19khl0Hn5ZnZ8TvbO5HZVww&#34;&gt;k8s1.16介质，ftq5&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;1准备&#34;&gt;1.准备&lt;/h2&gt;
&lt;h3 id=&#34;11系统准备&#34;&gt;1.1系统准备&lt;/h3&gt;
&lt;p&gt;需要将主机ip和主机名放在每台机器的&lt;code&gt;vi /etc/hosts&lt;/code&gt;下&lt;/p&gt;
&lt;p&gt;&lt;code&gt;192.168.11.21 i-fahx5c7k&lt;/code&gt;
&lt;code&gt;192.168.11.22 i-ouaaujhz&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;如果各个主机启用了防火墙，需要开启各个组件所需要的端口，可以查看https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
这里各个节点禁用防火墙&lt;/p&gt;
&lt;p&gt;&lt;code&gt;systemctl stop firewalld&lt;/code&gt;
&lt;code&gt;systemctl disable firewalld&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;禁用selinux&lt;/p&gt;
&lt;p&gt;&lt;code&gt;setenforce 0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;vi /etc/selinux/config
SELINUX=disabled
创建vi /etc/sysctl.d/k8s.conf文件，添加如下内容：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;net.bridge.bridge-nf-call-ip6tables = 1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;net.bridge.bridge-nf-call-iptables = 1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;net.ipv4.ip_forward = 1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;执行命令使修改生效&lt;/p&gt;
&lt;p&gt;&lt;code&gt;modprobe br_netfilter&lt;/code&gt;
&lt;code&gt;sysctl -p /etc/sysctl.d/k8s.conf&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;12-kube-proxy开启ipvs的前置条件&#34;&gt;1.2 kube-proxy开启ipvs的前置条件&lt;/h3&gt;
&lt;p&gt;在所有的节点上执行如下脚本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt; /etc/sysconfig/modules/ipvs.modules &amp;lt;&amp;lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; bash /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4

各个节点需要安装 ipset ipvsadm
yum install ipset ipvsadm -y
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;13-docker安装&#34;&gt;1.3 docker安装&lt;/h3&gt;
&lt;p&gt;安装docker的yum源,国内寻找清华源&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget -y
yum install -y yum-utils device-mapper-persistent-data lvm2
wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo
sed -i &#39;s+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+&#39; /etc/yum.repos.d/docker-ce.repo
yum makecache fast
yum install docker-ce -y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启docker：systemctl enable docker;systemctl restart docker
修改docker cgroup driver为systemd
创建或修改&lt;code&gt;vi /etc/docker/daemon.json&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启docker:&lt;code&gt;systemctl restart docker&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-使用kubeadm部署kubernetes&#34;&gt;2. 使用kubeadm部署kubernetes&lt;/h2&gt;
&lt;h3 id=&#34;21-安装kubeadm和kubelet&#34;&gt;2.1 安装kubeadm和kubelet&lt;/h3&gt;
&lt;p&gt;下面在各节点安装kubeadm和kubelet和kubectl，请在百度云上面下载rpm包，在k8s116目录下执行如下指令：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;yum install -y cri-tools-1.13.0-0.x86_64.rpm cni-0.7.5-0.x86_64.rpm kubelet-1.16.0-0.x86_64.rpm kubectl-1.16.0-0.x86_64.rpm kubeadm-1.16.0-0.x86_64.rpm &lt;/code&gt;&lt;/p&gt;
&lt;p&gt;k8s 1.8开始要求关闭系统的swap,不关闭，kubelet将无法启动，执行指令方法：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;swapoff -a&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载，使用free -m确认swap已经关闭。 swappiness参数调整，修改&lt;code&gt;vi /etc/sysctl.d/k8s.conf&lt;/code&gt;添加下面一行：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;vm.swappiness=0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;执行&lt;code&gt;sysctl -p /etc/sysctl.d/k8s.conf&lt;/code&gt;使修改生效。
因为这里本次测试主机上还运行其他服务，关闭swap可能会对其他服务产生影响，所以这里修改kubelet的配置去掉这个限制。
修改&lt;code&gt;vi /etc/sysconfig/kubelet&lt;/code&gt;，加入：&lt;code&gt;KUBELET_EXTRA_ARGS=--fail-swap-on=false&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;22-使用kubeadm-init初始化集群&#34;&gt;2.2 使用kubeadm init初始化集群&lt;/h3&gt;
&lt;p&gt;在各节点开机启动kubelet服务：&lt;code&gt;systemctl enable kubelet&lt;/code&gt;
使用&lt;code&gt;kubeadm config print init-defaults&lt;/code&gt;可以打印集群初始化默认的使用的配置:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: node1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: v1.14.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从默认的配置中可以看到，可以使用imageRepository定制在集群初始化时拉取k8s所需镜像的地址。advertiseAddress的ip替换本机，基于默认配置定制出本次使用kubeadm初始化集群所需的配置文件且在11.21机器上&lt;code&gt;vi kubeadm.yaml&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.11.21
  bindPort: 6443
nodeRegistration:
  taints:
  - effect: PreferNoSchedule
    key: node-role.kubernetes.io/master
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.16.0
networking:
  podSubnet: 10.244.0.0/16
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;font color=#DC143C &gt;说 明 : &lt;/font&gt;
使用kubeadm默认配置初始化的集群，会在master节点打上node-role.kubernetes.io/master:NoSchedule的污点，阻止master节点接受调度运行工作负载。这里测试环境只有两个节点，所以将这个taint修改为node-role.kubernetes.io/master:PreferNoSchedule。&lt;/p&gt;
&lt;p&gt;在开始初始化集群之前，kubeadm config images pull查看需要哪些镜像,需要从百度云上面下载tar包镜像下来，tar通过scp分别传到各个节点执行docker load -i解压，
镜像列表:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;k8s.gcr.io/kube-apiserver            v1.16.0             b305571ca60a        42 hours ago        217MB
k8s.gcr.io/kube-proxy                v1.16.0             c21b0c7400f9        42 hours ago        86.1MB
k8s.gcr.io/kube-controller-manager   v1.16.0             06a629a7e51c        42 hours ago        163MB
k8s.gcr.io/kube-scheduler            v1.16.0             301ddc62b80b        42 hours ago        87.3MB
k8s.gcr.io/etcd                      3.3.15-0            b2756210eeab        2 weeks ago         247MB
k8s.gcr.io/coredns                   1.6.2               bf261d157914        5 weeks ago         44.1MB
gcr.io/kubernetes-helm/tiller        v2.14.1             ac22eb1f780e        3 months ago        94.2MB
quay.io/coreos/flannel               v0.11.0-amd64       ff281650a721        7 months ago        52.6MB
k8s.gcr.io/pause                     3.1                 da86e6ba6ca1        21 months ago       742kB
radial/busyboxplus                   curl                71fa7369f437        5 years ago         4.23MB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;接下来使用kubeadm初始化集群，选择node1作为Master Node，在node1上执行下面的命令：
&lt;code&gt;kubeadm init --config kubeadm.yaml --ignore-preflight-errors=Swap&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubeadm join 192.168.11.21:6443 --token k6s0kn.nyao1ulwwmlm2org \ --discovery-token-ca-cert-hash sha256:b721d25356668f921c90bf5f554836cb346b827a62b78797d0bcbcba24214725 &lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;其中关键步骤：
* [kubelet-start] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml”
* [certs]生成相关的各种证书
* [kubeconfig]生成相关的kubeconfig文件
* [control-plane]使用/etc/kubernetes/manifests目录中的yaml文件创建apiserver、controller-manager、scheduler的静态pod
* [bootstraptoken]生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到
* 下面的命令是配置常规用户如何使用kubectl访问集群：
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
* 最后给出了将节点加入集群的命令kubeadm join 192.168.99.11:6443 –token 4qcl2f.gtl3h8e5kjltuo0r \ –discovery-token-ca-cert-hash sha256:7ed5404175cc0bf18dbfe53f19d4a35b1e3d40c19b10924275868ebf2a3bbe6e
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;需要在11.21机器上执行：
&lt;code&gt;mkdir -p $HOME/.kube&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo chown $(id -u):$(id -g) $HOME/.kube/config&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;查看集群状态，确认组件都处于healthy状态：
&lt;code&gt;kubectl get cs&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;集群初始化如果遇到问题，可以使用下面的命令进行清理：
&lt;code&gt;kubeadm reset&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;23-安装pod-network&#34;&gt;2.3 安装Pod Network&lt;/h3&gt;
&lt;p&gt;接下来安装flannel network add-on：
&lt;code&gt;mkdir -p ~/k8s/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cd ~/k8s&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;curl -O https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl apply -f  kube-flannel.yml&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这里注意kube-flannel.yml这个文件里的flannel的镜像是0.11.0，quay.io/coreos/flannel:v0.11.0-amd64
注意需要在vi /var/lib/kubelet/kubeadm-flags.env文件配置中去掉&amp;ndash;network-plugin=cni,然后重启kubelet,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl restart kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果Node有多个网卡的话，参考https://github.com/kubernetes/kubernetes/issues/39701，
目前需要在kube-flannel.yml中使用–iface参数指定集群主机内网网卡的名称，否则可能会出现dns无法解析。需要将kube-flannel.yml下载到本地，flanneld启动参数加上–iface=&lt;iface-name&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        - --iface=eth1
......
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;使用&lt;code&gt;kubectl get pods –-all-namespaces -o wide&lt;/code&gt;确保所有的Pod都处于Running状态。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kube-flannel.yml
[root@i-fahx5c7k k8s]# kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP              NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5c98db65d4-nbb4w             1/1     Running   0          6m29s   10.244.0.2      i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-5c98db65d4-wtm58             1/1     Running   0          6m29s   10.244.0.3      i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   etcd-i-fahx5c7k                      1/1     Running   0          5m26s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-apiserver-i-fahx5c7k            1/1     Running   0          5m37s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-controller-manager-i-fahx5c7k   1/1     Running   0          5m45s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-flannel-ds-amd64-bqswg          1/1     Running   0          58s     192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-zhzxj                     1/1     Running   0          6m29s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-scheduler-i-fahx5c7k            1/1     Running   0          5m20s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;24-测试集群dns是否可用&#34;&gt;2.4 测试集群DNS是否可用&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl run curl --image=radial/busyboxplus:curl -it&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;进入后执行&lt;code&gt;nslookup kubernetes.default&lt;/code&gt;确认解析正常:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[ root@curl-6bf6db5c4f-f8jjn:/ ]$ nslookup kubernetes.default
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;25-向kubernetes集群中添加node节点&#34;&gt;2.5 向Kubernetes集群中添加Node节点&lt;/h3&gt;
&lt;p&gt;下面将11.22这个主机添加到Kubernetes集群中，在11.22机器上执行:
&lt;code&gt;kubeadm join 192.168.11.21:6443 --token k6s0kn.nyao1ulwwmlm2org \ --discovery-token-ca-cert-hash sha256:b721d25356668f921c90bf5f554836cb346b827a62b78797d0bcbcba24214725&lt;/code&gt;
11.22加入集群很是顺利，下面在master节点上执行命令查看集群中的节点：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@i-fahx5c7k k8s]# kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
i-fahx5c7k   Ready    master   13m   v1.15.1
i-ouaaujhz   Ready    &amp;lt;none&amp;gt;   50s   v1.15.1
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;251-如何从集群中移除node&#34;&gt;2.5.1 如何从集群中移除Node&lt;/h4&gt;
&lt;p&gt;如果需要从集群中移除11.22这个Node执行下面的命令：
在master节点上执行：
&lt;code&gt;kubectl drain i-ouaaujhz --delete-local-data --force --ignore-daemonsets&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete node i-ouaaujhz&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;在11.22上执行：
&lt;code&gt;kubeadm reset&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;kube-proxy开启ipvs&#34;&gt;kube-proxy开启ipvs&lt;/h3&gt;
&lt;p&gt;修改ConfigMap的kube-system/kube-proxy中的config.conf，mode: “ipvs”&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl edit cm kube-proxy -n kube-system&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;之后重启各个节点上的kube-proxy pod：
&lt;code&gt;kubectl get pod -n kube-system | grep kube-proxy | awk &#39;{system(&amp;quot;kubectl delete pod &amp;quot;$1&amp;quot; -n kube-system&amp;quot;)}&#39;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;日志查看：&lt;code&gt;kubectl logs kube-proxy-62ntf  -n kube-system&lt;/code&gt;出现ipvs即开启。&lt;/p&gt;
&lt;h2 id=&#34;3kubernetes常用组件部署&#34;&gt;3.Kubernetes常用组件部署&lt;/h2&gt;
&lt;p&gt;使用Helm这个Kubernetes的包管理器，这里也将使用Helm安装Kubernetes的常用组件。&lt;/p&gt;
&lt;h3 id=&#34;31-helm的安装&#34;&gt;3.1 Helm的安装&lt;/h3&gt;
&lt;p&gt;Helm由客户端命helm令行工具和服务端tiller组成，Helm的安装十分简单。 下载helm命令行工具到master节点node1的/usr/local/bin下，这里下载的2.14.1版本：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;curl -O https://get.helm.sh/helm-v2.14.1-linux-amd64.tar.gz&lt;/code&gt;
&lt;code&gt;tar -zxvf helm-v2.14.1-linux-amd64.tar.gz&lt;/code&gt;
&lt;code&gt;cd linux-amd64/&lt;/code&gt;
&lt;code&gt;cp helm /usr/local/bin/&lt;/code&gt;
为了安装服务端tiller，还需要在这台机器上配置好kubectl工具和kubeconfig文件，确保kubectl工具可以在这台机器上访问apiserver且正常使用。 这里的11.21节点已经配置好了kubectl。
&lt;code&gt;helm init --output yaml &amp;gt; tiller.yaml&lt;/code&gt;
更新 tiller.yaml 两处：apiVersion 版本;增加选择器&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
...
spec:
  replicas: 1
  strategy: {}
  selector:
    matchLabels:
      app: helm
      name: tiller
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建：&lt;code&gt;kubectl create -f tiller.yaml&lt;/code&gt;
因为Kubernetes APIServer开启了RBAC访问控制，所以需要创建tiller使用的service account: tiller并分配合适的角色给它。这里简单起见直接分配cluster-admin这个集群内置的ClusterRole给它。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create serviceaccount --namespace kube-system tiller
kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
kubectl patch deploy --namespace kube-system tiller-deploy -p &#39;{&amp;quot;spec&amp;quot;:{&amp;quot;template&amp;quot;:{&amp;quot;spec&amp;quot;:{&amp;quot;serviceAccount&amp;quot;:&amp;quot;tiller&amp;quot;}}}}&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;检查helm是否安装成功&lt;code&gt;helm list&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;安装k8s116脚本的安装&#34;&gt;安装k8s1.16脚本的安装&lt;/h3&gt;
&lt;p&gt;解压包，然后执行脚本install-k8s.sh
&lt;code&gt;tar -xzvf k8s116.tar.gz&lt;/code&gt;
&lt;code&gt;./install-k8s.sh&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;参考文档&#34;&gt;参考文档&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/zh/docs/&#34;&gt;kubeadm安装&lt;/a&gt;
&lt;a href=&#34;https://kubernetes.io/zh/docs/setup/independent/create-cluster-kubeadm/&#34;&gt;kubeadm创建集群&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>k8s1.15.3在ubuntu系统部署</title>
      <link>https://Forest-L.github.io/post/k8s1-15-3-install-on-the-ubuntu/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s1-15-3-install-on-the-ubuntu/</guid>
      <description>&lt;h1 id=&#34;k8s1153在ubuntu系统部署&#34;&gt;k8s1.15.3在ubuntu系统部署&lt;/h1&gt;
&lt;p&gt;国内环境下，k8s1.15.3在ubuntu系统部署，相关的镜像以及docker的deb包和k8s核心组件的deb包在以下百度链接下。
&lt;a href=&#34;https://pan.baidu.com/s/1FqfkBiRfa03xaKbmKnqI2w&#34;&gt;k8s介质&lt;/a&gt;
提取码：05ef&lt;/p&gt;
&lt;h4 id=&#34;配置&#34;&gt;配置&lt;/h4&gt;
&lt;p&gt;2核4G
k8s：v1.15.3
ubuntu:18.04&lt;/p&gt;
&lt;h3 id=&#34;1-前期准备&#34;&gt;1. 前期准备&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;关闭ufw防火墙,Ubuntu默认未启用,无需设置。
&lt;code&gt;sudo ufw disable&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;禁用SELINUX （ubuntu19.04默认不安装）
&lt;code&gt;sudo setenforce 0&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;开启数据包转发,修改/etc/sysctl.conf，开启ipv4转发
&lt;code&gt;net.ipv4.ip_forward=1 注释取消&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;防火墙修改FORWARD链默认策略
&lt;code&gt;sudo iptables -P FORWARD ACCEPT&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;禁用swap
&lt;code&gt;sudo swapoff -a&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置iptables参数，使得流经网桥的流量也经过iptables/netfilter防火墙&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sudo tee /etc/sysctl.d/k8s.conf &amp;lt;&amp;lt;-&#39;EOF&#39;
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sudo sysctl --system
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;2docker安装&#34;&gt;2.docker安装&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;安装deb包,通过dpkg指令&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;dpkg -i containerd.io_1.2.5-1_amd64.deb &amp;amp;&amp;amp; \
dpkg -i docker-ce-cli_18.09.5~3-0~ubuntu-bionic_amd64.deb &amp;amp;&amp;amp; \
dpkg -i docker-ce_18.09.5~3-0~ubuntu-bionic_amd64.deb
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;docker使用加速器（阿里云加速器）&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;tee /etc/docker/daemon.json &amp;lt;&amp;lt;- &#39;EOF&#39;
{
&amp;quot;registry-mirrors&amp;quot;: [&amp;quot;https://5xcgs6ii.mirror.aliyuncs.com&amp;quot;]
}
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;设置docker开机自启动
&lt;code&gt;sudo systemctl enable docker &amp;amp;&amp;amp; sudo systemctl start docker&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3安装kubeadmkubeletkubectl&#34;&gt;3.安装kubeadm、kubelet、kubectl&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;通过dpkg -i 来安装k8s核心组件，指令如下&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;dpkg -i cri-tools_1.13.0-00_amd64.deb  kubernetes-cni_0.7.5-00_amd64.deb socat_1.7.3.2-2ubuntu2_amd64.deb conntrack_1%3a1.4.4+snapshot20161117-6ubuntu2_amd64.deb kubelet_1.15.3-00_amd64.deb  kubectl_1.15.3-00_amd64.deb kubeadm_1.15.3-00_amd64.deb
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;设置开机自启动
&lt;code&gt;sudo systemctl enable kubelet &amp;amp;&amp;amp; sudo systemctl start kubelet&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4kubeadm-init初始化集群&#34;&gt;4.kubeadm init初始化集群&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;先要将需要的镜像解压
&lt;code&gt;docker load -i k8s1153.tar&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;查看Kubernetes需要哪些镜像
&lt;code&gt;kubeadm config images list --kubernetes-version=v1.15.3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;注意apiserver-advertise-address要换成本机的IP&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sudo kubeadm init --apiserver-advertise-address=192.168.11.21 --pod-network-cidr=172.16.0.0/16 --service-cidr=10.233.0.0/16 --kubernetes-version=v1.15.3
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;创建kubectl使用的kubeconfig文件
&lt;code&gt;mkdir -p $HOME/.kube&lt;/code&gt;
&lt;code&gt;sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config&lt;/code&gt;
&lt;code&gt;sudo chown $(id -u):$(id -g) $HOME/.kube/config&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建flannel的pod，命令如下
以下两个文件在百度下链接下。
&lt;code&gt;kubectl create -f kube-flannel.yml&lt;/code&gt;
&lt;code&gt;kubectl apply -f weave-net.yml&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;5检查集群及重新添加节点&#34;&gt;5.检查集群及重新添加节点&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;检查node是否ready
&lt;code&gt;kubectl get nodes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;检查pod是否running
&lt;code&gt;kubectl get pod --all-namespaces -o wide&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;添加节点，如果忘记token了，可以在master上面执行如下指令获取
&lt;code&gt;kubeadm token list&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;添加节点，需要提前在新的机器上安装kubelet等服务及需要把相关的镜像拷贝过去解压。最后通过如下指令添加：
&lt;code&gt;kubeadm join –token=4fccd2.b0e0f8918bd95d3e 192.168.11.21:6443&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;参考文档&#34;&gt;参考文档&lt;/h5&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/qq_42346414/article/details/89949380&#34;&gt;参考&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>docker部署mysql5.7</title>
      <link>https://Forest-L.github.io/post/docker-deploy-mysql5-7/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/docker-deploy-mysql5-7/</guid>
      <description>&lt;h1 id=&#34;docker部署mysql57&#34;&gt;docker部署mysql5.7&lt;/h1&gt;
&lt;p&gt;越来越多服务容器化，下面以mysql5.7版本为例容器化部署。按三种方式：最简单、配置文件映射和数据映射来展开。&lt;/p&gt;
&lt;h2 id=&#34;准备条件&#34;&gt;准备条件&lt;/h2&gt;
&lt;p&gt;docker官方镜像：mysql:5.7
个人docker账号：lilinlinlin/mysql:5.7&lt;/p&gt;
&lt;h2 id=&#34;1-最简单的部署mysql&#34;&gt;1. 最简单的部署mysql&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=rootroot -d lilinlinlin/mysql:5.7&lt;/code&gt;
Navicat 测试连接 用户名:root 密码:rootroot 端口:3306&lt;/p&gt;
&lt;h2 id=&#34;2-数据映射到本机部署mysql&#34;&gt;2. 数据映射到本机部署mysql&lt;/h2&gt;
&lt;p&gt;先在本机新建一个目录，&lt;code&gt;mkdir -p /var/lib/mysql&lt;/code&gt;
然后run起来,-v前面是宿主机的目录，&amp;ndash;restart always表示重启容器
&lt;code&gt;docker run -p 3306:3306 --restart always -v /var/lib/mysql:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=mysqlroot -d 镜像ID&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3-配置文件映射到本机部署mysql&#34;&gt;3. 配置文件映射到本机部署mysql&lt;/h2&gt;
&lt;p&gt;在本机/etc下新建vi my.cnf配置文件，如果有的话先删除原来的，再加入创建新的&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[mysql]
#设置mysql客户端默认字符集
default-character-set=utf8
socket=/var/lib/mysql/mysql.sock

[mysqld]
#mysql5.7以后的不兼容问题处理
sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES
datadir=/var/lib/mysql
socket=/var/lib/mysql/mysql.sock
# Disabling symbolic-links is recommended to prevent assorted security risks
symbolic-links=0
# Settings user and group are ignored when systemd is used.
# If you need to run mysqld under a different user or group,
# customize your systemd unit file for mariadb according to the
# instructions in http://fedoraproject.org/wiki/Systemd
#允许最大连接数
max_connections=200
#服务端使用的字符集默认为8比特编码的latin1字符集
character-set-server=utf8
#创建新表时将使用的默认存储引擎
default-storage-engine=INNODB
lower_case_table_names=1
max_allowed_packet=16M 
#设置时区
default-time_zone=&#39;+8:00&#39;
[mysqld_safe]
log-error=/var/log/mariadb/mariadb.log
pid-file=/var/run/mariadb/mariadb.pid

#
# include all files from the config directory
#
!includedir /etc/mysql/conf.d/
!includedir /etc/mysql/mysql.conf.d/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行的指令：&amp;ndash;privileged=true 获取临时的selinux的权限。
&lt;code&gt;docker run -p 3306:3306 --privileged=true -v /etc/my.cnf:/etc/mysql/mysql.conf.d/mysqld.cnf -e MYSQL_ROOT_PASSWORD=mysql密码 -d 镜像ID&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;查看容器启动情况&#34;&gt;查看容器启动情况&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;docker ps -a|grep mysql&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>k8s之helm部署及用法</title>
      <link>https://Forest-L.github.io/post/helm-deployment-and-guide/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/helm-deployment-and-guide/</guid>
      <description>&lt;h1 id=&#34;k8s之helm部署及用法&#34;&gt;k8s之helm部署及用法&lt;/h1&gt;
&lt;p&gt;Helm是Kubernetes的一个包管理工具，用来简化Kubernetes应用的部署和管理。可以把Helm比作CentOS的yum工具。所以可以把该包在不同环境下部署起来,前提需要部署k8s环境。&lt;/p&gt;
&lt;h2 id=&#34;1-helm部署&#34;&gt;1. helm部署&lt;/h2&gt;
&lt;p&gt;Helm由两部分组成，客户端helm和服务端tiller。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tiller运行在Kubernetes集群上，管理chart安装的release&lt;/li&gt;
&lt;li&gt;helm是一个命令行工具，可在本地运行，一般运行在CI/CD Server上。一般我们用helm操作&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;11-客户端helm和服务端tiller安装以1921681120为例&#34;&gt;1.1 客户端helm和服务端tiller安装，以192.168.11.20为例&lt;/h3&gt;
&lt;p&gt;下载地址：https://github.com/helm/helm/releases
这里可以下载的是helm v2.14.1，解压缩后将可执行文件helm拷贝到/usr/local/bin下。这样客户端helm就在这台机器上安装完成了。
通过&lt;code&gt;helm version&lt;/code&gt;显示客户端安装好了，但是服务端没有好.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm version
Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.14.1&amp;quot;, GitCommit:&amp;quot;5270352a09c7e8b6e8c9593002a73535276507c0&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}
Error: Get http://localhost:8080/api/v1/namespaces/kube-system/pods?labelSelector=app%3Dhelm%2Cname%3Dtiller: dial tcp [::1]:8080: connect: connection refused
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;为了安装服务端tiller，还需要在这台机器上配置好kubectl工具和kubeconfig文件，确保kubectl工具可以在这台机器上访问apiserver且正常使用。
&lt;code&gt;kubectl get cs&lt;/code&gt;
使用helm在k8s上部署tiller：
&lt;code&gt;helm init --service-account tiller --skip-refresh&lt;/code&gt;
&lt;font color=#DC143C &gt;说明:&lt;/font&gt;
如果网络原因不能访问gcr.io，可以通过helm init –service-account tiller –tiller-image &lt;your-docker-registry&gt;/tiller:2.7.2 –skip-refresh使用私有镜像仓库中的tiller镜像。ps:lilinlinlin/tiller:2.7.2
tiller默认被部署在k8s集群中的kube-system这个namespace下。
&lt;code&gt;kubectl get pod -n kube-system -l app=helm&lt;/code&gt;
再次helm version可以打印客户端和服务端的版本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm version
Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.7.2&amp;quot;, GitCommit:&amp;quot;8
Server: &amp;amp;version.Version{SemVer:&amp;quot;v2.7.2&amp;quot;,
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2-kubernetes-rbac配置&#34;&gt;2. kubernetes RBAC配置&lt;/h2&gt;
&lt;p&gt;因为我们将tiller部署在Kubernetes 1.8上，Kubernetes APIServer开启了RBAC访问控制，所以我们需要创建tiller使用的service account: tiller并分配合适的角色给它。
这里简单起见直接分配cluster-admin这个集群内置的ClusterRole给它。创建&lt;code&gt;vi helm-rbac.yaml&lt;/code&gt;文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f helm-rbac.yaml&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3添加国内helm源&#34;&gt;3.添加国内helm源&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;helm repo add stable http://mirror.azure.cn/kubernetes/charts/&lt;/code&gt;
&lt;code&gt;helm repo add apphub https://apphub.aliyuncs.com&lt;/code&gt;
更新chart repo: &lt;code&gt;helm repo update&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;4-helm的基本使用&#34;&gt;4. helm的基本使用&lt;/h2&gt;
&lt;p&gt;下面我们开始尝试创建一个chart，这个chart用来部署一个简单的服务&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm create hello-test
Creating hello-test

tree hello-test/
hello-test/
├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── ingress.yaml
│   ├── NOTES.txt
│   └── service.yaml
└── values.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;charts目录中是本chart依赖的chart，当前是空的&lt;/li&gt;
&lt;li&gt;Chart.yaml这个yaml文件用于描述Chart的基本信息，如名称版本等&lt;/li&gt;
&lt;li&gt;templates是Kubernetes manifest文件模板目录，模板使用chart配置的值生成Kubernetes manifest文件。&lt;/li&gt;
&lt;li&gt;templates/NOTES.txt 纯文本文件，可在其中填写chart的使用说明&lt;/li&gt;
&lt;li&gt;value.yaml 是chart配置的默认值
在 values.yaml 中，可以看到，默认创建的是一个 Nginx 应用。为了方便外网访问测试，将 values.yaml 中 service 的属性修改为:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;service:
  type: NodePort
  port: 30080
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;41-部署应用&#34;&gt;4.1 部署应用&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;helm install ./hello-test&lt;/code&gt;
但实际上可以这样部署为,&lt;strong&gt;.tgz为chart包，&lt;/strong&gt;.yaml类似与values.yaml把变量文件定义出来。
&lt;code&gt;helm upgrade --install &amp;lt;name&amp;gt; **.tgz **.yaml --namespace &amp;lt;namespace-name&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;42-查看部署应用&#34;&gt;4.2 查看部署应用&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;helm list&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;43-删除部署应用&#34;&gt;4.3 删除部署应用&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;helm delete &amp;lt;name&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;44-打包chart&#34;&gt;4.4 打包chart&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;helm package &amp;lt;name&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考：&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kubernetes.org.cn/3435.html&#34;&gt;https://www.kubernetes.org.cn/3435.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>kubekey源码解读</title>
      <link>https://Forest-L.github.io/post/kubekey-source-code-interpretation/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/kubekey-source-code-interpretation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>kubernetes一致性认证的提交操作指南</title>
      <link>https://Forest-L.github.io/post/kubernetes-compliance-certification-submission-instructions/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/kubernetes-compliance-certification-submission-instructions/</guid>
      <description>&lt;p&gt;软件一致性尤为重要，它可以避免分裂，使众厂商将精力聚焦于共同推动软件发展而不是自成一家。2017年CNCF启动了Kubernetes一致性认证计划，CNCF提供一套测试工具，各厂商按照操作指导进行测试自身的产品，将测试报告上传给CNCF社区，CNCF审核测试报告后，会给符合条件的企业颁发一个证书。
大致流程：&lt;img src=&#34;https://ww1.sinaimg.cn/large/006bbiLEgy1gfsyxpcmmej30mx09zwew.jpg&#34; alt=&#34;操作流程图.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-环境信息&#34;&gt;1. 环境信息&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;亚太机器ubuntu（16.04.6）两台，192.168.0.3/192.168.0.4&lt;/li&gt;
&lt;li&gt;k8s1.18.3（1master+1node）（至少两台机器）&lt;/li&gt;
&lt;li&gt;sonobuoy0.18.3&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2k8s及云平台的部署&#34;&gt;2.k8s及云平台的部署&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;参考官网链接部署&lt;a href=&#34;https://github.com/kubesphere/kubekey&#34;&gt;K8s及云平台的部署&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;K8s版本为1.18.3及KubeSphere版本为v3.0.0，指令如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;curl -O -k https://kubernetes.pek3b.qingstor.com/tools/kubekey/kk
chmod +x kk
./kk create cluster --with-kubernetes v1.18.3 --with-kubesphere
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3-sonobuoy组件部署及运行&#34;&gt;3. sonobuoy组件部署及运行&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;sonobuoy二进制文件下载&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;wget https://github.com/vmware-tanzu/sonobuoy/releases/download/v0.18.3/sonobuoy_0.18.3_linux_amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;解压，&lt;code&gt;tar -xzvf sonobuoy_0.18.3_linux_amd64.tar.gz&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;配置环境变量，sonobuoy执行文件拷贝到/usr/local/bin下 &lt;code&gt;cp sonobuoy /usr/local/bin/&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-sonobuoy执行及相关指令&#34;&gt;4. sonobuoy执行及相关指令&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;在集群中部署一个sonobuoy的pod，&amp;ndash;mode=certified-conformance参数在Kubernetesv1.16(Sonobuoy v0.16)需要添加，指令为：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sonobuoy run --mode=certified-conformance
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;sonobuoy运行状态&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sonobuoy status
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;详细的日志&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sonobuoy logs
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;通过sonobuoy status显示“completed”，可以通过如下指令获取输出结果，需要提交的内容在plugins/e2e/results/global/{e2e.log,junit_01.xml}目录下&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sonobuoy retrieve
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;删除sonobuoy组件&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sonobuoy delete
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;5-提交的pr包含内容&#34;&gt;5. 提交的pr包含内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;5.1 fork k8s-conformance代码到你GitHub账号下，然后git clone到本地。&lt;/li&gt;
&lt;li&gt;5.2 在本地找到对应的k8s版本号，然后在里面建相关的名字即可，比如在v1.18版本下创建KubeSphere目录。&lt;/li&gt;
&lt;li&gt;5.3 自己项目目录下，需要包含以下四个文件，e2e.log、junit_01.xml、PRODUCT.yaml和README.md&lt;/li&gt;
&lt;li&gt;5.4 e2e.log和junit_01.xml两个文件是通过上面四步骤下解压的两个文件。&lt;/li&gt;
&lt;li&gt;5.5 PRODUCT.yaml包含的内容大致为：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vendor: 组织结构
name: 项目名
version: 版本号
website_url: 项目官方浏览页
repo_url: 项目官方镜像仓库地址
documentation_url: 项目官方文档
product_logo_url: 项目log图标
type: 开源/非开源
description: 项目的描述
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;5.6 README.md包含的内容大致为：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;To reproduce:
本身项目的安装方法
sonobuoy项目的安装方法
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;5.7 提交代码到自己GitHub账号下，然后提pr即可。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;6-参考文章&#34;&gt;6. 参考文章&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cncf/k8s-conformance/blob/master/instructions.md&#34;&gt;https://github.com/cncf/k8s-conformance/blob/master/instructions.md&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>kubesphere2-1-HA环境，一个master或者两个master宕机恢复</title>
      <link>https://Forest-L.github.io/post/kubesphere2-1-ha-environment-one-master-or-two-masters-down-to-recover/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/kubesphere2-1-ha-environment-one-master-or-two-masters-down-to-recover/</guid>
      <description>&lt;h1 id=&#34;kubesphere21-ha环境一个master或者两个master宕机恢复&#34;&gt;kubesphere2.1-HA环境，一个master或者两个master宕机恢复&lt;/h1&gt;
&lt;p&gt;kubesphere2.1版本提供了master节点的高可用功能，为生产环境保驾护航。然而很多事情都有意外，当其中一个master或者两个master卡住了，或者重启都不能自动恢复的情况下，那么怎么恢复呢，以下分一个master宕机和两个master宕机的恢复方法。&lt;/p&gt;
&lt;h3 id=&#34;验证环境信息&#34;&gt;验证环境信息&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;os: centos7.5
master1: 192.168.11.6
master2: 192.168.11.8
master3: 192.168.11.13
node1: 192.168.11.14
lb: 192.168.11.253
nfs服务端: 192.168.11.14
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;一个master宕机的模拟及恢复方法&#34;&gt;一个master宕机的模拟及恢复方法&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;正常的环境：nodes都running，etcd服务都正常。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes
NAME      STATUS   ROLES    AGE   VERSION
master1   Ready    master   19m   v1.15.5
master2   Ready    master   16m   v1.15.5
master3   Ready    master   16m   v1.15.5
node1     Ready    worker   14m   v1.15.5

export ETCDCTL_API=3

etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
192.168.11.6:2379, b3487da7c562b17, 3.2.18, 3.8 MB, true, 5, 4434
192.168.11.8:2379, 3d04d74d80b7f07b, 3.2.18, 3.8 MB, false, 5, 4434
192.168.11.13:2379, 3ad323c042cc4c05, 3.2.18, 3.8 MB, false, 5, 4434
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;一个master宕机情况，把master2重置，看nodes和etcd情况。还需在界面创建一些带有存储的pod用例。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes   
NAME      STATUS     ROLES    AGE   VERSION
master1   Ready      master   57m   v1.15.5
master2   NotReady   master   55m   v1.15.5
master3   Ready      master   55m   v1.15.5
node1     Ready      worker   52m   v1.15.5

etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
Failed to get the status of endpoint 192.168.11.8:2379 (context deadline exceeded)
192.168.11.6:2379, b3487da7c562b17, 3.2.18, 11 MB, true, 5, 14953
192.168.11.13:2379, 3ad323c042cc4c05, 3.2.18, 11 MB, false, 5, 14972
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;恢复方法：修改脚本中hosts.ini文件，需要注意顺序
在[all]组里面，master2行整体放在master3下面，[kube-master]和[etcd]组中master2放在master3下面,然后在另一个终端机器上重新执行安装脚本。以下恢复情况，etcd正常，nodes也正常，业务数据存在且业务pod没有中断正常使用。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes
NAME      STATUS     ROLES    AGE   VERSION
master1   Ready      master   83m   v1.15.5
master2   Ready      master   80m   v1.15.5
master3   Ready      master   80m   v1.15.5
node1     Ready      worker   78m   v1.15.5

etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
192.168.11.6:2379, b3487da7c562b17, 3.2.18, 11 MB, true, 11, 20292
192.168.11.8:2379, 3d04d74d80b7f07b, 3.2.18, 11 MB, false, 11, 20297
192.168.11.13:2379, 3ad323c042cc4c05, 3.2.18, 11 MB, false, 11, 20298

docker ps | grep tomcat
6863620b07cf        882487b8be1d                                     &amp;quot;catalina.sh run&amp;quot;        17 minutes ago       Up 17 minutes                           k8s_tomcat-2jl160_tomcat-v1-6c7745494-k64w5_demo_5b406841-df9b-4d5c-b018-998c400a2c3e_1
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;两个master宕机的模拟及恢复方法&#34;&gt;两个master宕机的模拟及恢复方法&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;两个master宕机情况，把master2和master3重置，看nodes和etcd情况，nodes不正常，etcd两个不正常。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes
Unable to connect to the server: EOF
[root@master1 ~]# 
[root@master1 ~]# 
[root@master1 ~]# etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
Failed to get the status of endpoint 192.168.11.8:2379 (context deadline exceeded)
Failed to get the status of endpoint 192.168.11.13:2379 (context deadline exceeded)
192.168.11.6:2379, b3487da7c562b17, 3.2.18, 11 MB, false, 12, 27944
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;恢复方法：
1、同样需要在host.ini文件修改master的顺序，由于我们重置2和3，所有此处不用修改顺序；
2、在已解压的安装介质目录下，进入k8s/roles/kubernetes/preinstall/tasks/main.yml文件，用#注释如下内容，重跑安装脚本，作用说明：在重置的master2和master3机器上安装docker和etcd，但整个集群还需修复。。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;#- import_tasks: 0020-verify-settings.yml
#  when:
#    - not dns_late
#  tags:
#    - asserts
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3、在master2机器上，临时修复master2节点的etcd服务，执行如下指令：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;停止etcd：systemctl stop etcd
备份etcd数据：mv /var/lib/etcd /var/lib/etcd-bak
从master1的/var/backups/kube_etcd/备份目录下拷贝最近时间的snapshot.db至master2机器上
在master2先转为版本3指令令：export ETCDCTL_API=3
在master2恢复指令：etcdctl snapshot restore /root/snapshot.db    --endpoints=192.168.11.8:2379    --cert=/etc/ssl/etcd/ssl/node-master2.pem    --key=/etc/ssl/etcd/ssl/node-master2-key.pem    --cacert=/etc/ssl/etcd/ssl/ca.pem --data-dir=/var/lib/etcd
重启etcd: systemctl restart etcd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4、修改host.ini文件master2和master3顺序，把[all][kube-master][etcd]三个组的master3放在最后面，再次跑安装脚本。其中的host.ini文件为&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[all]
master1 ansible_connection=local  ip=192.168.11.6
master2  ansible_host=192.168.11.8  ip=192.168.11.8  ansible_ssh_pass=
master3  ansible_host=192.168.11.13  ip=192.168.11.13  ansible_ssh_pass=
node1    ansible_host=192.168.11.14  ip=192.168.11.14  ansible_ssh_pass=

[kube-master]
master1
master2
master3

[kube-node]
node1

[etcd]
master1
master2
master3

[k8s-cluster:children]
kube-node
kube-master 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;5、由第四步只是临时修复master2的etcd，并不完全修复，先全部修复作如下处理：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;第四步执行过程中，z在“wait for etcd up”会报错，先ctrl +c 终止脚本；
在master2机器上，停止etcd服务：systemctl stop etcd
在master2机器上，删除etcd数据：rm -rf /var/lib/etcd
再次修改host.ini文件，master2和master3顺序，把[all][kube-master][etcd]三个组的master2放在最后面，再次跑安装脚本即可。
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;验证结果&#34;&gt;验证结果&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;集群中的nodes、etcd和带有存储数据的业务pod情况：&lt;/li&gt;
&lt;li&gt;集群中的nodes、etcd和带有存储数据的业务pod情况,nodes恢复正常，etcd服务也回复正常，带有存储的pod一直运行，集群恢复成功：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[root@master1 ~]# kubectl get nodes
NAME      STATUS   ROLES    AGE     VERSION
master1   Ready    master   4h56m   v1.15.5
master2   Ready    master   4h53m   v1.15.5
master3   Ready    master   4h53m   v1.15.5
node1     Ready    worker   4h51m   v1.15.5
[root@master1 ~]# 
[root@master1 ~]# 
[root@master1 ~]# 
[root@master1 ~]# etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
192.168.11.6:2379, b3487da7c562b17, 3.2.18, 12 MB, false, 1372, 32116
192.168.11.8:2379, 3d04d74d80b7f07b, 3.2.18, 12 MB, true, 1372, 32116
192.168.11.13:2379, 3ad323c042cc4c05, 3.2.18, 12 MB, false, 1372, 32116

docker ps | grep tomcat
6863620b07cf        882487b8be1d                                     &amp;quot;catalina.sh run&amp;quot;        4 hours ago         Up 4 hours                              k8s_tomcat-2jl160_tomcat-v1-6c7745494-k64w5_demo_5b406841-df9b-4d5c-b018-998c400a2c3e_1
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>kubesphere2-1-HA环境，某台master或者master的etcd宕机，新加机器恢复方法</title>
      <link>https://Forest-L.github.io/post/kubesphere2-1-ha-environment-a-master-or-master-etcd-down-a-new-machine-recovery-method/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/kubesphere2-1-ha-environment-a-master-or-master-etcd-down-a-new-machine-recovery-method/</guid>
      <description>&lt;p&gt;kubesphere2.1版本提供了master节点的高可用功能，为生产环境保驾护航。然而在生产环境中，为了业务更正常运行，当以下两种情形发生时，告诉大家怎么恢复。第一种情形是：其中某台master机器的etcd服务不能正常提供服务，而需要在另外一台机器上部署一个etcd服务加入到现etcd集群中；第二种情形是：其中某台master宕机，需要在另外一台机器上部署master，并加入到现master集群中。&lt;/p&gt;
&lt;h3 id=&#34;环境信息&#34;&gt;环境信息&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;os: centos7.5
master1: 192.168.11.6
master2: 192.168.11.16
master3: 192.168.11.13
node1: 192.168.11.14
lb: 192.168.11.253
nfs服务端: 192.168.11.14
新加机器master2: 192.168.11.8
安装介质机器：192.168.11.6
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;一个master宕机在另外一台服务器上恢复方法&#34;&gt;一个master宕机，在另外一台服务器上恢复方法&lt;/h2&gt;
&lt;p&gt;假设为master2（192.168.11.16）宕机了，现以新机器192.168.11.8作为恢复master2的机器。以下操作都在master1机器上执行。
1、在安装介质机器即master1机器上面，进入/etc/ssl/etcd/ssl目录下，将含有master2相关联的文件证书删掉（那个master有问题就出来那个）。
&lt;code&gt;rm -rf admin-master2-key.pem admin-master2.pem member-master2-key.pem member-master2.pem node-master2-key.pem node-master2.pem&lt;/code&gt;
2、将etcd集群中master2的节点移除。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;第一步先转为etcd3版本：export ETCDCTL_API=3
查看etcd集群的成员：
etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem member list
b3487da7c562b17, started, etcd1, https://192.168.11.6:2380, https://192.168.11.6:2379
3ad323c042cc4c05, started, etcd2, https://192.168.11.13:2380, https://192.168.11.13:2379
52a4779150442b41, started, etcd3, https://192.168.11.16:2380, https://192.168.11.16:2379
第三步：移除master2节点，如192.168.11.16
etcdctl member remove 52a4779150442b41 --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem  --key=/etc/ssl/etcd/ssl/node-master1-key.pem
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3、新打开master1机器窗口进入解压包目录下修改conf/hosts.ini,一定要新打开窗口（由于上一步操作使master1机器为etcd3版本指令）。
master2的IP由原来的192.168.11.16改成192.168.11.8；在[all]组里面，master2行整体放在master3下面，[kube-master]和[etcd]组中master2放在master3下面,然后执行安装脚本即可。
4、验证结果：
用&lt;code&gt;kubectl get nodes -o wide&lt;/code&gt;指令看master2IP是否替换；
看etcd集群中是否包含新的master2IP，指令为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、开启etcd3版本：export ETCDCTL_API=3
2、etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;一个master中etcd服务不正常在另外一台服务器上恢复etcd方法&#34;&gt;一个master中etcd服务不正常，在另外一台服务器上恢复etcd方法&lt;/h2&gt;
&lt;p&gt;假设为master2（192.168.11.16）宕机了，现以新机器192.168.11.8作为恢复master2的机器。以下操作都在master1机器上执行。
1、在安装介质机器即master1机器上面，进入/etc/ssl/etcd/ssl目录下，将含有master2相关联的文件证书删掉（那个master有问题就出来那个）。
&lt;code&gt;rm -rf admin-master2-key.pem admin-master2.pem member-master2-key.pem member-master2.pem node-master2-key.pem node-master2.pem&lt;/code&gt;
2、将etcd集群中master2的节点移除。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;第一步先转为etcd3版本：export ETCDCTL_API=3
查看etcd集群的成员：
etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem member list
b3487da7c562b17, started, etcd1, https://192.168.11.6:2380, https://192.168.11.6:2379
3ad323c042cc4c05, started, etcd2, https://192.168.11.13:2380, https://192.168.11.13:2379
52a4779150442b41, started, etcd3, https://192.168.11.16:2380, https://192.168.11.16:2379
第三步：移除master2节点，如192.168.11.16
etcdctl member remove 52a4779150442b41 --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem  --key=/etc/ssl/etcd/ssl/node-master1-key.pem
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3、新打开master1机器窗口进入解压包目录下修改conf/hosts.ini,一定要新打开窗口（由于上一步操作使master1机器为etcd3版本指令）。
master2的IP由原来的192.168.11.16改成192.168.11.8；在[all]组里面，master2行整体放在master3下面，[kube-master]和[etcd]组中master2放在master3下面。
4、进入解压包，scripts目录下，编辑install.sh脚本，用#将如下内容注释掉：
&lt;code&gt;ansible-playbook -i $BASE_FOLDER/../k8s/inventory/my_cluster/hosts.ini $BASE_FOLDER/../kubesphere/kubesphere.yml -b&lt;/code&gt;
5、进入解压包，k8s目录下，编辑cluster.yml文件，用#将以下开头的内容至结尾都注释掉&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- hosts: k8s-cluster
  any_errors_fatal: &amp;quot;{{ any_errors_fatal | default(true) }}&amp;quot;
  roles:
    - { role: kubespray-defaults}
    - { role: kubernetes/node, tags: node }
  environment: &amp;quot;{{ proxy_env }}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;6、重新到脚本目录，执行install.sh脚本即可。
7、验证结果：
看etcd集群中是否包含新的master2IP，指令为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、开启etcd3版本：export ETCDCTL_API=3
2、etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem member list
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>tcpdump抓包实战教程</title>
      <link>https://Forest-L.github.io/post/tcpdump-package-capture-tutorial/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/tcpdump-package-capture-tutorial/</guid>
      <description>&lt;h1 id=&#34;tcpdump抓包实战教程&#34;&gt;tcpdump抓包实战教程&lt;/h1&gt;
&lt;p&gt;做 web 开发，接口对接过程中，分析 http 请求报文数据包格式是否正确，定位问题，省去无用的甩锅过程，再比如抓取 tcp/udp 报文，分析 tcp 连接过程中的三次握手和四次挥手。windows使用wireshark工具，Linux使用的是tcpdump工具，也可以生成.pcap文件在wireshark图形化工具上分析。&lt;/p&gt;
&lt;h3 id=&#34;命令行&#34;&gt;命令行&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;-i 选择网卡接口&lt;/li&gt;
&lt;li&gt;-n： 不解析主机名&lt;/li&gt;
&lt;li&gt;-nn：不解析端口&lt;/li&gt;
&lt;li&gt;port 80： 抓取80端口上面的数据&lt;/li&gt;
&lt;li&gt;tcp： 抓取tcp的包&lt;/li&gt;
&lt;li&gt;udp：抓取udp的包&lt;/li&gt;
&lt;li&gt;-w： 保存成pcap文件&lt;/li&gt;
&lt;li&gt;dst：目的ip&lt;/li&gt;
&lt;li&gt;src：源ip&lt;/li&gt;
&lt;li&gt;-c：&amp;lt;数据包数目&amp;gt;&lt;/li&gt;
&lt;li&gt;-s0表示可按包长显示完整的包&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tcp标志位&#34;&gt;tcp标志位&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SYN，显示为S，同步标志位，用于建立会话连接，同步序列号；&lt;/li&gt;
&lt;li&gt;ACK，显示为.，确认标志位，对已接收的数据包进行确认；&lt;/li&gt;
&lt;li&gt;FIN，显示为F，完成标志位，表示我已经没有数据要发送了，即将关闭连接；&lt;/li&gt;
&lt;li&gt;RESET，显示为R，重置标志位，用于连接复位、拒绝错误和非法的数据包；&lt;/li&gt;
&lt;li&gt;PUSH，显示为P，推送标志位，表示该数据包被对方接收后应立即交给上层应用，而不在缓冲区排队；&lt;/li&gt;
&lt;li&gt;URGENT，显示为U，紧急标志位，表示数据包的紧急指针域有效，用来保证连接不被阻断，并督促中间设备尽&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;nginx连接问题&#34;&gt;nginx连接问题。&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;抓取nginx(端口30880)交互的包：我们知道nginx交互其实是tcp协议，因此使用如下命令
&lt;code&gt;tcpdump -i eth0 tcp and port 30880 -n -nn -C 20 -W 50 -s 0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;tcp建立连接：先在服务的机器上执行如下命令，接着把multinode.ks.dev.chenshaowen.com:30880的url在浏览器访问即可。以下标志位含义可以参考上面描述。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[root@master ~]#tcpdump -i eth0 tcp and port 30880 -n -nn -C 20 -W 50 -s 0 
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes
10:44:36.391603 IP 139.198.254.12.61476 &amp;gt; 192.168.12.2.30880: Flags [S], seq 3950767038, win 64240, options [mss 1360,nop,wscale 8,nop,nop,sackOK], length 0
10:44:36.391864 IP 192.168.12.2.30880 &amp;gt; 139.198.254.12.61476: Flags [S.], seq 1197638836, ack 3950767039, win 29200, options [mss 1460,nop,nop,sackOK,nop,wscale 7], length 0
10:44:36.447960 IP 139.198.254.12.61478 &amp;gt; 192.168.12.2.30880: Flags [.], ack 1, win 515, length 0
10:44:36.448242 IP 139.198.254.12.61476 &amp;gt; 192.168.12.2.30880: Flags [.], ack 1, win 515, length 0
10:44:36.592856 IP 192.168.12.2.30880 &amp;gt; 139.198.254.12.61476: Flags [P.], seq 5250:6642, ack 1511, win 252, length 1392
10:44:36.647585 IP 139.198.254.12.61476 &amp;gt; 192.168.12.2.30880: Flags [.], ack 6642, win 515, length 0
10:44:41.592442 IP 192.168.12.2.30880 &amp;gt; 139.198.254.12.61476: Flags [F.], seq 6642, ack 1511, win 252, length 0
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;简单分析&#34;&gt;简单分析&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;当url一打开，tcpdump就有数据显示。看到的S标志位，建立会话连接；接着看到.标志位，对接受包进行确认；然后看到P标志位，表示数据包被对方接受上交给上层应用；最后看到F标志位，表示完成，没有数据要发送。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;非服务端tcpdump客户端的数据&#34;&gt;非服务端tcpdump客户端的数据&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;一般情况下，我们都是在服务端使用tcpdump工具抓包的；如果需要在非服务端使用tcpdump抓包可以通过网络流量镜像方式，使服务端的流量到目标地址上。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;某服务不正常tcpdump测试结果&#34;&gt;某服务不正常tcpdump测试结果&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;抓取某不正常的服务(端口30881)交互的包：我们知道nginx交互其实是tcp协议，因此使用如下命令
&lt;code&gt;tcpdump -i eth0 tcp and port 30881 -n -nn -C 20 -W 50 -s 0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;tcp建立连接：先在服务的机器上执行如下命令，接着把multinode.ks.dev.chenshaowen.com:30881的url在浏览器访问即可。以下标志位含义可以参考上面描述。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[root@master ~]#tcpdump -i eth0 tcp and port 30881 -n -nn -C 20 -W 50 -s 0 
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes
10:56:00.569543 IP 139.198.254.12.61608 &amp;gt; 192.168.12.2.30881: Flags [S], seq 1702724216, win 64240, options [mss 1360,nop,wscale 8,nop,nop,sackOK], length 0
10:56:00.569723 IP 192.168.12.2.30881 &amp;gt; 139.198.254.12.61608: Flags [R.], seq 0, ack 1702724217, win 0, length 0
10:56:00.571570 IP 139.198.254.12.61609 &amp;gt; 192.168.12.2.30881: Flags [S], seq 97188145, win 64240, options [mss 1360,nop,wscale 8,nop,nop,sackOK], length 0
10:56:00.571637 IP 192.168.12.2.30881 &amp;gt; 139.198.254.12.61609: Flags [R.], seq 0, ack 97188146, win 0, length 0
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;简单分析-1&#34;&gt;简单分析&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;当url一打开，一开始出现S标志位，表示建立会话连接；接着出现R标志位，表示重置，用于连接复位，拒绝错误和非法的数据包；最后有出现S标志位，再次建立会话连接，一直S与R交替出现，说明该服务不正常。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;参考文章&#34;&gt;参考文章&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://dreamgoing.github.io/tcpdump%E5%AE%9E%E6%88%98.html&#34;&gt;https://dreamgoing.github.io/tcpdump%E5%AE%9E%E6%88%98.html&lt;/a&gt;
&lt;a href=&#34;https://klionsec.github.io/2017/01/31/tcpdump-sniffer-pass/#menu&#34;&gt;https://klionsec.github.io/2017/01/31/tcpdump-sniffer-pass/#menu&lt;/a&gt;
&lt;a href=&#34;https://hubinwei.me/2018/07/25/tcpdump%E6%8A%93%E5%8C%85%E7%BB%83%E4%B9%A0/&#34;&gt;https://hubinwei.me/2018/07/25/tcpdump%E6%8A%93%E5%8C%85%E7%BB%83%E4%B9%A0/&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ubuntu快速安装wecenter</title>
      <link>https://Forest-L.github.io/post/ubuntu-quickly-installs-wecenter/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/ubuntu-quickly-installs-wecenter/</guid>
      <description>&lt;h1 id=&#34;ubuntu快速安装wecenter&#34;&gt;ubuntu快速安装wecenter&lt;/h1&gt;
&lt;p&gt;以镜像的形式安装wecenter，简化了nginx和php环境的单独安装。WeCenter（wecenter.com）是一款建立知识社区的开源程序（免费版），专注于企业和行业社区内容的整理、归类、检索和分享，是知识化问答社区的首选软件。后台使用PHP开发，MVC架构，前端使用Bootstrap框架。&lt;/p&gt;
&lt;h2 id=&#34;准备&#34;&gt;准备&lt;/h2&gt;
&lt;p&gt;mysql需要搭建，参考：
&lt;a href=&#34;https://lilinlinlin.github.io/2019/08/13/docker%E9%83%A8%E7%BD%B2mysql5-7/#more&#34;&gt;https://lilinlinlin.github.io/2019/08/13/docker%E9%83%A8%E7%BD%B2mysql5-7/#more&lt;/a&gt;
镜像：wecenter/wecenter:3.3.2&lt;/p&gt;
&lt;h2 id=&#34;wecenter安装&#34;&gt;wecenter安装&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;docker run --name wecenter1 -p 8081:80  -d wecenter/wecenter:3.3.2&lt;/code&gt;
通过外网ip加端口访问,链接为：
&lt;code&gt;eip:8081/install&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;安装成功浏览器打开之后&#34;&gt;安装成功，浏览器打开之后&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/006bbiLEgy1g5ygifdatkj30xv0q8abj.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;点击下一步，进入配置系统，正确填入数据库的主机、账号、密码和数据库的名称（默认wecenter），点开始安装。&lt;/li&gt;
&lt;li&gt;上步成功之后，管理员配置，用户名admin，密码自己定义。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考文档&#34;&gt;参考文档：&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://help.websoft9.com/lamp-guide/installation/wecenter/install.html&#34;&gt;http://help.websoft9.com/lamp-guide/installation/wecenter/install.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>基于keepalived-haproxy部署kubesphere高可用方案</title>
      <link>https://Forest-L.github.io/post/deploy-kubesphere-high-availability-solution-based-on-keepalived-haproxy/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/deploy-kubesphere-high-availability-solution-based-on-keepalived-haproxy/</guid>
      <description>&lt;h1 id=&#34;基于keepalivedhaproxy部署kubesphere高可用方案&#34;&gt;基于keepalived+haproxy部署kubesphere高可用方案&lt;/h1&gt;
&lt;p&gt;通过keepalived + haproxy实现的，其中keepalived提供一个VIP，通过VIP关联所有的Master节点；然后haproxy提供端口转发功能。由于VIP还是存在Master的机器上的，默认配置API Server的端口是6443，所以我们需要将另外一个端口关联到这个VIP上，一般用8443。&lt;/p&gt;
&lt;h4 id=&#34;环境信息&#34;&gt;环境信息：&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;青云机器操作系统：centos7.5
master1:192.168.0.10
master2:192.168.0.11
master3:192.168.0.12
node:192.168.0.6
vip:192.168.0.200
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;提前说明及遇到过坑&#34;&gt;提前说明及遇到过坑&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;keepalived提供VIP时，需提前规划的vip不能ping通。&lt;/li&gt;
&lt;li&gt;腾讯云服务器上不提供由keepalived方式产生VIP，需要提前在云平台界面HAVIP上创建，创建出的vip再在keepalived.conf中配置。（特别注意这点）。&lt;/li&gt;
&lt;li&gt;通过keepalived服务，vip只能在其中的某一个master中看到，如果ip a方式在每个master都看到，说明keepalived有问题。&lt;/li&gt;
&lt;li&gt;keepalived+haproxy正常安装之后，检查node节点可以和vip通信。&lt;/li&gt;
&lt;li&gt;common.yaml配置文件需要填写正确的ip和转发的端口。&lt;/li&gt;
&lt;li&gt;hosts.ini文件master需要添加master1、master2和master3。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;keepalived安装和配置三台master机器都要安装&#34;&gt;keepalived安装和配置，三台master机器都要安装。&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;安装keepalived, &lt;code&gt;yum install -y keepalived&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;/etc/keepalived/keepalived.conf文件下修改配置,需要修改自己场景的vIP,填写正确服务器的网卡名如：eth0。&lt;/li&gt;
&lt;li&gt;其中killall组件，还需要安装yum install psmisc -y。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;global_defs {
    router_id lb-backup
}
vrrp_script check_haproxy {
  script &amp;quot;/usr/bin/killall -0 haproxy&amp;quot;
  interval 2
  weight 2
}
vrrp_instance VI-kube-master {
    state MASTER
    priority 110
    dont_track_primary
    interface eth0
    virtual_router_id 90
    advert_int 3
    virtual_ipaddress {
        192.168.0.200
    }
	track_script {
    check_haproxy
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启keepalived服务及断电自启：&lt;code&gt;systemctl enable keepalived;systemctl restart keepalived&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;haproxy安装和配置三台master机器都要安装&#34;&gt;haproxy安装和配置，三台master机器都要安装。&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;安装haproyx，&lt;code&gt;yum install -y haproxy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;/etc/haproxy/haproxy.cfg 文件下修改配置，server服务端分别为master的IP值。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;global
        log /dev/log    local0
        log /dev/log    local1 notice
        chroot /var/lib/haproxy
        #stats socket /run/haproxy/admin.sock mode 660 level admin
        stats timeout 30s
        user haproxy
        group haproxy
        daemon
        nbproc 1

defaults
        log     global
        timeout connect 5000
        timeout client  50000
        timeout server  50000

listen kube-master
        bind 0.0.0.0:8443
        mode tcp
        option tcplog
        balance roundrobin
        server master1 192.168.0.10:6443  check inter 10000 fall 2 rise 2 weight 1
        server master2 192.168.0.11:6443  check inter 10000 fall 2 rise 2 weight 1
        server master3 192.168.0.12:6443  check inter 10000 fall 2 rise 2 weight 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启haproxy服务及断电自启：&lt;code&gt;systemctl enable haproxy;systemctl restart haproxy&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;hostsini配置和commonyaml配置&#34;&gt;hosts.ini配置和common.yaml配置。&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hosts.ini配置实例如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[all]
master1 ansible_connection=local  ip=192.168.0.10
master2  ansible_host=192.168.0.11  ip=192.168.0.11  ansible_ssh_pass=****
master3  ansible_host=192.168.0.12  ip=192.168.0.12  ansible_ssh_pass=****
node1  ansible_host=192.168.0.6  ip=192.168.0.6  ansible_ssh_pass=****

[kube-master]
master1
master2
master3

[kube-node]
node1


[etcd]
master1
master2
master3

[k8s-cluster:children]
kube-node
kube-master 
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;common.yaml的lb配置，注意此处填写VIP，及转发的端口8443。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# apiserver_loadbalancer_domain_name: &amp;quot;lb.kubesphere.local&amp;quot;
loadbalancer_apiserver:
  address: 192.168.0.200
  port: 8443
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;安装kubesphere及结果&#34;&gt;安装kubesphere及结果&lt;/h3&gt;
&lt;p&gt;kubesphere-all-v2.1.0/scripts目录下，执行./install.sh，选择2+yes即可，然后等待脚本的安装。
node正常的结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes -o wide
NAME      STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION          CONTAINER-RUNTIME
master1   Ready    master   95m   v1.15.5   192.168.0.10   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-862.el7.x86_64   docker://18.9.7
master2   Ready    master   88m   v1.15.5   192.168.0.11   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-862.el7.x86_64   docker://18.9.7
master3   Ready    master   88m   v1.15.5   192.168.0.12   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-862.el7.x86_64   docker://18.9.7
node1     Ready    worker   86m   v1.15.5   192.168.0.6    &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-862.el7.x86_64   docker://18.9.7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;apiserver中vip生效的结果&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;telnet 192.168.0.200 8443
Trying 192.168.0.200...
Connected to 192.168.0.200.
Escape character is &#39;^]&#39;.
^CConnection closed by foreign host.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>排查linux服务器的工具</title>
      <link>https://Forest-L.github.io/post/tools-to-troubleshoot-linux-servers/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/tools-to-troubleshoot-linux-servers/</guid>
      <description>&lt;p&gt;当服务器遇到问题，或者提前查看服务器质量怎么样，一般可以通过以下几点分析：服务器整体情况，CPU使用情况，内存，磁盘，磁盘io，网络io等。&lt;/p&gt;
&lt;h3 id=&#34;不同环境插件安装&#34;&gt;不同环境插件安装&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;centos&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;yum install sysstat -y&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ubuntu&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;apt install sysstat -y&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;1整体分析之top&#34;&gt;1、整体分析之top&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;执行top指令&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# top
top - 10:41:30 up 6 days, 11:23,  1 user,  load average: 0.98, 0.66, 0.57
Tasks: 304 total,   1 running, 200 sleeping,   0 stopped,   2 zombie
%Cpu(s):  3.1 us,  0.7 sy,  0.0 ni, 95.5 id,  0.6 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem : 32937840 total, 25930000 free,  2666144 used,  4341696 buff/cache
KiB Swap:        0 total,        0 free,        0 used. 30586896 avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
28484 root      20   0 1468632 1.280g  68832 S  39.1  4.1 260:25.96 kube-apiserver
25442 root      20   0 10.277g 308160  80692 S   9.3  0.9 166:39.40 etcd
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;第 1 行：系统时间、运行时间、登录终端数、系统负载（三个数值分别为1分钟、5分钟、15分钟内的平均值，数值越小意味着负载越低）。&lt;/li&gt;
&lt;li&gt;第 2 行：进程总数、运行中的进程数、睡眠中的进程数、停止的进程数、僵死的进程数。一般情况下，只要没有僵死的进程，就没啥大问题。&lt;/li&gt;
&lt;li&gt;第 3 行：用户占用资源百分比、系统内核占用资源百分比、改变过优先级的进程资源百分比、空闲的资源百分比等。&lt;/li&gt;
&lt;li&gt;第 4 行：物理内存总量、内存空闲量、内存使用量、作为内核缓存的内存量。&lt;/li&gt;
&lt;li&gt;第 5 行：虚拟内存总量、虚拟内存空闲量、虚拟内存使用量、已被提前加载的内存量。&lt;/li&gt;
&lt;li&gt;第 6 行里面主要看 PID 和 COMMAND 这两个参数，其中 PID 就是进程 ID ， COMMAND 就是执行的命令，能够看到比较靠前的两个进程都是k8s进程。&lt;/li&gt;
&lt;li&gt;干掉僵尸进程的指令，&lt;code&gt;ps -A -ostat,ppid | grep -e &#39;^[Zz]&#39; | awk &#39;{print $2}&#39; | xargs kill -HUP &amp;gt; /dev/null 2&amp;gt;&amp;amp;1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;在当前这个界面，按下数字键盘 1 能够看到各个 CPU 的详细利用率&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;top - 10:46:53 up 6 days, 11:29,  1 user,  load average: 0.15, 0.53, 0.57
Tasks: 303 total,   1 running, 200 sleeping,   0 stopped,   2 zombie
%Cpu0  :  1.7 us,  1.3 sy,  0.0 ni, 96.3 id,  0.7 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu1  :  2.0 us,  0.7 sy,  0.0 ni, 97.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu2  :  0.7 us,  0.3 sy,  0.0 ni, 99.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;2cpu分析之vmstat&#34;&gt;2、cpu分析之vmstat&lt;/h3&gt;
&lt;p&gt;一般 vmstat 工具的使用是通过两个数字参数来完成的，第一个参数是采样的时间间隔，单位是秒，第二个参数是采样的次数，这次的命令是：vmstat -n 3 2 意思就是隔 3 秒取样一次，一共取样 2 次。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;执行vmstat指令&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# vmstat -n 3 2
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
0  0      0 25910152 684436 3669440    0    0     1    38    4    0  5  2 89  1  3
3  0      0 25910008 684436 3669488    0    0     0   441 8905 15261  1  1 97  1  0
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;21procs&#34;&gt;2.1、procs:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;r ：运行和等待 CPU 时间片的进程数，一般来说整个系统的运行队列不要超过总核数的 2 倍，要不然系统压力太大了。&lt;/li&gt;
&lt;li&gt;b : 等待资源的进程数，比如正在等待磁盘 IO ，网络 IO 这种。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;22cpu&#34;&gt;2.2、cpu：&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;us  ：用户进程消耗 CPU 时间百分比， us 值高的话，说明用户进程消耗 CPU 时间比较长，如果长期大于 50% 的话，那就说明程序还有需要优化的地方。&lt;/li&gt;
&lt;li&gt;sy ：内核进程消耗的 CPU 时间百分比。&lt;/li&gt;
&lt;li&gt;us + sy 参考值为 80% ，如果大于 80% 的话，说明可能存在 CPU 不足。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3内存分析之free&#34;&gt;3、内存分析之free&lt;/h3&gt;
&lt;p&gt;一般我们使用free -m即可&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# free -m
              total        used        free      shared  buff/cache   available
Mem:          32165        2608       25354          13        4203       29861
Swap:             0           0           0
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;如果应用程序可用内存/系统物理内存大于 70% 的话，说明内存是充足的，没啥问题，但是如果小于 20% 的话，就要考虑增加内存了。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4磁盘分析之df&#34;&gt;4、磁盘分析之df&lt;/h3&gt;
&lt;p&gt;排查磁盘问题，首先要排查磁盘空间够不够，df和du就可以。df查看磁盘使用情况；du查看目录占用磁盘情况及子文件占用情况。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# df -hT
Filesystem     Type      Size  Used Avail Use% Mounted on
udev           devtmpfs   16G     0   16G   0% /dev
tmpfs          tmpfs     3.2G   14M  3.2G   1% /run
/dev/vda2      ext4       99G  9.4G   84G  11% /
tmpfs          tmpfs      16G     0   16G   0% /dev/shm
tmpfs          tmpfs     5.0M     0  5.0M   0% /run/lock
tmpfs          tmpfs      16G     0   16G   0% /sys/fs/cgroup

root@master2:~# du -sh
516M    .
root@master2:~# du -sh *
16K     1.sh
434M    etcd
82M     etcd.tar.gz
4.0K    etcd.txt
root@master2:~# du -h --max-depth=1
4.0K    ./.cache
8.0K    ./.gnupg
8.0K    ./.ansible
434M    ./etcd
948K    ./.kube
8.0K    ./.ssh
8.0K    ./.vim
516M    .
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;5磁盘io分析之iostat&#34;&gt;5、磁盘io分析之iostat&lt;/h3&gt;
&lt;p&gt;在对数据库进行操作时，第一要考虑就是磁盘io操作，因为相对来说，如果在某个时间段给磁盘进行大量的写入操作会造成程序等待时间长，导致客户端那边好久都没啥反应。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# iostat -xdk 3 2
Linux 4.15.0-115-generic (master2)      09/11/2020      _x86_64_        (16 CPU)

Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
loop0            0.00    0.00      0.02      0.00     0.00     0.00   0.00   0.00    1.75    0.00   0.00    18.07     0.00   0.16   0.00
loop1            0.05    0.00      0.07      0.00     0.00     0.00   0.00   0.00    1.66    0.00   0.00     1.49     0.00   0.06   0.00
vda              0.74   19.38     18.68    154.10     0.00    10.14   0.07  34.35    6.74   17.52   0.28    25.39     7.95   0.79   1.60

Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
loop0            0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00
loop1            0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00
vda              0.00    7.00      0.00     61.33     0.00     6.00   0.00  46.15    0.00    1.33   0.00     0.00     8.76   0.00   0.00
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;rkB/s ：每秒读取数据量 kB 。&lt;/li&gt;
&lt;li&gt;wkB/s ：每秒写入数据量 kB 。&lt;/li&gt;
&lt;li&gt;svctm ：I/O 请求的平均服务时间，单位毫秒。&lt;/li&gt;
&lt;li&gt;util ：一秒中有百分之几的时间用于 I/O 操作，如果接近 100% 说明磁盘带宽跑满了，这个时候就要优化程序或者增加磁盘了。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;6网络io分析之sar&#34;&gt;6、网络io分析之sar&lt;/h3&gt;
&lt;p&gt;网络 IO 的话，可以通过 sar -n DEV 3 2 这条命令来看，和上面的差不多，意思就是每隔 3 秒取样一次，一共取样 2 次。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# sar -n DEV 3 2
Linux 4.15.0-115-generic (master2)      09/11/2020      _x86_64_        (16 CPU)

11:06:46 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
11:06:49 AM      eth0    854.33    859.33    199.93    210.99      0.00      0.00      0.00      0.00
11:06:49 AM     tunl0    113.67    108.33     10.92      9.86      0.00      0.00      0.00      0.00
11:06:49 AM calide035c655d8     98.33    108.33     11.04     12.63      0.00      0.00      0.00      0.00
11:06:49 AM        lo     58.00     58.00     13.18     13.18      0.00      0.00      0.00      0.00
11:06:49 AM cali3d31c61f18c      6.00      3.33      4.41      2.08      0.00      0.00      0.00      0.00
11:06:49 AM nodelocaldns      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
11:06:49 AM kube-ipvs0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
11:06:49 AM cali7d0b83575fc      7.00      8.00      2.46      0.81      0.00      0.00      0.00      0.00
11:06:49 AM cali056accc6554     24.33     21.00      2.93     10.81      0.00      0.00      0.00      0.00
11:06:49 AM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00

11:06:49 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
11:06:52 AM      eth0    623.67    620.00    109.84    127.33      0.00      0.00      0.00      0.00
11:06:52 AM     tunl0     97.33     90.00      6.64      8.74      0.00      0.00      0.00      0.00
11:06:52 AM calide035c655d8     93.67    104.00     10.52      8.68      0.00      0.00      0.00      0.00
11:06:52 AM        lo     65.67     65.67     11.60     11.60      0.00      0.00      0.00      0.00
11:06:52 AM cali3d31c61f18c      6.00      3.67      5.62      2.78      0.00      0.00      0.00      0.00
11:06:52 AM nodelocaldns      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
11:06:52 AM kube-ipvs0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
11:06:52 AM cali7d0b83575fc      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
11:06:52 AM cali056accc6554      3.67      2.33      0.24      0.39      0.00      0.00      0.00      0.00
11:06:52 AM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00

Average:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
Average:         eth0    739.00    739.67    154.89    169.16      0.00      0.00      0.00      0.00
Average:        tunl0    105.50     99.17      8.78      9.30      0.00      0.00      0.00      0.00
Average:    calide035c655d8     96.00    106.17     10.78     10.65      0.00      0.00      0.00      0.00
Average:           lo     61.83     61.83     12.39     12.39      0.00      0.00      0.00      0.00
Average:    cali3d31c61f18c      6.00      3.50      5.02      2.43      0.00      0.00      0.00      0.00
Average:    nodelocaldns      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:    kube-ipvs0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:    cali7d0b83575fc      3.50      4.00      1.23      0.40      0.00      0.00      0.00      0.00
Average:    cali056accc6554     14.00     11.67      1.58      5.60      0.00      0.00      0.00      0.00
Average:      docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;IFACE ：LAN 接口&lt;/li&gt;
&lt;li&gt;rxpck/s ：每秒钟接收的数据包&lt;/li&gt;
&lt;li&gt;txpck/s ：每秒钟发送的数据包&lt;/li&gt;
&lt;li&gt;rxKB/s ：每秒接收的数据量，单位 KByte&lt;/li&gt;
&lt;li&gt;txKB/s ：每秒发出的数据量，单位 KByte&lt;/li&gt;
&lt;li&gt;rxcmp/s ：每秒钟接收的压缩数据包&lt;/li&gt;
&lt;li&gt;txcmp/s ：每秒钟发送的压缩数据包&lt;/li&gt;
&lt;li&gt;rxmcst/s：每秒钟接收的多播数据包&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>提前预防K8s集群资源不足的处理方式配置</title>
      <link>https://Forest-L.github.io/post/prevent-the-configuration-of-the-k8s-cluster-from-under-resourcing-in-advance/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/prevent-the-configuration-of-the-k8s-cluster-from-under-resourcing-in-advance/</guid>
      <description>&lt;h1 id=&#34;提前预防k8s集群资源不足的处理方式配置&#34;&gt;提前预防K8s集群资源不足的处理方式配置&lt;/h1&gt;
&lt;p&gt;在管理集群的时候我们常常会遇到资源不足的情况，在这种情况下我们要保证整个集群可用，并且尽可能减少应用的损失。根据该问题提出以下两种方案：一种为优化kubelet参数，另一种为脚本化诊断处理。&lt;/p&gt;
&lt;h2 id=&#34;概念解释&#34;&gt;概念解释&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;CPU 的使用时间是可压缩的，换句话说它本身无状态，申请资源很快，也能快速正常回收。&lt;/li&gt;
&lt;li&gt;内存（memory）大小是不可压缩的，因为它是有状态的（内存里面保存的数据），申请资源很慢（需要计算和分配内存块的空间），并且回收可能失败（被占用的内存一般不可回收）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;优化kubelet参数&#34;&gt;优化kubelet参数&lt;/h2&gt;
&lt;p&gt;优化kubelet参数通过k8s资源表示、节点资源配置及kubelet参数设置、应用优先级和资源动态调整这几个方面来介绍。k8s资源表示为yaml文件中如何添加requests和limites参数。节点资源配置及kubelet参数设置描述为一个node上面资源配置情况，从而来优化kubelet参数。应用优先级描述为当资源不足时，优先保留那些pod不被驱逐。资源动态调整描述为运算能力的增减，如：HPA 、VPA和Cluster Auto Scaler。&lt;/p&gt;
&lt;h3 id=&#34;k8s资源表示&#34;&gt;k8s资源表示&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;在k8s中，资源表示配置字段是 spec.containers[].resource.limits/request.cpu/memory。yaml格式如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;spec:
  template:
    ...
    spec:
      containers:
        ...
        resources:
          limits:
            cpu: &amp;quot;1&amp;quot;
            memory: 1000Mi
          requests:
            cpu: 20m
            memory: 100Mi
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;资源动态调整&#34;&gt;资源动态调整&lt;/h3&gt;
&lt;p&gt;动态调整的思路：应用的实际流量会不断变化，因此使用率也是不断变化的，为了应对应用流量的变化，我们应用能够自动调整应用的资源。比如在线商品应用在促销的时候访问量会增加，我们应该自动增加 pod 运算能力来应对；当促销结束后，有需要自动降低 pod 的运算能力防止浪费。
运算能力的增减有两种方式：改变单个 pod 的资源，已经增减 pod 的数量。这两种方式对应了 kubernetes 的 HPA 和 VPA和Cluster Auto Scaler。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HPA: 横向 pod 自动扩展的思路是这样的：kubernetes 会运行一个 controller，周期性地监听 pod 的资源使用情况，当高于设定的阈值时，会自动增加 pod 的数量；当低于某个阈值时，会自动减少 pod 的数量。自然，这里的阈值以及 pod 的上限和下限的数量都是需要用户配置的。&lt;/li&gt;
&lt;li&gt;VPA: VPA 调整的是单个 pod 的 request 值（包括 CPU 和 memory）VPA 包括三个组件：
（1）Recommander：消费 metrics server 或者其他监控组件的数据，然后计算 pod 的资源推荐值
（2）Updater：找到被 vpa 接管的 pod 中和计算出来的推荐值差距过大的，对其做 update 操作（目前是 evict，新建的 pod 在下面 admission controller 中会使用推荐的资源值作为 request）
（3）Admission Controller：新建的 pod 会经过该 Admission Controller，如果 pod 是被 vpa 接管的，会使用 recommander 计算出来的推荐值&lt;/li&gt;
&lt;li&gt;CLuster Auto Scaler：能够根据整个集群的资源使用情况来增减节点。Cluster Auto Scaler 就是监控这个集群因为资源不足而 pending 的 pod，根据用户配置的阈值调用公有云的接口来申请创建机器或者销毁机器。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;节点资源配置及kubelet参数设置&#34;&gt;节点资源配置及kubelet参数设置&lt;/h3&gt;
&lt;p&gt;节点资源的配置一般分为 2 种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;资源预留：为系统进程和 k8s 进程预留资源&lt;/li&gt;
&lt;li&gt;pod 驱逐：节点资源到达一定使用量，开始驱逐 pod&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Node Capacity&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;kube-reserved&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;system-reserved&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;eviction-threshold&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Allocatable&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Node Capacity：Node的所有硬件资源&lt;/li&gt;
&lt;li&gt;kube-reserved：给kube组件预留的资源：kubelet,kube-proxy以及docker等&lt;/li&gt;
&lt;li&gt;system-reserved：给system进程预留的资源&lt;/li&gt;
&lt;li&gt;eviction-threshold：kubelet eviction的阈值设定&lt;/li&gt;
&lt;li&gt;Allocatable：真正scheduler调度Pod时的参考值（保证Node上所有Pods的request resource不超过Allocatable）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;allocatable的值即对应 describe node 时看到的allocatable容量，pod 调度的上限&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;计算公式：节点上可配置值 = 总量 - 预留值 - 驱逐阈值

Allocatable = Capacity - Reserved(kube+system) - Eviction Threshold
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以上配置均在kubelet 中添加，涉及的参数有：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--kube-reserved=cpu=200m,memory=250Mi \
--system-reserved=cpu=200m,memory=250Mi \
--eviction-hard=memory.available&amp;lt;5%,nodefs.available&amp;lt;10%,imagefs.available&amp;lt;10% \
--eviction-soft=memory.available&amp;lt;10%,nodefs.available&amp;lt;15%,imagefs.available&amp;lt;15% \
--eviction-soft-grace-period=memory.available=2m,nodefs.available=2m,imagefs.available=2m \
--eviction-max-pod-grace-period=120 \
--eviction-pressure-transition-period=30s \
--eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=500Mi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以上配置均为百分比，举例：&lt;/p&gt;
&lt;p&gt;以2核4GB内存40GB磁盘空间的配置为例，Allocatable是1.6 CPU，3.3Gi 内存，25Gi磁盘。当pod的总内存消耗大于3.3Gi或者磁盘消耗大于25Gi时，会根据相应策略驱逐pod。
Allocatable = 4Gi - 250Mi -250Mi - 4Gi*5% = 3.3Gi&lt;/p&gt;
&lt;p&gt;（1） 配置 k8s组件预留资源的大小，CPU、Mem&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;指定为k8s系统组件（kubelet、kube-proxy、dockerd等）预留的资源量，
如：--kube-reserved=cpu=1,memory=2Gi,ephemeral-storage=1Gi。
这里的kube-reserved只为非pod形式启动的kube组件预留资源，假如组件要是以static pod（kubeadm）形式启动的，那并不在这个kube-reserved管理并限制的cgroup中，而是在kubepod这个cgroup中。
（ephemeral storage需要kubelet开启feature-gates，预留的是临时存储空间（log，EmptyDir），生产环境建议先不使用）
ephemeral-storage是kubernetes1.8开始引入的一个资源限制的对象，kubernetes 1.10版本中kubelet默认已经打开的了,到目前1.11还是beta阶段，主要是用于对本地临时存储使用空间大小的限制，如对pod的empty dir、/var/lib/kubelet、日志、容器可读写层的使用大小的限制。
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（2）配置 系统守护进程预留资源的大小（预留的值需要根据机器上容器的密度做一个合理的值）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;含义：为系统守护进程(sshd, udev等)预留的资源量，
如：--system-reserved=cpu=500m,memory=1Gi,ephemeral-storage=1Gi。
注意，除了考虑为系统进程预留的量之外，还应该为kernel和用户登录会话预留一些内存。
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（3）配置 驱逐pod的硬阈值&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;含义：设置进行pod驱逐的阈值，这个参数只支持内存和磁盘。
通过--eviction-hard标志预留一些内存后，当节点上的可用内存降至保留值以下时，
kubelet 将会对pod进行驱逐。
配置：--eviction-hard=memory.available&amp;lt;5%,nodefs.available&amp;lt;10%,imagefs.available&amp;lt;10%
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（4）配置 驱逐pod的软阈值&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--eviction-soft=memory.available&amp;lt;10%,nodefs.available&amp;lt;15%,imagefs.available&amp;lt;15%
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（5）定义达到软阈值之后，持续时间超过多久才进行驱逐&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--eviction-soft-grace-period=memory.available=2m,nodefs.available=2m,imagefs.available=2m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（6）驱逐pod前最大等待时间=min(pod.Spec.TerminationGracePeriodSeconds, eviction-max-pod-grace-period)，单位为秒&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--eviction-max-pod-grace-period=120
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（7）至少回收的资源量&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=500Mi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（8）防止波动,kubelet 多久才上报节点的状态&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--eviction-pressure-transition-period=30s
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;应用优先级&#34;&gt;应用优先级&lt;/h3&gt;
&lt;p&gt;当资源不足时，配置了如上驱逐参数，pod之间的驱逐顺序是怎样的呢？以下描述设置不同优先级来确保集群中核心的组件不被驱逐还正常运行，OOM 的优先级如下,pod oom 值越低，也就越不容易被系统杀死。：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;BestEffort Pod &amp;gt; Burstable Pod &amp;gt; 其它进程（内核init进程等） &amp;gt; Guaranteed Pod &amp;gt; kubelet/docker 等 &amp;gt; sshd 等进程
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;kubernetes 把 pod 分成了三个 QoS 等级，而其中和limits和requests参数有关：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Guaranteed：oom优先级最低，可以考虑数据库应用或者一些重要的业务应用。除非 pods 使用超过了它们的 limits，或者节点的内存压力很大而且没有 QoS 更低的 pod，否则不会被杀死。&lt;/li&gt;
&lt;li&gt;Burstable：这种类型的 pod 可以多于自己请求的资源（上限有 limit 指定，如果 limit 没有配置，则可以使用主机的任意可用资源），但是重要性认为比较低，可以是一般性的应用或者批处理任务。&lt;/li&gt;
&lt;li&gt;Best Effort：oom优先级最高，集群不知道 pod 的资源请求情况，调度不考虑资源，可以运行到任意节点上（从资源角度来说），可以是一些临时性的不重要应用。pod 可以使用节点上任何可用资源，但在资源不足时也会被优先杀死。
Pod 的 requests 和 limits 是如何对应到这三个 QoS 等级上的，可以用下面一张表格概括：&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;request是否配置&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;limits是否配置&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;两者的关系&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Qos&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;requests=limits&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Guaranteed&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;所有容器的cpu和memory都必须配置相同的requests和limits&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;request&amp;lt;limit&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Burstable&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;只要有容器配置了cpu或者memory的request和limits就行&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;否&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Burstable&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;只要有容器配置了cpu或者memory的request就行&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;否&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Guaranteed/Burstable&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;如果配置了limits，k8s会自动把对应资源的request设置和limits一样。如果所有容器所有资源都配置limits，那就是Guaranteed;如果只有部分配置了limits，就是Burstable&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;否&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;否&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Best Effort&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;所有的容器都没有配置资源requests或limits&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;request和limits相同，可以参考资源动态调整中的VPA设置合理值。&lt;/li&gt;
&lt;li&gt;如果只配置了limits，没有配置request，k8s会把request值和limits值一样。&lt;/li&gt;
&lt;li&gt;如果只配置了request，没有配置limits，该pod共享node上可用的资源，实际上很反对这样设置。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;
&lt;p&gt;动态地资源调整通过 kubelet 驱逐程序进行的，但需要和应用优先级配合使用才能达到很好的效果，否则可能驱逐集群中核心组件。&lt;/p&gt;
&lt;h2 id=&#34;脚本化诊断处理&#34;&gt;脚本化诊断处理&lt;/h2&gt;
&lt;p&gt;什么叫脚本化诊断处理呢？它的含义为：当集群中的某台机器资源（一般指memory）用到85%-90%时，脚本自动检查到且该节点为不可调度。缺点为：背离了资源动态调整中CLuster Auto Scaler特点。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;集群中每台机器都可以执行kubectl指令：
如果没有设置，可将master机器上的$HOME/.kube/config文件拷贝到node机器上。&lt;/li&gt;
&lt;li&gt;可以通过&lt;a href=&#34;https://github.com/Forest-L/shell-operator&#34;&gt;shell-operator&lt;/a&gt;自动诊断机器资源且做cordon操作处理&lt;/li&gt;
&lt;li&gt;脚本中关键说明
（1）获取本地IP：ip a | grep &amp;lsquo;state UP&amp;rsquo; -A2| grep inet | grep -v inet6 | grep -v 127 | sed &amp;rsquo;s/^[ \t]*//g&amp;rsquo; | cut -d &#39; &#39; -f2 | cut -d &amp;lsquo;/&amp;rsquo; -f1
（2）获取本地ip对应的node名：kubectl get nodes -o  wide | grep &amp;ldquo;本地ip&amp;rdquo; | awk &amp;lsquo;{print $1}&amp;rsquo;
（3）不可调度：kubectl cordon node &amp;lt;node名&amp;gt;
（4）获取总内存： free -m | awk &amp;lsquo;NR==2{print $2}&amp;rsquo;
（5）获取使用内存： free -m | awk &amp;lsquo;NR==2{print $3}&amp;rsquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考文档&#34;&gt;参考文档&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/zh/docs/tasks/administer-cluster/out-of-resource/&#34;&gt;https://kubernetes.io/zh/docs/tasks/administer-cluster/out-of-resource/&lt;/a&gt;
&lt;a href=&#34;https://cizixs.com/2018/06/25/kubernetes-resource-management/&#34;&gt;https://cizixs.com/2018/06/25/kubernetes-resource-management/&lt;/a&gt;
&lt;a href=&#34;https://segmentfault.com/a/1190000021402192&#34;&gt;https://segmentfault.com/a/1190000021402192&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>构建arm-x86架构的docker-image操作指南</title>
      <link>https://Forest-L.github.io/post/docker-image-operation-guide-for-building-arm-x86-architecture/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/docker-image-operation-guide-for-building-arm-x86-architecture/</guid>
      <description>&lt;h1 id=&#34;构建armx86架构的docker-image操作指南&#34;&gt;构建arm/x86架构的docker image操作指南&lt;/h1&gt;
&lt;p&gt;由于arm环境越来越受欢迎，镜像不单单满足x86结构的docker镜像，还需要arm操作系统的镜像，以下说明在x86机器上如何build一个arm结构的镜像，使用buildx指令来同时构建arm/x86结构的镜像。&lt;/p&gt;
&lt;h2 id=&#34;1启动一台ubuntu的机器并安装docker-1903&#34;&gt;1.	启动一台ubuntu的机器，并安装docker 19.03&lt;/h2&gt;
&lt;p&gt;在测试过程中发现 Centos7.5 有下面的问题，这里我们直接绕过
&lt;a href=&#34;https://github.com/multiarch/qemu-user-static/issues/38&#34;&gt;issue&lt;/a&gt;
docker安装参考&lt;a href=&#34;https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/&#34;&gt;docker安装&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;2运行下列命令安装并测试qemu&#34;&gt;2.	运行下列命令安装并测试qemu&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;查看机器的架构&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;uname -m
x86_64
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;正常测试docker启动一个arm镜像容器&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -t arm64v8/ubuntu uname -m
standard_init_linux.go:211: exec user process caused &amp;quot;exec format error&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;添加特权模式安装qemu，且启动一个arm镜像容器&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm --privileged multiarch/qemu-user-static --reset -p yes
docker run --rm -t arm64v8/ubuntu uname -m
aarch64
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3--启用docker--buildx-命令&#34;&gt;3.	  启用docker  buildx 命令&lt;/h2&gt;
&lt;p&gt;docker buildx 为跨平台构建 docker 镜像所使用的命令。目前为实验特性，可以设置dokcer cli的配置，将实验特性开启。&lt;/p&gt;
&lt;p&gt;将下面配置添加到CLI配置文件当中~/.docker/config.json&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;experimental&amp;quot;: &amp;quot;enabled&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;4创建新的builder实例默认的docker实例不支持镜像导出&#34;&gt;4.	创建新的builder实例（默认的docker实例不支持镜像导出）&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;docker buildx create --name ks-all&lt;/code&gt;
&lt;code&gt;docker buildx use ks-all&lt;/code&gt;
&lt;code&gt;docker buildx inspect --bootstrap&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;执行下面命令可以看到 builder 已经创建好，并且支持多种平台的构建。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker buildx ls
NAME/NODE DRIVER/ENDPOINT             STATUS  PLATFORMS
ks-all *  docker-container
  ks-all0 unix:///var/run/docker.sock running linux/amd64, linux/arm64, linux/riscv64, linux/ppc64le, linux/s390x, linux/386, linux/arm/v7, linux/arm/v6
default   docker
  default default                     running linux/amd64, linux/386
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;5执行构建命令以ks-installer为例&#34;&gt;5.	执行构建命令（以ks-installer为例）&lt;/h2&gt;
&lt;p&gt;在 ks-installer目录下执行命令可以构建 arm64与amd64的镜像，并自动推送到镜像仓库中。
&lt;code&gt;docker buildx build -f /root/ks-installer/Dockerfile --output=type=registry --platform linux/arm64  -t lilinlinlin/ks-installer:2.1.0-arm64 .&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;(需要注意现在 ks-installer 的 Dockerfile中 go build 命令带有 GOOS GOARCH等，这些要删除)&lt;/p&gt;
&lt;p&gt;构建成功之后，可以在dockerhub下图当中可以看到是支持两种arch&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;DIGEST                       OS/ARCH                          COMPRESSED SIZE
97dd2142cac6                 linux/amd64                       111.13 MB
ce366ad696cb                 linux/arm64                       111.13 MB
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;6-构建并保存为tar-文件&#34;&gt;6.	 构建并保存为tar 文件&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;可以参考 buildx 的官方文档
&lt;a href=&#34;https://github.com/docker/buildx#-o---outputpath-typetypekeyvalue&#34;&gt;https://github.com/docker/buildx#-o---outputpath-typetypekeyvalue&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker buildx build  --output=type=docker,dest=/root/ks-installer.tar --platform  linux/arm64 -t lilinlinlin/ks-installer:2.1.0-arm64 ./pkg/db/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;构建tar包时需要注意output的类型需要是docker，而不是tar&lt;/p&gt;
&lt;h2 id=&#34;更多参考&#34;&gt;更多参考&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/docker/buildx&#34;&gt;https://github.com/docker/buildx&lt;/a&gt;
&lt;a href=&#34;https://github.com/multiarch/qemu-user-static&#34;&gt;https://github.com/multiarch/qemu-user-static&lt;/a&gt;
&lt;a href=&#34;https://community.arm.com/developer/tools-software/tools/b/tools-software-ides-blog/posts/getting-started-with-docker-for-arm-on-linux&#34;&gt;https://community.arm.com/developer/tools-software/tools/b/tools-software-ides-blog/posts/getting-started-with-docker-for-arm-on-linux&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>测量网络速度工具之iperf认知</title>
      <link>https://Forest-L.github.io/post/iperf-cognition-as-a-network-speed-measurement-tool/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/iperf-cognition-as-a-network-speed-measurement-tool/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;iperf工具是测量服务器网络速度工具，它通过测量服务器可以处理的最大网络吞吐量来测试网络速度，在遇到网络问题时特别有用。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;1下载源代码服务端和客户端都要安装&#34;&gt;1、下载源代码（服务端和客户端都要安装）&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;wget https://iperf.fr/download/source/iperf-2.0.8-source.tar.gz&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;安装编译环境
&lt;code&gt;yum install gcc-c++ -y&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;解压并安装iperf&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;tar -xvf iperf-2.0.8-source.tar.gz

cd iperf-2.0.8/

./configure &amp;amp;&amp;amp; make &amp;amp;&amp;amp; make install
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;2测试&#34;&gt;2、测试&lt;/h3&gt;
&lt;h4 id=&#34;21服务端执行iperf指令&#34;&gt;2.1、服务端执行iperf指令&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;iperf -s -p 12345 -i 1&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;-s表示以服务器模式运行。&lt;/li&gt;
&lt;li&gt;-p设置服务监听端口，测试时该端口在服务上没有被占用即可。&lt;/li&gt;
&lt;li&gt;-i设置每次报告之间的时间间隔，单位为s。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;iperf -s -p 12345 -i 1
------------------------------------------------------------
Server listening on TCP port 12345
TCP window size: 85.3 KByte (default)
------------------------------------------------------------
[  4] local 192.168.0.8 port 12345 connected with 192.168.0.10 port 52648
[ ID] Interval       Transfer     Bandwidth
[  4]  0.0- 1.0 sec   119 MBytes   999 Mbits/sec
[  4]  1.0- 2.0 sec  60.3 MBytes   506 Mbits/sec
[  4]  2.0- 3.0 sec  61.7 MBytes   517 Mbits/sec
[  4]  3.0- 4.0 sec  60.9 MBytes   511 Mbits/sec
[  4]  4.0- 5.0 sec  59.9 MBytes   503 Mbits/sec
[  4]  5.0- 6.0 sec  61.1 MBytes   512 Mbits/sec
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;22客户端执行iperf指令&#34;&gt;2.2、客户端执行iperf指令&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;iperf -c XX.XX.XX.XX -p 1234 -i 1&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;其中XX.XX.XX.XX为服务端的ip。&lt;/li&gt;
&lt;li&gt;-p要和服务端设置的相同。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;iperf -c 192.168.0.8 -p 12345 -i 1
------------------------------------------------------------
Client connecting to 192.168.0.8, TCP port 12345
TCP window size: 45.0 KByte (default)
------------------------------------------------------------
[  3] local 192.168.0.10 port 52648 connected with 192.168.0.8 port 12345
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   123 MBytes  1.03 Gbits/sec
[  3]  1.0- 2.0 sec  60.1 MBytes   504 Mbits/sec
[  3]  2.0- 3.0 sec  62.0 MBytes   520 Mbits/sec
[  3]  3.0- 4.0 sec  60.4 MBytes   506 Mbits/sec
[  3]  4.0- 5.0 sec  60.2 MBytes   505 Mbits/sec
[  3]  5.0- 6.0 sec  60.6 MBytes   509 Mbits/sec
[  3]  6.0- 7.0 sec  62.1 MBytes   521 Mbits/sec
[  3]  7.0- 8.0 sec  59.4 MBytes   498 Mbits/sec
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.huaweicloud.com/kunpeng/software/iperf.html&#34;&gt;iperf&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>添加公网ip到kubernetes的apiserver操作指南</title>
      <link>https://Forest-L.github.io/post/add-public-ip-to-kubernetes-apiserver-operation-guide/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/add-public-ip-to-kubernetes-apiserver-operation-guide/</guid>
      <description>&lt;p&gt;通常情况下，我们的kubernetes集群是内网环境，如果希望通过本地访问这个集群，怎么办呢？大家想到的是Kubeadm在初始化的时候会为管理员生成一个 Kubeconfig文件，把它下载下来 是不是就可以？事实证明这样不行， 因为这个集群是内网集群，Kubeconfig文件 中APIServer的地址是内网ip。解决方案很简单，把公网ip签到证书里面就可以，其中有apiServerCertSANs这个选项，只要把公网IP写到这里，再启动这个集群的时候，这个公网ip就会签到证书里。&lt;/p&gt;
&lt;h2 id=&#34;1-环境信息&#34;&gt;1. 环境信息&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;安装方式：kubeadm&lt;/li&gt;
&lt;li&gt;内网IP：192.168.0.8&lt;/li&gt;
&lt;li&gt;外网IP：139.198.19.37&lt;/li&gt;
&lt;li&gt;证书目录：/etc/kubernetes/pki&lt;/li&gt;
&lt;li&gt;kubeadm配置文件目录：/etc/kubernetes/kubeadm-config.yaml&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-查看apiserver的证书包含的ip进入到证书目录执行&#34;&gt;2. 查看apiserver的证书包含的ip,进入到证书目录执行&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;cd /etc/kubernetes/pki&lt;/code&gt;
&lt;code&gt;openssl x509 -in apiserver.crt -noout -text&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;openssl x509 -in apiserver.crt -noout -text
Certificate:
    Data:
        ................
        Validity
            Not Before: Jun  5 02:26:44 2020 GMT
            Not After : Jun  5 02:26:44 2021 GMT
        ..................
        X509v3 extensions:
            ..........
            X509v3 Subject Alternative Name:
                IP Address:192.168.0.8
    .......
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3-添加公网ip到apiserver&#34;&gt;3. 添加公网IP到apiserver&lt;/h2&gt;
&lt;p&gt;绑定的公网ip为 139.198.19.37 ，确保公网ip的防火墙已经打开6443端口&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3.1 登录到主节点，进入 /etc/kubernetes/目录下&lt;/li&gt;
&lt;li&gt;3.2 修改kubeadm-config.yaml，找到 ClusterConfiguration 中的 	certSANs (如无，在 apiServer 下添加这一配置)，如下。添加刚才绑定的 139.198.19.37 到 certSANs 下，保存文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
etcd:
  external:
    endpoints:
    - https://192.168.0.8:2379
apiServer:
  extraArgs:
    authorization-mode: Node,RBAC
  timeoutForControlPlane: 4m0s
  certSANs:
    - 192.168.0.8
    - 139.198.19.37
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;3.3 执行如下命令更新 apiserver.crt apiserver.key
注意需要把之前apiserver.crt apiserver.key做备份,进入到pki目录下，执行如下指令做备份：
&lt;code&gt;mv apiserver.crt apiserver.crt-bak&lt;/code&gt;
&lt;code&gt;mv apiserver.key apiserver.key-bak&lt;/code&gt;
备份完之后，回到/etc/kubernetes目录下，执行公网ip添加到apiserver操作指令为：
kubeadm init phase certs apiserver &amp;ndash;config kubeadm-config.yaml&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubeadm init phase certs apiserver --config kubeadm-config.yaml
[certs] Generating &amp;quot;apiserver&amp;quot; certificate and key
[certs] apiserver serving cert is signed for DNS names [192.168.0.8  139.198.19.37]
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;3.4 再次查看apiserver中证书包含的ip，指令如下,看的公网ip则操作成功。
openssl x509 -in pki/apiserver.crt -noout -text | grep 139.198.19.37&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;openssl x509 -in pki/apiserver.crt -noout -text | grep 139.198.19.37
                IP Address:192.168.0.8, IP Address:139.198.19.37
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;3.5 重启kube-apiserver
如果是高可用集群，直接杀死当前节点的kube-apiserver进程，等待kubelet拉起kube-apiserver即可。需要在三个节点执行步骤1到步骤4，逐一更新。
如果是非高可用集群，杀死kube-apiserver可能会导致服务有中断，需要在业务低峰的时候操作。
进入/etc/kubernetes/manifests目录下，mv kube-apiserver.yaml文件至别的位置，然后又移回来即可&lt;/li&gt;
&lt;li&gt;3.6 修改kubeconfig中的server ip地址为 139.198.19.37，保存之后就可以直接通过公网访问kubernetes集群
&lt;code&gt;kubectl --kubeconfig config config view&lt;/code&gt;
&lt;code&gt;kubectl --kubeconfig config get node&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-附录-证书过期处理方式&#34;&gt;4. 附录-证书过期处理方式&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;kubeadm部署的k8s集群，默认证书目录为：/etc/kubernetes/pki,如果非pki，以ssl为例，需要创建软链接。证书过期包含核心组件apiserver和node上的token。&lt;/li&gt;
&lt;li&gt;4.1 master节点-apiserver处理方式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;1、查看证书有效期
cd /etc/kubernetes
openssl x509 -in ssl/apiserver.crt -noout -enddate 
2. 更新过期证书（/etc/kubernetes） (先在master1 节点执行)
创建软连接pki -&amp;gt; ssl ： ln -s ssl/ pki    (如pki存在，可略过)

kubeadm alpha certs renew apiserver 
kubeadm alpha certs renew apiserver-kubelet-client 
kubeadm alpha certs renew front-proxy-client 
3. 更新kubeconfig（/etc/kubernetes）(master1 节点)
需更新admin.conf / scheduler.conf / controller-manager.conf / kubelet.conf

kubeadm alpha certs renew admin.conf
kubeadm alpha certs renew controller-manager.conf
kubeadm alpha certs renew scheduler.conf

特别注意：以master1为例，将如下master1替换实际的节点名称。
kubeadm alpha kubeconfig user --client-name=system:node:master1 --org=system:nodes &amp;gt; kubelet.conf

4. 如上述kubeconfig中apiserver地址非lb地址，则修改为lb地址：(master1 节点)
https://192.168.0.13:6443 -&amp;gt; https://{ lb domain or ip }:6443

5. 重启k8s master组件：(master1 节点)
docker ps -af name=k8s_kube-apiserver* -q | xargs --no-run-if-empty docker rm -f
docker ps -af name=k8s_kube-scheduler* -q | xargs --no-run-if-empty docker rm -f
docker ps -af name=k8s_kube-controller-manager* -q | xargs --no-run-if-empty docker rm -f
systemctl restart kubelet

6. 验证kubeconfig有效性及查看节点状态 (master1 节点)

kubectl get node –kubeconfig admin.conf
kubectl get node –kubeconfig scheduler.conf
kubectl get node –kubeconfig controller-manager.conf
kubectl get node –kubeconfig kubelet.conf

7. 特别注意：同步master1证书/etc/kubernetes/ssl至master2、master3的对应路径中/etc/kubernetes/ssl（同步前建议备份旧证书）
证书路径：/etc/kubernetes/ssl

8. 更新kubeconfig（/etc/kubernetes）(master2, master3)

kubeadm alpha certs renew admin.conf
kubeadm alpha certs renew controller-manager.conf
kubeadm alpha certs renew scheduler.conf

特别注意：以下命令中以master2、master3为例，将如下master2/master3替换实际的节点名称。
kubeadm alpha kubeconfig user --client-name=system:node:master2 --org=system:nodes &amp;gt; kubelet.conf （master2）
kubeadm alpha kubeconfig user --client-name=system:node:master3 --org=system:nodes &amp;gt; kubelet.conf （master3）

9. 如上述kubeconfig中apiserver地址非lb地址，则修改为lb地址：(master2、master3)

https://192.168.0.13:6443 -&amp;gt; https://{ lb domain or ip }:6443

注：涉及文件：admin.conf、controller-manager.conf、scheduler.conf、kubelet.conf

10. 重启master2、master3中对应master组件

docker ps -af name=k8s_kube-apiserver* -q | xargs --no-run-if-empty docker rm -f
docker ps -af name=k8s_kube-scheduler* -q | xargs --no-run-if-empty docker rm -f
docker ps -af name=k8s_kube-controller-manager* -q | xargs --no-run-if-empty docker rm -f
systemctl restart kubelet

11. 验证kubeconfig有效性 （master2、master3）

kubectl get node –kubeconfig admin.conf
kubectl get node –kubeconfig scheduler.conf
kubectl get node –kubeconfig controller-manager.conf
kubectl get node –kubeconfig kubelet.conf
12. 更新~/.kube/config （master1、master2、master3）

cp admin.conf ~/.kube/config
注：如node节点也需使用kubectl，将master1上的~/.kube/config拷贝至对应node节点~/.kube/config

13. 验证~/.kube/config有效性：

 kubectl get node  查看集群状态
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;4.2 node节点token证书处理方式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;1. kubeadm token list 查看输出若为空或显示日期过期，则需重新生成。

2. kubeadm token create 重新生成token

3. 记录token值,保存下来。

4. 替换node节点/etc/kubernetes/ bootstrap-kubelet.conf中token （所有node节点）

5. 删除/etc/kubernetes/kubelet.conf （所有node节点）

rm -rf /etc/kubernetes/kubelet.conf
6. 重启kubelet （所有node节点）

systemctl restart kubelet
7. 查看节点状态：

kubectl get node 验证集群状态
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>脚本化触发Kubernetes集群事件的工具-Shell-operator</title>
      <link>https://Forest-L.github.io/post/scripting-the-tool-shell-operator-that-triggers-kubernetes-cluster-events/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/scripting-the-tool-shell-operator-that-triggers-kubernetes-cluster-events/</guid>
      <description>&lt;p&gt;Shell-operator是用于在Kubernetes集群中运行事件驱动脚本工具。Shell-operator通过脚本作为事件触发的钩子（hook），在Kubernetes集群事件和Shell脚本之间提供了一个转化层。触发钩子包含add, update和delete。以pod add为例通俗的话说，当新创建了一个pod时，会自动触发脚本中else部分。&lt;/p&gt;
&lt;h4 id=&#34;shell-operator特点&#34;&gt;Shell-operator特点：&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;轻松管理Kubernetes集群：可以是bash，python和kubectl。&lt;/li&gt;
&lt;li&gt;Kubernetes对象事件：钩子触发包含add, update或delete事件。&lt;/li&gt;
&lt;li&gt;对象选择器和属性过滤器：可以监视一组特定的对象并检测其属性的变化。&lt;/li&gt;
&lt;li&gt;配置简单：钩子绑定语法格式为yaml/json输出。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1环境准备只要kubernetes环境即可&#34;&gt;1、环境准备（只要Kubernetes环境即可）&lt;/h2&gt;
&lt;p&gt;Kubernetes: v1.18.3&lt;/p&gt;
&lt;h2 id=&#34;2快速开始包含bash和python实例&#34;&gt;2、快速开始（包含bash和python实例）&lt;/h2&gt;
&lt;p&gt;目录结构&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 shell-operator]# tree
.
├── Dockerfile
├── hooks
│   └── pods-hook.sh
└── shell-operator-pod.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;21shell-operator最简设置步骤为&#34;&gt;2.1、Shell-operator最简设置步骤为：&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;用钩子（脚本）构建的镜像。&lt;/li&gt;
&lt;li&gt;在Kubernetes集群中创建必要的RBAC对象。&lt;/li&gt;
&lt;li&gt;使用构建的镜像运行一个pod/deployment。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;22用钩子脚本构建镜像&#34;&gt;2.2、用钩子脚本构建镜像&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;钩子是一个脚本，当执行&amp;ndash;config选项时，配置将以yaml/json格式输出。&lt;/li&gt;
&lt;li&gt;以下创建一个简单的operator将来监视所有namespaces下的所有pod，并记录新pod的名字。&lt;/li&gt;
&lt;li&gt;包含pods-hook.sh和Dockerfile&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以bash脚本为例&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pods-hook.sh&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;#!/usr/bin/env bash

if [[ $1 == &amp;quot;--config&amp;quot; ]] ; then
  cat &amp;lt;&amp;lt;EOF
configVersion: v1
kubernetes:
- apiVersion: v1
  kind: Pod
  executeHookOnEvent: [&amp;quot;Added&amp;quot;]
EOF
else
  podName=$(jq -r .[0].object.metadata.name $BINDING_CONTEXT_PATH)
  echo &amp;quot;Pod &#39;${podName}&#39; added&amp;quot;
fi
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;添加执行权限。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;chmod +x pods-hook.sh&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于flant/shell-operator:latest的基础镜像构建新的Dockerfile。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;FROM flant/shell-operator:latest
ADD pods-hook.sh /hooks
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;构建一个新的镜像。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker build -t lilinlinlin/shell-operator:monitor-pods&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;推镜像至dockerhub, 仓库名根据自身的情况而定，也可以不操作此步骤。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker push lilinlinlin/shell-operator:monitor-pods&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;23创建rbac对象&#34;&gt;2.3、创建RBAC对象&lt;/h3&gt;
&lt;p&gt;需要监视所有namespaces下的pods，意味着我们需要shell-operator的特定RBAC定义。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create namespace example-monitor-pods
kubectl create serviceaccount monitor-pods-acc --namespace example-monitor-pods
kubectl create clusterrole monitor-pods --verb=get,watch,list --resource=pods
kubectl create clusterrolebinding monitor-pods --clusterrole=monitor-pods --serviceaccount=example-monitor-pods:monitor-pods-acc
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;24在集群中部署shell-operator&#34;&gt;2.4、在集群中部署shell-operator&lt;/h3&gt;
&lt;p&gt;shell-operator-pod.yaml文件为&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: shell-operator
spec:
  containers:
  - name: shell-operator
    image: lilinlinlin/shell-operator:monitor-pods
    imagePullPolicy: IfNotPresent
  serviceAccountName: monitor-pods-acc
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;部署shell-operator时，pods-hook.sh脚本中if的部分就会被执行。新创建pod时，else部分就会被执行。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl -n example-monitor-pods apply -f shell-operator-pod.yaml&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3测试验证效果&#34;&gt;3、测试验证效果&lt;/h2&gt;
&lt;p&gt;部署一个nginx服务,查看日志。会出现Pod nginx-****** added字样，说明监视生效了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl run nginx --image=nginx
kubectl -n example-monitor-pods logs pod/shell-operator -f
...
INFO[0027] queue task HookRun:main                       operator.component=handleEvents queue=main
INFO[0030] Execute hook                                  binding=kubernetes hook=pods-hook.sh operator.component=taskRunner queue=main task=HookRun
INFO[0030] Pod &#39;nginx-775dd7f59c-hr7kj&#39; added  binding=kubernetes hook=pods-hook.sh output=stdout queue=main task=HookRun
INFO[0030] Hook executed successfully                    binding=kubernetes hook=pods-hook.sh operator.component=taskRunner queue=main task=HookRun
...
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;4python实例&#34;&gt;4、python实例&lt;/h2&gt;
&lt;p&gt;实例中直接运行else部分。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、hooks/00-hook.py
#!/usr/bin/env python

import sys

if __name__ == &amp;quot;__main__&amp;quot;:
    if len(sys.argv)&amp;gt;1 and sys.argv[1] == &amp;quot;--config&amp;quot;:
        print &#39;{&amp;quot;configVersion&amp;quot;:&amp;quot;v1&amp;quot;, &amp;quot;onStartup&amp;quot;: 10}&#39;
    else:
        print &amp;quot;OnStartup Python powered hook&amp;quot;

2、Dockerfile
FROM flant/shell-operator:latest-alpine3.11
RUN apk --no-cache add python
ADD hooks /hooks

3、shell-operator-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: shell-operator
spec:
  containers:
  - name: shell-operator
    image: registry.mycompany.com/shell-operator:startup-python
    imagePullPolicy: IfNotPresent

4、运行

docker build -t &amp;quot;registry.mycompany.com/shell-operator:startup-python&amp;quot; .
kubectl create ns example-startup-python
kubectl -n example-startup-python apply -f shell-operator-pod.yaml
kubectl -n example-startup-python logs -f po/shell-operator
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;5清理环境&#34;&gt;5、清理环境&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;kubectl delete ns example-monitor-pods
kubectl delete clusterrole monitor-pods
kubectl delete clusterrolebinding monitor-pods
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;6参考&#34;&gt;6、参考&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/flant/shell-operator&#34;&gt;shell-operator&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openebs基于k8s安装</title>
      <link>https://Forest-L.github.io/post/k8s-openebs-install/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s-openebs-install/</guid>
      <description>&lt;h2 id=&#34;ceph介绍&#34;&gt;ceph介绍&lt;/h2&gt;
&lt;p&gt;ceph为块存储，以下通过cephadm安装ceph集群，ceph主要有三个基本角色：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OSD 用于集群中所有数据与对象的存储，处理集群数据的复制、恢复、回填、再均衡，并向其他osd守护进程发送心跳，然后向 Monitor 提供一些监控信息。&lt;/li&gt;
&lt;li&gt;Monitor 监控整个集群的状态，维护集群的 cluster MAP 二进制表，保证集群数据的一致性。&lt;/li&gt;
&lt;li&gt;MDS (可选) 为 Ceph 文件系统提供元数据计算、缓存与同步。MDS 进程并不是必须的进程，只有需要使用 CephFS 时，才需要配置 MDS 节点。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1环境准备&#34;&gt;1、环境准备&lt;/h2&gt;
&lt;h3 id=&#34;11节点规划&#34;&gt;1.1、节点规划&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;节点  &lt;/th&gt;
&lt;th&gt;os  &lt;/th&gt;
&lt;th&gt;IP   &lt;/th&gt;
&lt;th&gt;磁盘&lt;/th&gt;
&lt;th&gt;角色&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ceph1&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.13&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;cephadm，mgr, mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph2&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.14&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph3&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.16&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;12修改hosts文件&#34;&gt;1.2、修改hosts文件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hostnamectl set-hostname ceph1  #节点1上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph2  #节点2上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph3  #节点3上执行&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;13配置hosts&#34;&gt;1.3、配置hosts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt; /etc/hosts &amp;lt;&amp;lt;EOF  
192.168.0.13 ceph1
192.168.0.14 ceph2
192.168.0.16 ceph3
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;14关闭防火墙&#34;&gt;1.4、关闭防火墙&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都要执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld
setenforce 0
sed -i &#39;s/^SELINUX=.*/SELINUX=disabled/&#39; /etc/selinux/config
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;15时间同步&#34;&gt;1.5、时间同步&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;yum install -y chrony
systemctl enable --now chronyd

以ceph1作为时间参考，以下在三个节点都操作，添加server ceph1 prefer
vi /etc/chrony.conf
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
server ceph1 prefer

重启chrony
systemctl restart chronyd
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;16dockerlvm2和python3安装&#34;&gt;1.6、docker、lvm2和python3安装&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;三个节点都需要执行：
curl -fsSL get.docker.com -o get-docker.sh 
sh get-docker.sh
systemctl enable docker
systemctl restart docker

yum install -y python3

yum -y install lvm2
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2cephadm安装ceph1&#34;&gt;2、cephadm安装(ceph1)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;官方下载方法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;git下载&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;git clone --single-branch --branch octopus https://github.com/ceph/ceph.git&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cp ceph/src/cephadm/cephadm ./&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cephadm安装(ceph1)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;./cephadm add-repo --release octopus
./cephadm install

mkdir -p /etc/ceph
cephadm bootstrap --mon-ip 192.168.0.13

安装完成提示：

             URL: https://ceph1:8443/
            User: admin
        Password: 86yvswzdzd

INFO:cephadm:You can access the Ceph CLI with:

        sudo /usr/sbin/cephadm shell --fsid 3861dbf4-fd47-11ea-a373-5254228af870 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

INFO:cephadm:Please consider enabling telemetry to help improve Ceph:

        ceph telemetry on

For more information see:

        https://docs.ceph.com/docs/master/mgr/telemetry/

INFO:cephadm:Bootstrap complete.
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;配置ceph.pub
将ceph1的/etc/ceph/ceph.pub放入到ceph2和ceph3的.ssh/authorized_keys,ceph2和ceph3下root目录下没有.ssh，得先创建，最低权限为700，authorized_keys最低权限600。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ceph指令生效&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;cephadm shell&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;添加节点&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph3&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3osd部署&#34;&gt;3、OSD部署&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cephadm shell下执行&lt;/li&gt;
&lt;li&gt;若块设备没显示，可加&amp;ndash;refresh参数&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[ceph: root@ceph1 /]# ceph orch device ls
HOST   PATH      TYPE   SIZE  DEVICE_ID     MODEL  VENDOR  ROTATIONAL  AVAIL  REJECT REASONS
ceph1  /dev/vdc  hdd   50.0G  vol-e801hi3v  n/a    0x1af4  1           True
ceph1  /dev/vda  hdd   20.0G  i-2iwis9yr    n/a    0x1af4  1           False  locked
ceph1  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
ceph2  /dev/vdc  hdd   50.0G  vol-lqcchpox  n/a    0x1af4  1           True
ceph2  /dev/vda  hdd   20.0G  i-r53b2flp    n/a    0x1af4  1           False  locked
ceph2  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  locked, Insufficient space (&amp;lt;5GB)
ceph3  /dev/vdc  hdd    100G  vol-p0eksa5m  n/a    0x1af4  1           True
ceph3  /dev/vda  hdd   20.0G  i-f6njdmtq    n/a    0x1af4  1           False  locked
ceph3  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
[ceph: root@ceph1 /]#

devcice确认true后，执行
[ceph: root@ceph1 /]# ceph orch apply osd --all-available-devices
Scheduled osd.all-available-devices update...
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;查看集群状态,ceph方式及运行容器方式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;ceph方式：
[ceph: root@ceph1 /]# ceph -s
  cluster:
    id:     af1f33f0-fd69-11ea-8530-5254228af870
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 66s)
    mgr: ceph1.rbarkj(active, since 12m), standbys: ceph2.uzdnki
    osd: 3 osds: 3 up (since 14s), 3 in (since 14s)

  data:
    pools:   1 pools, 1 pgs
    objects: 1 objects, 0 B
    usage:   3.0 GiB used, 197 GiB / 200 GiB avail
    pgs:     1 active+clean

容器方式：
[root@ceph1 ceph]# docker ps -a
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES
b417841153a9        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-osd -…&amp;quot;   5 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-osd.2
9dbd720f4a6f        prom/prometheus:v2.18.1      &amp;quot;/bin/prometheus --c…&amp;quot;   6 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-prometheus.ceph1
f860282adf85        prom/alertmanager:v0.20.0    &amp;quot;/bin/alertmanager -…&amp;quot;   6 minutes ago       Up 6 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-alertmanager.ceph1
a6cb4b354f46        ceph/ceph-grafana:6.6.2      &amp;quot;/bin/sh -c &#39;grafana…&amp;quot;   16 minutes ago      Up 16 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-grafana.ceph1
b745f57895f3        prom/node-exporter:v0.18.1   &amp;quot;/bin/node_exporter …&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-node-exporter.ceph1
b00c1a2c7ef6        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-crash…&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-crash.ceph1
99c703f18f67        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mgr -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mgr.ceph1.rbarkj
e972d3d3f13a        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mon -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mon.ceph1

&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.ceph.com/en/latest/cephadm/install/&#34;&gt;https://docs.ceph.com/en/latest/cephadm/install/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>rook基于k8s安装</title>
      <link>https://Forest-L.github.io/post/k8s-rook-install/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s-rook-install/</guid>
      <description>&lt;h2 id=&#34;ceph介绍&#34;&gt;ceph介绍&lt;/h2&gt;
&lt;p&gt;ceph为块存储，以下通过cephadm安装ceph集群，ceph主要有三个基本角色：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OSD 用于集群中所有数据与对象的存储，处理集群数据的复制、恢复、回填、再均衡，并向其他osd守护进程发送心跳，然后向 Monitor 提供一些监控信息。&lt;/li&gt;
&lt;li&gt;Monitor 监控整个集群的状态，维护集群的 cluster MAP 二进制表，保证集群数据的一致性。&lt;/li&gt;
&lt;li&gt;MDS (可选) 为 Ceph 文件系统提供元数据计算、缓存与同步。MDS 进程并不是必须的进程，只有需要使用 CephFS 时，才需要配置 MDS 节点。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1环境准备&#34;&gt;1、环境准备&lt;/h2&gt;
&lt;h3 id=&#34;11节点规划&#34;&gt;1.1、节点规划&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;节点  &lt;/th&gt;
&lt;th&gt;os  &lt;/th&gt;
&lt;th&gt;IP   &lt;/th&gt;
&lt;th&gt;磁盘&lt;/th&gt;
&lt;th&gt;角色&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ceph1&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.13&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;cephadm，mgr, mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph2&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.14&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph3&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.16&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;12修改hosts文件&#34;&gt;1.2、修改hosts文件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hostnamectl set-hostname ceph1  #节点1上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph2  #节点2上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph3  #节点3上执行&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;13配置hosts&#34;&gt;1.3、配置hosts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt; /etc/hosts &amp;lt;&amp;lt;EOF  
192.168.0.13 ceph1
192.168.0.14 ceph2
192.168.0.16 ceph3
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;14关闭防火墙&#34;&gt;1.4、关闭防火墙&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都要执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld
setenforce 0
sed -i &#39;s/^SELINUX=.*/SELINUX=disabled/&#39; /etc/selinux/config
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;15时间同步&#34;&gt;1.5、时间同步&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;yum install -y chrony
systemctl enable --now chronyd

以ceph1作为时间参考，以下在三个节点都操作，添加server ceph1 prefer
vi /etc/chrony.conf
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
server ceph1 prefer

重启chrony
systemctl restart chronyd
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;16dockerlvm2和python3安装&#34;&gt;1.6、docker、lvm2和python3安装&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;三个节点都需要执行：
curl -fsSL get.docker.com -o get-docker.sh 
sh get-docker.sh
systemctl enable docker
systemctl restart docker

yum install -y python3

yum -y install lvm2
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2cephadm安装ceph1&#34;&gt;2、cephadm安装(ceph1)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;官方下载方法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;git下载&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;git clone --single-branch --branch octopus https://github.com/ceph/ceph.git&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cp ceph/src/cephadm/cephadm ./&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cephadm安装(ceph1)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;./cephadm add-repo --release octopus
./cephadm install

mkdir -p /etc/ceph
cephadm bootstrap --mon-ip 192.168.0.13

安装完成提示：

             URL: https://ceph1:8443/
            User: admin
        Password: 86yvswzdzd

INFO:cephadm:You can access the Ceph CLI with:

        sudo /usr/sbin/cephadm shell --fsid 3861dbf4-fd47-11ea-a373-5254228af870 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

INFO:cephadm:Please consider enabling telemetry to help improve Ceph:

        ceph telemetry on

For more information see:

        https://docs.ceph.com/docs/master/mgr/telemetry/

INFO:cephadm:Bootstrap complete.
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;配置ceph.pub
将ceph1的/etc/ceph/ceph.pub放入到ceph2和ceph3的.ssh/authorized_keys,ceph2和ceph3下root目录下没有.ssh，得先创建，最低权限为700，authorized_keys最低权限600。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ceph指令生效&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;cephadm shell&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;添加节点&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph3&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3osd部署&#34;&gt;3、OSD部署&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cephadm shell下执行&lt;/li&gt;
&lt;li&gt;若块设备没显示，可加&amp;ndash;refresh参数&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[ceph: root@ceph1 /]# ceph orch device ls
HOST   PATH      TYPE   SIZE  DEVICE_ID     MODEL  VENDOR  ROTATIONAL  AVAIL  REJECT REASONS
ceph1  /dev/vdc  hdd   50.0G  vol-e801hi3v  n/a    0x1af4  1           True
ceph1  /dev/vda  hdd   20.0G  i-2iwis9yr    n/a    0x1af4  1           False  locked
ceph1  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
ceph2  /dev/vdc  hdd   50.0G  vol-lqcchpox  n/a    0x1af4  1           True
ceph2  /dev/vda  hdd   20.0G  i-r53b2flp    n/a    0x1af4  1           False  locked
ceph2  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  locked, Insufficient space (&amp;lt;5GB)
ceph3  /dev/vdc  hdd    100G  vol-p0eksa5m  n/a    0x1af4  1           True
ceph3  /dev/vda  hdd   20.0G  i-f6njdmtq    n/a    0x1af4  1           False  locked
ceph3  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
[ceph: root@ceph1 /]#

devcice确认true后，执行
[ceph: root@ceph1 /]# ceph orch apply osd --all-available-devices
Scheduled osd.all-available-devices update...
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;查看集群状态,ceph方式及运行容器方式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;ceph方式：
[ceph: root@ceph1 /]# ceph -s
  cluster:
    id:     af1f33f0-fd69-11ea-8530-5254228af870
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 66s)
    mgr: ceph1.rbarkj(active, since 12m), standbys: ceph2.uzdnki
    osd: 3 osds: 3 up (since 14s), 3 in (since 14s)

  data:
    pools:   1 pools, 1 pgs
    objects: 1 objects, 0 B
    usage:   3.0 GiB used, 197 GiB / 200 GiB avail
    pgs:     1 active+clean

容器方式：
[root@ceph1 ceph]# docker ps -a
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES
b417841153a9        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-osd -…&amp;quot;   5 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-osd.2
9dbd720f4a6f        prom/prometheus:v2.18.1      &amp;quot;/bin/prometheus --c…&amp;quot;   6 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-prometheus.ceph1
f860282adf85        prom/alertmanager:v0.20.0    &amp;quot;/bin/alertmanager -…&amp;quot;   6 minutes ago       Up 6 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-alertmanager.ceph1
a6cb4b354f46        ceph/ceph-grafana:6.6.2      &amp;quot;/bin/sh -c &#39;grafana…&amp;quot;   16 minutes ago      Up 16 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-grafana.ceph1
b745f57895f3        prom/node-exporter:v0.18.1   &amp;quot;/bin/node_exporter …&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-node-exporter.ceph1
b00c1a2c7ef6        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-crash…&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-crash.ceph1
99c703f18f67        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mgr -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mgr.ceph1.rbarkj
e972d3d3f13a        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mon -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mon.ceph1

&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.ceph.com/en/latest/cephadm/install/&#34;&gt;https://docs.ceph.com/en/latest/cephadm/install/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>基于cephadm安装ceph</title>
      <link>https://Forest-L.github.io/post/k8s-ceph-install/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s-ceph-install/</guid>
      <description>&lt;h2 id=&#34;ceph介绍&#34;&gt;ceph介绍&lt;/h2&gt;
&lt;p&gt;ceph为块存储，以下通过cephadm安装ceph集群，ceph主要有三个基本角色：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OSD 用于集群中所有数据与对象的存储，处理集群数据的复制、恢复、回填、再均衡，并向其他osd守护进程发送心跳，然后向 Monitor 提供一些监控信息。&lt;/li&gt;
&lt;li&gt;Monitor 监控整个集群的状态，维护集群的 cluster MAP 二进制表，保证集群数据的一致性。&lt;/li&gt;
&lt;li&gt;MDS (可选) 为 Ceph 文件系统提供元数据计算、缓存与同步。MDS 进程并不是必须的进程，只有需要使用 CephFS 时，才需要配置 MDS 节点。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1环境准备&#34;&gt;1、环境准备&lt;/h2&gt;
&lt;h3 id=&#34;11节点规划&#34;&gt;1.1、节点规划&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;节点  &lt;/th&gt;
&lt;th&gt;os  &lt;/th&gt;
&lt;th&gt;IP   &lt;/th&gt;
&lt;th&gt;磁盘&lt;/th&gt;
&lt;th&gt;角色&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ceph1&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.13&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;cephadm，mgr, mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph2&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.14&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph3&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.16&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;12修改hosts文件&#34;&gt;1.2、修改hosts文件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hostnamectl set-hostname ceph1  #节点1上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph2  #节点2上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph3  #节点3上执行&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;13配置hosts&#34;&gt;1.3、配置hosts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt; /etc/hosts &amp;lt;&amp;lt;EOF  
192.168.0.13 ceph1
192.168.0.14 ceph2
192.168.0.16 ceph3
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;14关闭防火墙&#34;&gt;1.4、关闭防火墙&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都要执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld
setenforce 0
sed -i &#39;s/^SELINUX=.*/SELINUX=disabled/&#39; /etc/selinux/config
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;15时间同步&#34;&gt;1.5、时间同步&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;yum install -y chrony
systemctl enable --now chronyd

以ceph1作为时间参考，以下在三个节点都操作，添加server ceph1 prefer
vi /etc/chrony.conf
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
server ceph1 prefer

重启chrony
systemctl restart chronyd
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;16dockerlvm2和python3安装&#34;&gt;1.6、docker、lvm2和python3安装&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;三个节点都需要执行：
curl -fsSL get.docker.com -o get-docker.sh 
sh get-docker.sh
systemctl enable docker
systemctl restart docker

yum install -y python3

yum -y install lvm2
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2cephadm安装ceph1&#34;&gt;2、cephadm安装(ceph1)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;官方下载方法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;git下载&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;git clone --single-branch --branch octopus https://github.com/ceph/ceph.git&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cp ceph/src/cephadm/cephadm ./&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cephadm安装(ceph1)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;./cephadm add-repo --release octopus
./cephadm install

mkdir -p /etc/ceph
cephadm bootstrap --mon-ip 192.168.0.13

安装完成提示：

             URL: https://ceph1:8443/
            User: admin
        Password: 86yvswzdzd

INFO:cephadm:You can access the Ceph CLI with:

        sudo /usr/sbin/cephadm shell --fsid 3861dbf4-fd47-11ea-a373-5254228af870 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

INFO:cephadm:Please consider enabling telemetry to help improve Ceph:

        ceph telemetry on

For more information see:

        https://docs.ceph.com/docs/master/mgr/telemetry/

INFO:cephadm:Bootstrap complete.
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;配置ceph.pub
将ceph1的/etc/ceph/ceph.pub放入到ceph2和ceph3的.ssh/authorized_keys,ceph2和ceph3下root目录下没有.ssh，得先创建，最低权限为700，authorized_keys最低权限600。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ceph指令生效&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;cephadm shell&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;添加节点&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph3&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3osd部署&#34;&gt;3、OSD部署&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cephadm shell下执行&lt;/li&gt;
&lt;li&gt;若块设备没显示，可加&amp;ndash;refresh参数&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[ceph: root@ceph1 /]# ceph orch device ls
HOST   PATH      TYPE   SIZE  DEVICE_ID     MODEL  VENDOR  ROTATIONAL  AVAIL  REJECT REASONS
ceph1  /dev/vdc  hdd   50.0G  vol-e801hi3v  n/a    0x1af4  1           True
ceph1  /dev/vda  hdd   20.0G  i-2iwis9yr    n/a    0x1af4  1           False  locked
ceph1  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
ceph2  /dev/vdc  hdd   50.0G  vol-lqcchpox  n/a    0x1af4  1           True
ceph2  /dev/vda  hdd   20.0G  i-r53b2flp    n/a    0x1af4  1           False  locked
ceph2  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  locked, Insufficient space (&amp;lt;5GB)
ceph3  /dev/vdc  hdd    100G  vol-p0eksa5m  n/a    0x1af4  1           True
ceph3  /dev/vda  hdd   20.0G  i-f6njdmtq    n/a    0x1af4  1           False  locked
ceph3  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
[ceph: root@ceph1 /]#

devcice确认true后，执行
[ceph: root@ceph1 /]# ceph orch apply osd --all-available-devices
Scheduled osd.all-available-devices update...
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;查看集群状态,ceph方式及运行容器方式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;ceph方式：
[ceph: root@ceph1 /]# ceph -s
  cluster:
    id:     af1f33f0-fd69-11ea-8530-5254228af870
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 66s)
    mgr: ceph1.rbarkj(active, since 12m), standbys: ceph2.uzdnki
    osd: 3 osds: 3 up (since 14s), 3 in (since 14s)

  data:
    pools:   1 pools, 1 pgs
    objects: 1 objects, 0 B
    usage:   3.0 GiB used, 197 GiB / 200 GiB avail
    pgs:     1 active+clean

容器方式：
[root@ceph1 ceph]# docker ps -a
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES
b417841153a9        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-osd -…&amp;quot;   5 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-osd.2
9dbd720f4a6f        prom/prometheus:v2.18.1      &amp;quot;/bin/prometheus --c…&amp;quot;   6 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-prometheus.ceph1
f860282adf85        prom/alertmanager:v0.20.0    &amp;quot;/bin/alertmanager -…&amp;quot;   6 minutes ago       Up 6 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-alertmanager.ceph1
a6cb4b354f46        ceph/ceph-grafana:6.6.2      &amp;quot;/bin/sh -c &#39;grafana…&amp;quot;   16 minutes ago      Up 16 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-grafana.ceph1
b745f57895f3        prom/node-exporter:v0.18.1   &amp;quot;/bin/node_exporter …&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-node-exporter.ceph1
b00c1a2c7ef6        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-crash…&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-crash.ceph1
99c703f18f67        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mgr -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mgr.ceph1.rbarkj
e972d3d3f13a        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mon -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mon.ceph1

&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.ceph.com/en/latest/cephadm/install/&#34;&gt;https://docs.ceph.com/en/latest/cephadm/install/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>k8s-default-Storage-Class搭建</title>
      <link>https://Forest-L.github.io/post/k8s-default-storage-class-installer/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s-default-storage-class-installer/</guid>
      <description>&lt;h1 id=&#34;k8s-default-storage-class搭建&#34;&gt;k8s default Storage Class搭建&lt;/h1&gt;
&lt;p&gt;在k8s中，StorageClass为动态存储，存储大小设置不确定，对存储并发要求高和读写速度要求高等方面有很大优势；pv为静态存储，存储大小要确定。而default Storage Class的作用为pvc文件没有标识任何和storageclass相关联的信息，但通过annotations属性关联起来。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://Forest-L.github.io/img/storageclass.png&#34; alt=&#34;storageClass&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-创建storageclass&#34;&gt;1. 创建storageClass&lt;/h2&gt;
&lt;p&gt;要使用 StorageClass，我们就得安装对应的自动配置程序，比如我们这里存储后端使用的是 nfs，那么我们就需要使用到一个 nfs-client 的自动配置程序，我们也叫它 Provisioner，这个程序使用我们已经配置好的 nfs 服务器，来自动创建持久卷，也就是自动帮我们创建 PV。
nfs服务器参考博客&lt;a href=&#34;https://kubesphereio.com/post/linux-nfs-install/&#34;&gt;nfs服务器搭建&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;11-nfs-client-provisioner&#34;&gt;1.1 nfs-client-provisioner&lt;/h3&gt;
&lt;p&gt;前提：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes 1.9+&lt;/li&gt;
&lt;li&gt;Existing NFS Share&lt;/li&gt;
&lt;li&gt;helm&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;111-安装nfs-client-provisioner指令&#34;&gt;1.1.1 安装nfs-client-provisioner指令：&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;helm install --name nfs-client --set nfs.server=192.168.0.9 --set nfs.path=/nfsdatas stable/nfs-client-provisioner&lt;/code&gt;
如果安装报错，显示没有该helm的stable，在机器上添加helm 源
&lt;code&gt;helm repo add stable http://mirror.azure.cn/kubernetes/charts/&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;112-卸载指令&#34;&gt;1.1.2 卸载指令：&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;helm delete nfs-client&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;113-验证storageclass是否存在&#34;&gt;1.1.3 验证storageClass是否存在&lt;/h4&gt;
&lt;p&gt;在相应安装nfs-client-provisioner机器上执行：&lt;code&gt;kubectl get sc&lt;/code&gt;即可，如下所示：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@i-cj1r8a8m ~]# kubectl get sc
NAME              PROVISIONER                                   AGE
nfs-client    cluster.local/my-release-nfs-client-provisioner   1h
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2-defaultstorageclass&#34;&gt;2. DefaultStorageClass&lt;/h2&gt;
&lt;p&gt;在定义StorageClass时，可以在Annotation中添加一个键值对：storageclass.kubernetes.io/is-default-class: true，那么此StorageClass就变成默认的StorageClass了。&lt;/p&gt;
&lt;h3 id=&#34;21-第一种方法&#34;&gt;2.1 第一种方法&lt;/h3&gt;
&lt;p&gt;在这个PVC对象中添加一个声明StorageClass对象的标识，这里我们可以利用一个annotations属性来标识，如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvctest
  annotations:
    volume.beta.kubernetes.io/storage-class: &amp;quot;nfs-client&amp;quot;
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 10Mi
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;22-第二种方法&#34;&gt;2.2 第二种方法&lt;/h3&gt;
&lt;p&gt;用 kubectl patch 命令来更新：
&lt;code&gt;kubectl patch storageclass nfs-client -p &#39;{&amp;quot;metadata&amp;quot;: {&amp;quot;annotations&amp;quot;:{&amp;quot;storageclass.kubernetes.io/is-default-class&amp;quot;:&amp;quot;true&amp;quot;}}}&#39;&lt;/code&gt;
最后结果中包含default为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@i-cj1r8a8m ~]# kubectl get sc
NAME                 PROVISIONER                                   AGE
nfs-client (default)cluster.local/my-release-nfs-client-provisioner 2h
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>k8s-service-认知</title>
      <link>https://Forest-L.github.io/post/k8s-service-cognize/</link>
      <pubDate>Mon, 29 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s-service-cognize/</guid>
      <description></description>
    </item>
    
    <item>
      <title>k8s-ConfigMap认知</title>
      <link>https://Forest-L.github.io/post/k8s-configmap-cognize/</link>
      <pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s-configmap-cognize/</guid>
      <description>&lt;h1 id=&#34;k8s-configmap-认知&#34;&gt;k8s configMap 认知&lt;/h1&gt;
&lt;p&gt;许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息，而ConfigMap作用是保存配置信息，格式为键值对，可以单独一个key/value使用，也可以多个key/value构成的文件使用。数据不包含敏感信息的字符串。ConfigMap必须在Pod引用它之前创建;Pod只能使用同一个命名空间内的ConfigMap。&lt;/p&gt;
&lt;h2 id=&#34;1-常见configmap创建方式&#34;&gt;1 常见configMap创建方式&lt;/h2&gt;
&lt;h3 id=&#34;11-从key-value字符串创建configmap&#34;&gt;1.1 从key-value字符串创建ConfigMap&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl create configmap config1 --from-literal=config.test=good&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;12-从目录创建rootconfigmaptest1和rootconfigmaptest2-中&#34;&gt;1.2 从目录创建,/root/configmap/test1和/root/configmap/test2 中&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;vi test1
a:a1
vi test2
b:b1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create configmap special-config --from-file=/root/configmap/&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;13-通过pod形式创建最常用的方式configtestyaml内容如下&#34;&gt;1.3 通过pod形式创建,最常用的方式configtest.yaml内容如下：&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;kind: ConfigMap
apiVersion: v1
metadata:
  name: configtest
  namespace: default
data:
  test.property.1: a1
  test.property.2: b2
  test.property.file: |-
    property.1=a1
    property.2=b2
    property.3=c3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f configtest.yaml&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-常见查看configmap方式&#34;&gt;2. 常见查看configmap方式&lt;/h2&gt;
&lt;p&gt;以上面所示的第三种方式创建的configmap为例，名为：configtest。configmap可以简称cm。&lt;/p&gt;
&lt;h3 id=&#34;21--o-json格式&#34;&gt;2.1 &amp;ldquo;-o json&amp;quot;格式，&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl get cm configtest -o json&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;22-go模板的格式&#34;&gt;2.2 go模板的格式&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;-o go-template=&#39;{{.data}}&#39;格式
kubectl get configmap configtest -o go-template=&#39;{{.data}}&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3-常见使用configmap场景&#34;&gt;3. 常见使用configmap场景&lt;/h2&gt;
&lt;p&gt;通过多种方式在Pod中使用，比如设置环境变量、设置容器命令行参数、在Volume中创建配置文件等。
&lt;font color=#DC143C &gt;说 明 : &lt;/font&gt;
镜像以：buysbox镜像为例，如果能连国外的网，选择gcr.io/google_containers/busybox，不能连国外的网但能国内的外网使用：busybox:1.28.4这个镜像。
下面所有command里面都加了sleep,方便大家进容器查看配置是否起作用。&lt;/p&gt;
&lt;h3 id=&#34;31-configmap先创建好以下创建两种类型&#34;&gt;3.1 configmap先创建好,以下创建两种类型。&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm&lt;/code&gt;
&lt;code&gt;kubectl create configmap env-config --from-literal=log_level=INFO&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;32-环境参数注意busybox镜像选择及sleep时间test-podyaml&#34;&gt;3.2 环境参数，注意busybox镜像选择及sleep时间,test-pod.yaml。&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ &amp;quot;/bin/sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;sleep 60 ;env&amp;quot; ]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.type
      envFrom:
        - configMapRef:
            name: env-config
  restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f test-pod.yaml&lt;/code&gt;
进入对应的容器里，输入ENV则会包含如下结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SPECIAL_LEVEL_KEY=very
SPECIAL_TYPE_KEY=charm
log_level=INFO
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;33-命令行参数注意busybox镜像选择及sleep时间dapi-test-podyaml&#34;&gt;3.3 命令行参数，注意busybox镜像选择及sleep时间,dapi-test-pod.yaml。&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ &amp;quot;/bin/sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;sleep 60 ;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&amp;quot; ]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.type
  restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f dapi-test-pod.yaml&lt;/code&gt;
进入容器中，执行&lt;code&gt;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&lt;/code&gt;指令得到结果：
very charm&lt;/p&gt;
&lt;h3 id=&#34;34-volume将configmap作为文件或目录直接挂载注意busybox镜像选择及sleep时间当存在同名文件时直接覆盖掉vol-test-podyaml&#34;&gt;3.4 volume将ConfigMap作为文件或目录直接挂载，注意busybox镜像选择及sleep时间,当存在同名文件时，直接覆盖掉，vol-test-pod.yaml。&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: vol-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ &amp;quot;/bin/sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;sleep 60 ;cat /etc/config/special.how&amp;quot; ]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: special-config
  restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f vol-test-pod.yaml&lt;/code&gt;
进入容器，cat的结果：very&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>glusterfs安装</title>
      <link>https://Forest-L.github.io/post/k8s-glusterfs-install/</link>
      <pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/k8s-glusterfs-install/</guid>
      <description>&lt;h2 id=&#34;ceph介绍&#34;&gt;ceph介绍&lt;/h2&gt;
&lt;p&gt;ceph为块存储，以下通过cephadm安装ceph集群，ceph主要有三个基本角色：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OSD 用于集群中所有数据与对象的存储，处理集群数据的复制、恢复、回填、再均衡，并向其他osd守护进程发送心跳，然后向 Monitor 提供一些监控信息。&lt;/li&gt;
&lt;li&gt;Monitor 监控整个集群的状态，维护集群的 cluster MAP 二进制表，保证集群数据的一致性。&lt;/li&gt;
&lt;li&gt;MDS (可选) 为 Ceph 文件系统提供元数据计算、缓存与同步。MDS 进程并不是必须的进程，只有需要使用 CephFS 时，才需要配置 MDS 节点。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1环境准备&#34;&gt;1、环境准备&lt;/h2&gt;
&lt;h3 id=&#34;11节点规划&#34;&gt;1.1、节点规划&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;节点  &lt;/th&gt;
&lt;th&gt;os  &lt;/th&gt;
&lt;th&gt;IP   &lt;/th&gt;
&lt;th&gt;磁盘&lt;/th&gt;
&lt;th&gt;角色&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ceph1&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.13&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;cephadm，mgr, mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph2&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.14&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph3&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.16&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;12修改hosts文件&#34;&gt;1.2、修改hosts文件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hostnamectl set-hostname ceph1  #节点1上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph2  #节点2上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph3  #节点3上执行&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;13配置hosts&#34;&gt;1.3、配置hosts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt; /etc/hosts &amp;lt;&amp;lt;EOF  
192.168.0.13 ceph1
192.168.0.14 ceph2
192.168.0.16 ceph3
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;14关闭防火墙&#34;&gt;1.4、关闭防火墙&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都要执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld
setenforce 0
sed -i &#39;s/^SELINUX=.*/SELINUX=disabled/&#39; /etc/selinux/config
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;15时间同步&#34;&gt;1.5、时间同步&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;yum install -y chrony
systemctl enable --now chronyd

以ceph1作为时间参考，以下在三个节点都操作，添加server ceph1 prefer
vi /etc/chrony.conf
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
server ceph1 prefer

重启chrony
systemctl restart chronyd
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;16dockerlvm2和python3安装&#34;&gt;1.6、docker、lvm2和python3安装&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;三个节点都需要执行：
curl -fsSL get.docker.com -o get-docker.sh 
sh get-docker.sh
systemctl enable docker
systemctl restart docker

yum install -y python3

yum -y install lvm2
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2cephadm安装ceph1&#34;&gt;2、cephadm安装(ceph1)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;官方下载方法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;git下载&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;git clone --single-branch --branch octopus https://github.com/ceph/ceph.git&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cp ceph/src/cephadm/cephadm ./&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cephadm安装(ceph1)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;./cephadm add-repo --release octopus
./cephadm install

mkdir -p /etc/ceph
cephadm bootstrap --mon-ip 192.168.0.13

安装完成提示：

             URL: https://ceph1:8443/
            User: admin
        Password: 86yvswzdzd

INFO:cephadm:You can access the Ceph CLI with:

        sudo /usr/sbin/cephadm shell --fsid 3861dbf4-fd47-11ea-a373-5254228af870 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

INFO:cephadm:Please consider enabling telemetry to help improve Ceph:

        ceph telemetry on

For more information see:

        https://docs.ceph.com/docs/master/mgr/telemetry/

INFO:cephadm:Bootstrap complete.
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;配置ceph.pub
将ceph1的/etc/ceph/ceph.pub放入到ceph2和ceph3的.ssh/authorized_keys,ceph2和ceph3下root目录下没有.ssh，得先创建，最低权限为700，authorized_keys最低权限600。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ceph指令生效&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;cephadm shell&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;添加节点&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph3&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3osd部署&#34;&gt;3、OSD部署&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cephadm shell下执行&lt;/li&gt;
&lt;li&gt;若块设备没显示，可加&amp;ndash;refresh参数&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[ceph: root@ceph1 /]# ceph orch device ls
HOST   PATH      TYPE   SIZE  DEVICE_ID     MODEL  VENDOR  ROTATIONAL  AVAIL  REJECT REASONS
ceph1  /dev/vdc  hdd   50.0G  vol-e801hi3v  n/a    0x1af4  1           True
ceph1  /dev/vda  hdd   20.0G  i-2iwis9yr    n/a    0x1af4  1           False  locked
ceph1  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
ceph2  /dev/vdc  hdd   50.0G  vol-lqcchpox  n/a    0x1af4  1           True
ceph2  /dev/vda  hdd   20.0G  i-r53b2flp    n/a    0x1af4  1           False  locked
ceph2  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  locked, Insufficient space (&amp;lt;5GB)
ceph3  /dev/vdc  hdd    100G  vol-p0eksa5m  n/a    0x1af4  1           True
ceph3  /dev/vda  hdd   20.0G  i-f6njdmtq    n/a    0x1af4  1           False  locked
ceph3  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
[ceph: root@ceph1 /]#

devcice确认true后，执行
[ceph: root@ceph1 /]# ceph orch apply osd --all-available-devices
Scheduled osd.all-available-devices update...
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;查看集群状态,ceph方式及运行容器方式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;ceph方式：
[ceph: root@ceph1 /]# ceph -s
  cluster:
    id:     af1f33f0-fd69-11ea-8530-5254228af870
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 66s)
    mgr: ceph1.rbarkj(active, since 12m), standbys: ceph2.uzdnki
    osd: 3 osds: 3 up (since 14s), 3 in (since 14s)

  data:
    pools:   1 pools, 1 pgs
    objects: 1 objects, 0 B
    usage:   3.0 GiB used, 197 GiB / 200 GiB avail
    pgs:     1 active+clean

容器方式：
[root@ceph1 ceph]# docker ps -a
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES
b417841153a9        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-osd -…&amp;quot;   5 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-osd.2
9dbd720f4a6f        prom/prometheus:v2.18.1      &amp;quot;/bin/prometheus --c…&amp;quot;   6 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-prometheus.ceph1
f860282adf85        prom/alertmanager:v0.20.0    &amp;quot;/bin/alertmanager -…&amp;quot;   6 minutes ago       Up 6 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-alertmanager.ceph1
a6cb4b354f46        ceph/ceph-grafana:6.6.2      &amp;quot;/bin/sh -c &#39;grafana…&amp;quot;   16 minutes ago      Up 16 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-grafana.ceph1
b745f57895f3        prom/node-exporter:v0.18.1   &amp;quot;/bin/node_exporter …&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-node-exporter.ceph1
b00c1a2c7ef6        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-crash…&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-crash.ceph1
99c703f18f67        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mgr -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mgr.ceph1.rbarkj
e972d3d3f13a        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mon -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mon.ceph1

&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.ceph.com/en/latest/cephadm/install/&#34;&gt;https://docs.ceph.com/en/latest/cephadm/install/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Linux中nfs搭建</title>
      <link>https://Forest-L.github.io/post/linux-nfs-install/</link>
      <pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/linux-nfs-install/</guid>
      <description>&lt;h1 id=&#34;linux-下nfs-服务器的搭建及配置&#34;&gt;Linux 下NFS 服务器的搭建及配置&lt;/h1&gt;
&lt;p&gt;nfs是网络存储文件系统，客户端通过网络访问不同主机上磁盘的数据，用于unix系统。&lt;/p&gt;
&lt;p&gt;演示nfs机器信息，分为服务端和客户端介绍，以下服务端的ip请根据自己环境来替换。
服务端：192.168.0.9，客户端：192.168.0.10&lt;/p&gt;
&lt;h2 id=&#34;11服务端安装19216809&#34;&gt;1.1服务端安装（192.168.0.9）&lt;/h2&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;centos:&lt;/font&gt;
&lt;code&gt;sudo yum install nfs-utils -y&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;ubuntu16.04:&lt;/font&gt;
&lt;code&gt;sudo apt install nfs-kernel-server&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;12服务端配置&#34;&gt;1.2服务端配置&lt;/h2&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;centos:&lt;/font&gt;
rpcbind服务的开机自启和启动：
&lt;code&gt;sudo systemctl enable rpcbind;sudo systemctl restart rpcbind&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;nfs服务的开机自启和启动：
&lt;code&gt;sudo systemctl enable nfs;sudo systemctl restart nfs&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;ubuntu16.04:&lt;/font&gt;
&lt;code&gt;sudo systemctl enable nfs-kernel-server;sudo systemctl restart nfs-kernel-server&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;13配置共享目录&#34;&gt;1.3配置共享目录&lt;/h2&gt;
&lt;p&gt;目录为服务端目录，后续存储的数据在该目录下
&lt;code&gt;sudo mkdir -p /nfsdatas&lt;/code&gt;
&lt;code&gt;sudo chmod 755 /nfsdatas&lt;/code&gt;
&lt;font color=#DC143C &gt;重要:&lt;/font&gt; 根据上面创建的目录，相应配置导出目录
&lt;code&gt;sudo vi /etc/exports&lt;/code&gt;
添加如下配置再保存，重启nfs服务即可：
/nfsdatas/ 192.168.0.0/16(rw,sync,no_root_squash,no_all_squash)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;/nfsdatas:共享目录位置。&lt;/li&gt;
&lt;li&gt;192.168.0.0/24：客户端IP范围，*代表所有。&lt;/li&gt;
&lt;li&gt;rw：权限设置，可读可写。&lt;/li&gt;
&lt;li&gt;sync：同步共享目录。&lt;/li&gt;
&lt;li&gt;no_root_squash: 可以使用root授权。&lt;/li&gt;
&lt;li&gt;no_all_squash: 可以使用普通用户授权&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;centos:&lt;/font&gt;
&lt;code&gt;systemctl restart nfs&lt;/code&gt;
&lt;font color=#DC143C &gt;ubuntu16.04:&lt;/font&gt;
&lt;code&gt;sudo systemctl restart nfs-kernel-server&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;21客户端安装192168010&#34;&gt;2.1客户端安装（192.168.0.10）&lt;/h2&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;centos:&lt;/font&gt;
&lt;code&gt;sudo yum install nfs-utils -y&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;ubuntu16.04:&lt;/font&gt;
&lt;code&gt;sudo apt install nfs-common&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;22客户端开机自启和启动即可&#34;&gt;2.2客户端开机自启和启动即可&lt;/h2&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;centos:&lt;/font&gt;
&lt;code&gt;sudo systemctl enable rpcbind&lt;/code&gt;
&lt;code&gt;sudo systemctl start rpcbind&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;ubuntu16.04:&lt;/font&gt;
&lt;code&gt;sudo systemctl enable nfs-common;sudo systemctl restart nfs-common&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;23客户端验证及测试&#34;&gt;2.3客户端验证及测试&lt;/h2&gt;
&lt;p&gt;检查服务端的共享目录：
&lt;code&gt;showmount -e 192.168.0.9&lt;/code&gt;
客户端创建目录
&lt;code&gt;sudo mkdir -p /tmp/nfsdata&lt;/code&gt;
挂载指令：
&lt;code&gt;sudo mount -t nfs 192.168.0.9:/nfsdatas /tmp/nfsdata&lt;/code&gt;
然后进入/tmp/nfsdata目录下，新建文件
&lt;code&gt;sudo touch test&lt;/code&gt;
之后在nfs服务端192.168.0.9的/nfsdatas目录查看是否有test文件
卸载指令：不要在/tmp/nfsdata目录下执行卸载指令。
&lt;code&gt;umount /tmp/nfsdata&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;cenots和ubuntu脚本部署&#34;&gt;cenots和ubuntu脚本部署&lt;/h2&gt;
&lt;p&gt;下面内容添加：vi nfs-install.sh
加权限和执行：chmod +x nfs-install.sh &amp;amp;&amp;amp; ./nfs-install.sh
以下脚本安装的服务端目录为：/nfsdatas,如果需要修改的话，脚本内容需要修改。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/bash

function centostest(){
    yum clean all;yum makecache
    yum install nfs-utils -y
    systemctl enable rpcbind;sudo systemctl restart rpcbind
    systemctl enable nfs;sudo systemctl restart nfs
    mkdir -p /nfsdatas
    chmod 755 /nfsdatas
    echo &amp;quot;/nfsdatas/ 192.168.0.0/16(rw,sync,no_root_squash,no_all_squash)&amp;quot; &amp;gt; /etc/exports
    systemctl restart nfs
    showmount -e localhost
    if [[ $? -eq 0 ]]; then
        #statements
        str=&amp;quot;successsful!&amp;quot;
        echo -e &amp;quot;\033[30;47m$str\033[0m&amp;quot;  
    else
        str=&amp;quot;failed!&amp;quot;
        echo -e &amp;quot;\033[31;47m$str\033[0m&amp;quot;
        exit
    fi
}

function ubuntutest(){
    apt-get update
    sudo apt install nfs-kernel-server
    sudo systemctl enable nfs-kernel-server;sudo systemctl restart nfs-kernel-server
    mkdir -p /nfsdatas
    chmod 755 /nfsdatas
    echo &amp;quot;/nfsdatas/ 192.168.0.0/16(rw,sync,no_root_squash,no_all_squash)&amp;quot; &amp;gt; /etc/exports
    sudo systemctl restart nfs-kernel-server
    showmount -e localhost
    if [[ $? -eq 0 ]]; then
        #statements
        str=&amp;quot;successsful!&amp;quot;
        echo -e &amp;quot;\033[30;47m$str\033[0m&amp;quot;  
    else
        str=&amp;quot;failed!&amp;quot;
        echo -e &amp;quot;\033[31;47m$str\033[0m&amp;quot;
        exit
    fi
}

cat /etc/redhat-release

if [[ $? -eq 0 ]]; then
    str=&amp;quot;centos!&amp;quot;
    echo -e &amp;quot;\033[30;47m$str\033[0m&amp;quot;
    centostest
else
    str=&amp;quot;ubuntu!&amp;quot;
    echo -e &amp;quot;\033[31;47m$str\033[0m&amp;quot;
    ubuntutest
fi
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Markdown语法入门</title>
      <link>https://Forest-L.github.io/post/markdown-syntax/</link>
      <pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/markdown-syntax/</guid>
      <description>&lt;p&gt;This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.&lt;/p&gt;
&lt;h2 id=&#34;headings&#34;&gt;Headings&lt;/h2&gt;
&lt;p&gt;The following HTML &lt;code&gt;&amp;lt;h1&amp;gt;&lt;/code&gt;—&lt;code&gt;&amp;lt;h6&amp;gt;&lt;/code&gt; elements represent six levels of section headings. &lt;code&gt;&amp;lt;h1&amp;gt;&lt;/code&gt; is the highest section level while &lt;code&gt;&amp;lt;h6&amp;gt;&lt;/code&gt; is the lowest.&lt;/p&gt;
&lt;h1 id=&#34;h1&#34;&gt;H1&lt;/h1&gt;
&lt;h2 id=&#34;h2&#34;&gt;H2&lt;/h2&gt;
&lt;h3 id=&#34;h3&#34;&gt;H3&lt;/h3&gt;
&lt;h4 id=&#34;h4&#34;&gt;H4&lt;/h4&gt;
&lt;h5 id=&#34;h5&#34;&gt;H5&lt;/h5&gt;
&lt;h6 id=&#34;h6&#34;&gt;H6&lt;/h6&gt;
&lt;h2 id=&#34;paragraph&#34;&gt;Paragraph&lt;/h2&gt;
&lt;p&gt;Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.&lt;/p&gt;
&lt;p&gt;Itatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.&lt;/p&gt;
&lt;h2 id=&#34;blockquotes&#34;&gt;Blockquotes&lt;/h2&gt;
&lt;p&gt;The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a &lt;code&gt;footer&lt;/code&gt; or &lt;code&gt;cite&lt;/code&gt; element, and optionally with in-line changes such as annotations and abbreviations.&lt;/p&gt;
&lt;h4 id=&#34;blockquote-without-attribution&#34;&gt;Blockquote without attribution&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Tiam, ad mint andaepu dandae nostion secatur sequo quae.
&lt;strong&gt;Note&lt;/strong&gt; that you can use &lt;em&gt;Markdown syntax&lt;/em&gt; within a blockquote.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;blockquote-with-attribution&#34;&gt;Blockquote with attribution&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Don&amp;rsquo;t communicate by sharing memory, share memory by communicating.&lt;/p&gt;
— &lt;cite&gt;Rob Pike&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;tables&#34;&gt;Tables&lt;/h2&gt;
&lt;p&gt;Tables aren&amp;rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Age&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Bob&lt;/td&gt;
&lt;td&gt;27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Alice&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;inline-markdown-within-tables&#34;&gt;Inline Markdown within tables&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Inline   &lt;/th&gt;
&lt;th&gt;Markdown   &lt;/th&gt;
&lt;th&gt;In   &lt;/th&gt;
&lt;th&gt;Table&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;italics&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;bold&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;del&gt;strikethrough&lt;/del&gt;   &lt;/td&gt;
&lt;td&gt;&lt;code&gt;code&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;code-blocks&#34;&gt;Code Blocks&lt;/h2&gt;
&lt;h4 id=&#34;code-block-with-backticks&#34;&gt;Code block with backticks&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;html
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang=&amp;quot;en&amp;quot;&amp;gt;
&amp;lt;head&amp;gt;
  &amp;lt;meta charset=&amp;quot;UTF-8&amp;quot;&amp;gt;
  &amp;lt;title&amp;gt;Example HTML5 Document&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
  &amp;lt;p&amp;gt;Test&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;code-block-indented-with-four-spaces&#34;&gt;Code block indented with four spaces&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang=&amp;quot;en&amp;quot;&amp;gt;
&amp;lt;head&amp;gt;
  &amp;lt;meta charset=&amp;quot;UTF-8&amp;quot;&amp;gt;
  &amp;lt;title&amp;gt;Example HTML5 Document&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
  &amp;lt;p&amp;gt;Test&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;code-block-with-hugos-internal-highlight-shortcode&#34;&gt;Code block with Hugo&amp;rsquo;s internal highlight shortcode&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-html&#34; data-lang=&#34;html&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;!DOCTYPE html&amp;gt;&lt;/span&gt;
&amp;lt;&lt;span style=&#34;color:#f92672&#34;&gt;html&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;lang&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;en&amp;#34;&lt;/span&gt;&amp;gt;
&amp;lt;&lt;span style=&#34;color:#f92672&#34;&gt;head&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span style=&#34;color:#f92672&#34;&gt;meta&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;charset&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;UTF-8&amp;#34;&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span style=&#34;color:#f92672&#34;&gt;title&lt;/span&gt;&amp;gt;Example HTML5 Document&amp;lt;/&lt;span style=&#34;color:#f92672&#34;&gt;title&lt;/span&gt;&amp;gt;
&amp;lt;/&lt;span style=&#34;color:#f92672&#34;&gt;head&lt;/span&gt;&amp;gt;
&amp;lt;&lt;span style=&#34;color:#f92672&#34;&gt;body&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span style=&#34;color:#f92672&#34;&gt;p&lt;/span&gt;&amp;gt;Test&amp;lt;/&lt;span style=&#34;color:#f92672&#34;&gt;p&lt;/span&gt;&amp;gt;
&amp;lt;/&lt;span style=&#34;color:#f92672&#34;&gt;body&lt;/span&gt;&amp;gt;
&amp;lt;/&lt;span style=&#34;color:#f92672&#34;&gt;html&lt;/span&gt;&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;list-types&#34;&gt;List Types&lt;/h2&gt;
&lt;h4 id=&#34;ordered-list&#34;&gt;Ordered List&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;First item&lt;/li&gt;
&lt;li&gt;Second item&lt;/li&gt;
&lt;li&gt;Third item&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;unordered-list&#34;&gt;Unordered List&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;List item&lt;/li&gt;
&lt;li&gt;Another item&lt;/li&gt;
&lt;li&gt;And another item&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;nested-list&#34;&gt;Nested list&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Item&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;First Sub-item&lt;/li&gt;
&lt;li&gt;Second Sub-item&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;other-elements--abbr-sub-sup-kbd-mark&#34;&gt;Other Elements — abbr, sub, sup, kbd, mark&lt;/h2&gt;
&lt;p&gt;&lt;abbr title=&#34;Graphics Interchange Format&#34;&gt;GIF&lt;/abbr&gt; is a bitmap image format.&lt;/p&gt;
&lt;p&gt;H&lt;sub&gt;2&lt;/sub&gt;O&lt;/p&gt;
&lt;p&gt;X&lt;sup&gt;n&lt;/sup&gt; + Y&lt;sup&gt;n&lt;/sup&gt; = Z&lt;sup&gt;n&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Press &lt;kbd&gt;&lt;kbd&gt;CTRL&lt;/kbd&gt;+&lt;kbd&gt;ALT&lt;/kbd&gt;+&lt;kbd&gt;Delete&lt;/kbd&gt;&lt;/kbd&gt; to end the session.&lt;/p&gt;
&lt;p&gt;Most &lt;mark&gt;salamanders&lt;/mark&gt; are nocturnal, and hunt for insects, worms, and other small creatures.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The above quote is excerpted from Rob Pike&amp;rsquo;s &lt;a href=&#34;https://www.youtube.com/watch?v=PAAkCSZUG1c&#34;&gt;talk&lt;/a&gt; during Gopherfest, November 18, 2015. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</description>
    </item>
    
    <item>
      <title>终端录屏软件入门</title>
      <link>https://Forest-L.github.io/post/asciinema/</link>
      <pubDate>Tue, 05 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/asciinema/</guid>
      <description>&lt;h2 id=&#34;1-asciinema-终端录屏&#34;&gt;1. asciinema 终端录屏&lt;/h2&gt;
&lt;p&gt;asciinema是一个在终端下非常棒的录制分享软件，基于文本的录屏工具，对终端输入输出进行捕捉， 然后以文本的形式来记录和回放！对多种系统都支持。&lt;/p&gt;
&lt;h2 id=&#34;2-asciinema安装&#34;&gt;2. asciinema安装&lt;/h2&gt;
&lt;p&gt;各种安装方法如连接，以下按centos为例进行：
&lt;a href=&#34;https://asciinema.org/docs/installation&#34;&gt;https://asciinema.org/docs/installation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;yum install -y epel-release &lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;yum install -y asciinema&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;检查是否成功，看asciinema版本&lt;/p&gt;
&lt;p&gt;&lt;code&gt;asciinema --version&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3-指令及常见用法&#34;&gt;3. 指令及常见用法&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;[root@i-cvswezkk ~]# asciinema --help
usage: asciinema [-h] [--version] {rec,play,upload,auth} ...

Record and share your terminal sessions, the right way.

positional arguments:
  {rec,play,upload,auth}
    rec         Record terminal session                                 # 记录终端会话
    play        Replay terminal session                                 # 播放重播终端会话
    upload      Upload locally saved terminal session to asciinema.org  #上传本地保存的终端会话到asciinema.org
    auth                Manage recordings on asciinema.org account      # 管理asciinema.org帐户上的记录

optional arguments:
  -h, --help            show this help message and exit
  --version             show program&#39;s version number and exit
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;31-各个指令具体含义&#34;&gt;3.1 各个指令具体含义：&lt;/h3&gt;
&lt;p&gt;.cast和.json文件是一样的，注册asciinema就需要邮箱即可，然后邮箱认证即可，可能收到邮箱的时间长点。
记录终端并将其上传到asciinema.org&lt;/p&gt;
&lt;p&gt;&lt;code&gt;asciinema rec&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;将终端记录到本地文件&lt;/p&gt;
&lt;p&gt;&lt;code&gt;asciinema rec demo.cast&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;记录终端并将其上传到asciinema.org，指定标题：&amp;ldquo;my aslog1&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;asciinema rec -t &amp;quot;my aslog1&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;将终端记录到本地文件，将空闲时间限制到最大2.5秒&lt;/p&gt;
&lt;p&gt;&lt;code&gt;asciinema rec -i 2.5 demo.cast&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;从本地文件重放终端记录&lt;/p&gt;
&lt;p&gt;&lt;code&gt;asciinema play demo.cast&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;asciinema还提供了一个可以管理asciinema个人账户所拥有的会话文件的功能, 命令为&lt;/p&gt;
&lt;p&gt;&lt;code&gt;asciinema auth&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;执行命令&amp;quot;asciinema auth&amp;quot;命令后, 会返回一个网络地址, 点击这个地址就会打开asciinema个人账号注册界面
本地修改记录文件&lt;/p&gt;
&lt;p&gt;&lt;code&gt;vi demo.cast&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;本地修改记录文件再重新上传至asciinema.org，&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# asciinema upload /root/260132.json
View the recording at:

    https://asciinema.org/a/M1K9rrUHl5D0q3TOVDhtRcJIn
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;4浏览器访问效果&#34;&gt;4.浏览器访问效果&lt;/h2&gt;
&lt;p&gt;双击对应的一个录屏，有share  download settings，且settings可以设置参数
&lt;img src=&#34;https://Forest-L.github.io/img/asciinema.png&#34; alt=&#34;asciinema-demo效果图&#34;&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title></title>
      <link>https://Forest-L.github.io/post/terraform-docking-qingcloud-installation-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://Forest-L.github.io/post/terraform-docking-qingcloud-installation-k8s/</guid>
      <description>&lt;p&gt;title = &amp;ldquo;terraform对接qingcloud安装K8s&amp;rdquo;
date = &amp;ldquo;2019-08-13&amp;rdquo;
tags = [
&amp;ldquo;iptables&amp;rdquo;,
&amp;ldquo;Linux tools&amp;rdquo;
]
+++&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用Terraform作用是不需要用户在iaas平台上单独创建机器，配置好参数之后由Terraform自动创建，实现一键自动化部署。&lt;/li&gt;
&lt;li&gt;单节点的安装主要分为三部分，var.tf、kubesphere.tf和install.sh，以下文件内容只需要修改API密钥值就可以部署Ks集群。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;执行步骤说明&#34;&gt;执行步骤说明&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;安装terraform工具&lt;/li&gt;
&lt;li&gt;创建一个目录，把var.tf、kubesphere.tf和install.sh三个文件放到该目录下。&lt;/li&gt;
&lt;li&gt;修改iaas的API密码&lt;/li&gt;
&lt;li&gt;进入目录下执行&lt;code&gt;terraform init&lt;/code&gt;指令，显示成功。&lt;/li&gt;
&lt;li&gt;init成功之后，然后执行&lt;code&gt;terraform apply&lt;/code&gt;即可就开始创建机器，安装Ks。&lt;/li&gt;
&lt;li&gt;删除机器指令操作为&lt;code&gt;terraform destroy&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;1terraform安装单独一台机器&#34;&gt;1、terraform安装（单独一台机器）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;以centos操作系统为例来安装，需要执行以下指令即可。&lt;/li&gt;
&lt;li&gt;其余操作系统，参考&lt;a href=&#34;https://learn.hashicorp.com/tutorials/terraform/install-cli?in=terraform/aws-get-started&#34;&gt;terraform&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sudo yum install -y yum-utils
sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo
sudo yum -y install terraform
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;是否安装成功及版本输出&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;terraform version&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;2vartf文件说明&#34;&gt;2、var.tf文件说明&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;access_key和secret_key为必填项。点击iaas用户&amp;ndash;》API密钥&amp;ndash;》创建即可，然后把两个参数填入到下面的配置文件中。zone可以根据自己需求来修改，默认pek3a。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;terraform {
  required_providers {
    qingcloud = {
      source = &amp;quot;shaowenchen/qingcloud&amp;quot;
      version = &amp;quot;1.2.6&amp;quot;
    }
  }
}

variable &amp;quot;access_key&amp;quot; {
  default = &amp;quot;***&amp;quot;
}

variable &amp;quot;secret_key&amp;quot; {
  default = &amp;quot;***&amp;quot;
}

variable &amp;quot;zone&amp;quot; {
  default = &amp;quot;pek3a&amp;quot;
}

provider &amp;quot;qingcloud&amp;quot; {
  access_key = &amp;quot;${var.access_key}&amp;quot;
  secret_key = &amp;quot;${var.secret_key}&amp;quot;
  zone = &amp;quot;${var.zone}&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;3kubespheretf文件说明&#34;&gt;3、kubesphere.tf文件说明&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;resource qingcloud_eip为创建外网ip，也可以用已存在外网IP，如果用已存在外网ip，就不需要qingcloud_eip resource模块。&lt;/li&gt;
&lt;li&gt;qingcloud_security_group为创建防火墙，也可以使用已存在防火墙，同理。&lt;/li&gt;
&lt;li&gt;qingcloud_security_group_rule为创建防火墙开放的端口。&lt;/li&gt;
&lt;li&gt;qingcloud_keypair为密钥创建，此处注释掉，用密码形式。&lt;/li&gt;
&lt;li&gt;qingcloud_instance创建机器，包含名字，操作系统，内存，cpu，磁盘，密码，绑定外网IP，防火墙，子网和类型等。&lt;/li&gt;
&lt;li&gt;null_resource 和install_kubesphere包含文件拷贝及执行命令。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;resource &amp;quot;qingcloud_eip&amp;quot; &amp;quot;init&amp;quot;{
  name = &amp;quot;tf_eip&amp;quot;
  description = &amp;quot;&amp;quot;
  billing_mode = &amp;quot;traffic&amp;quot;
  bandwidth = 20
  need_icp = 0
}

resource &amp;quot;qingcloud_security_group&amp;quot; &amp;quot;basic&amp;quot;{
  name = &amp;quot;防火墙&amp;quot;
  description = &amp;quot;这是第一个防火墙&amp;quot;
}

resource &amp;quot;qingcloud_security_group_rule&amp;quot; &amp;quot;openport&amp;quot; {
  security_group_id = &amp;quot;${qingcloud_security_group.basic.id}&amp;quot;
  protocol = &amp;quot;tcp&amp;quot;
  priority = 0
  action = &amp;quot;accept&amp;quot;
  direction = 0
  from_port = 22
  to_port = 40000
}

# qingcloud_keypair upload an SSH public key
# In this example, upload ~/.ssh/id_rsa.pub content.
# You may not have this file in your system, you will need to create your own SSH key.
#resource &amp;quot;qingcloud_keypair&amp;quot; &amp;quot;arthur&amp;quot;{
#  name = &amp;quot;arthur&amp;quot;
#  public_key = &amp;quot;${file(&amp;quot;~/.ssh/id_rsa.pub&amp;quot;)}&amp;quot;
#}

resource &amp;quot;qingcloud_instance&amp;quot; &amp;quot;init&amp;quot;{
  count = 1
  name = &amp;quot;master-${count.index}&amp;quot;
  image_id = &amp;quot;centos76x64a&amp;quot;
  cpu = &amp;quot;16&amp;quot;
  memory = &amp;quot;32768&amp;quot;
  instance_class = &amp;quot;0&amp;quot;
  managed_vxnet_id=&amp;quot;vxnet-0&amp;quot;
#  keypair_ids = [&amp;quot;${qingcloud_keypair.arthur.id}&amp;quot;]
  login_passwd = &amp;quot;Qcloud@123&amp;quot;
  security_group_id =&amp;quot;${qingcloud_security_group.basic.id}&amp;quot;
  eip_id = &amp;quot;${qingcloud_eip.init.id}&amp;quot;
}

resource &amp;quot;null_resource&amp;quot; &amp;quot;install_kubesphere&amp;quot; {
  provisioner &amp;quot;file&amp;quot; {
    destination = &amp;quot;./install.sh&amp;quot;
    source      = &amp;quot;./install.sh&amp;quot;

    connection {
      type        = &amp;quot;ssh&amp;quot;
      user        = &amp;quot;root&amp;quot;
      host        = &amp;quot;${qingcloud_eip.init.addr}&amp;quot;
      password    = &amp;quot;Qcloud@123&amp;quot;
#      private_key = &amp;quot;${file(&amp;quot;~/.ssh/id_rsa&amp;quot;)}&amp;quot;
      port        = &amp;quot;22&amp;quot;
    }
  }

  provisioner &amp;quot;remote-exec&amp;quot; {
    inline = [
      &amp;quot;sh install.sh&amp;quot;
    ]

    connection {
      type        = &amp;quot;ssh&amp;quot;
      user        = &amp;quot;root&amp;quot;
      host        = &amp;quot;${qingcloud_eip.init.addr}&amp;quot;
      password    = &amp;quot;Qcloud@123&amp;quot;
 #     private_key = &amp;quot;${file(&amp;quot;~/.ssh/id_rsa&amp;quot;)}&amp;quot;
      port        = &amp;quot;22&amp;quot;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;4installsh文件说明&#34;&gt;4、install.sh文件说明&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;具体执行命令&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;curl -O -k https://kubernetes.pek3b.qingstor.com/tools/kubekey/kk
chmod +x kk
yum install -y vim openssl socat conntrack ipset
echo -e &#39;yes\n&#39; | /root/kk create cluster --with-kubesphere
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;5执行及删除&#34;&gt;5、执行及删除&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;进入目录，执行init，出现successfully字样。&lt;/li&gt;
&lt;li&gt;terraform apply, 输入yes开始安装。&lt;/li&gt;
&lt;li&gt;terraform destroy&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[root@node4 ~]# cd terraform
[root@node4 terraform]# ls
install.sh  kubesphere.tf  var.tf
[root@node4 terraform]# terraform init

Initializing the backend...

Initializing provider plugins...
Terraform has been successfully initialized!

[root@node4 terraform]# terraform apply

An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create
Enter a value: yes
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;6参考&#34;&gt;6、参考&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/yunify/terraform-provider-qingcloud&#34;&gt;https://github.com/yunify/terraform-provider-qingcloud&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>