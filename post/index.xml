<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 李林博客</title>
    <link>https://kubesphereio.com/post/</link>
    <description>Recent content in Posts on 李林博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <copyright>Copyright © 2008–2020</copyright>
    <lastBuildDate>Tue, 12 Jan 2021 20:03:26 +0800</lastBuildDate>
    
	<atom:link href="https://kubesphereio.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>K8s中单体服务设计模式</title>
      <link>https://kubesphereio.com/post/singleton-service/</link>
      <pubDate>Tue, 12 Jan 2021 20:03:26 +0800</pubDate>
      
      <guid>https://kubesphereio.com/post/singleton-service/</guid>
      <description>&lt;p&gt;Singleton服务模式确保了应用的一个实例同时只有一个是激活的，但又是高度可用的。这种模式可以从应用内部实现，也可以完全委托给Kubernetes。&lt;/p&gt;
&lt;h2 id=&#34;存在问题&#34;&gt;存在问题&lt;/h2&gt;
&lt;p&gt;Kubernetes提供的主要功能之一是能够轻松透明地扩展应用。Pods可以通过单一命令（如kubectl scale）强制扩展，或通过控制器定义（如ReplicaSet）声明性扩展，甚至可以根据应用负载Elastic Scale动态扩展。通过运行同一服务的多个实例（不是Kubernetes服务，而是以Pod为代表的分布式应用的一个组件），系统通常会增加吞吐量和可用性。可用性增加的原因是，如果一个服务实例变得不健康，请求调度器会将未来的请求转发给其他健康的实例。在Kubernetes中，多个实例是一个Pod的复本，Service资源负责请求调度。&lt;/p&gt;
&lt;p&gt;但是，在某些情况下，一次只允许运行一个服务的实例。 例如，如果一个服务中有一个周期性执行的任务，而同一服务又有多个实例，那么每个实例都会在预定的时间间隔内触发任务，导致重复，而不是像预期的那样只有一个任务被触发。另一个例子是对特定资源（文件系统或数据库）执行轮询的服务，我们要确保只有一个实例，甚至可能只有一个线程执行轮询和处理。第三种情况发生在我们要用一个单线程的消费者从消息经纪人那里依次消费消息，这个消费者也是一个单人服务。&lt;/p&gt;
&lt;p&gt;在所有这些和类似的情况下，我们需要对每一次一个激活的服务多少个实例（通常只需要一个）进行一些控制，不管有多少个实例被启动并保持运行。&lt;/p&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;p&gt;运行同一个Pod的多个副本会创建一个主-主的拓扑，其中一个服务的所有实例都是主的。我们需要的是一种主-被（或主从）拓扑，其中只有一个实例是主的，而其他所有实例都是被的。从根本上说，这可以在两个可能的层次上实现：应用外锁定和应用内锁定。&lt;/p&gt;
&lt;h4 id=&#34;应用外锁&#34;&gt;应用外锁&lt;/h4&gt;
&lt;p&gt;顾名思义，这种机制依赖于应用程序之外的管理进程，以确保应用程序只有一个实例在运行。应用程序的实现本身并不知道这个约束，而是作为一个单体实例运行。从这个角度来看，它类似于拥有一个Java类，它只被管理运行时（如Spring框架）实例化一次。类的实现并不知道它是作为单例运行的，也不知道它包含任何代码构造来防止实例化多个实例。图1-1显示了如何借助StatefulSet或ReplicaSet控制器与一个副本实现应用外锁定。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;out-of-application-locking.png&#34; alt=&#34;out-of-application-locking.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;在Kubernetes中实现的方法是用一个副本启动一个Pod。单单这个活动并不能保证单体Pod的高可用。我们要做的是还要用一个控制器（比如ReplicaSet）来支持Pod，将单体Pod变成一个高可用的单体。这种拓扑结构并不完全是主-动（没有被动实例），但效果是一样的，因为Kubernetes保证了Pod的一个实例一直在运行。此外，单体Pod实例是高度可用的，这要归功于控制器在Pod出现故障时执行健康检查、HealthProbe和愈合。&lt;/p&gt;
&lt;p&gt;这种方式主要需要注意的是副本数，不要一不小心就增加了，因为没有平台级的机制来防止副本数的变化。&lt;/p&gt;
&lt;p&gt;任何时候都只有一个实例在运行，这并不完全正确，尤其是当事情出错时。Kubernetes 基元（如 ReplicaSet）倾向于可用性而非一致性&amp;ndash;这是为了实现高可用和可扩展的分布式系统而做出的慎重决定。这意味着ReplicaSet对其副本采用 &amp;ldquo;至少 &amp;ldquo;而非 &amp;ldquo;最多 &amp;ldquo;的语义。如果我们将ReplicaSet配置为具有副本的单例：1，控制器确保至少有一个实例一直在运行，但偶尔也可以有更多的实例。&lt;/p&gt;
&lt;p&gt;这里最常见的情况是，当一个带有控制器管理的Pod的节点变得不健康并与Kubernetes集群的其他节点断开连接时。在这种情况下，ReplicaSet控制器会在一个健康的节点上启动另一个Pod实例（假设有足够的容量），而不确保断开连接的节点上的Pod被关闭。同样，当改变副本数量或将Pod迁移到不同节点时，Pod的数量可能会暂时超过所需数量。这种临时增加的目的是为了确保高可用性和避免中断，这是无状态和可扩展应用的需要。&lt;/p&gt;
&lt;p&gt;单例可以具有弹性和恢复能力，但根据定义，它不是高可用的。单子通常倾向于一致性而非可用性。Kubernetes资源同样倾向于一致性而非可用性，并提供所需的严格单例保证是StatefulSet。如果ReplicaSets不能为你的应用提供所需的保证，而你又有严格的单例要求，StatefulSets可能是答案。StatefulSets旨在为有状态的应用程序提供许多特性，包括更强的单例保证，但它们也增加了复杂性。我们将讨论有关单例的问题，并在第后续章Stateful Service中更详细地介绍StatefulSets。&lt;/p&gt;
&lt;p&gt;通常情况下，在Kubernetes上的Pod中运行的单体应用会打开与消息中介、关系型数据库、文件服务器或其他Pod上运行的系统或外部系统的传出连接。然而，偶尔，你的单例Pod可能需要接受传入连接，在Kubernetes上启用的方式是通过Service资源。&lt;/p&gt;
&lt;p&gt;我们在下面章 &amp;ldquo;服务发现 &amp;ldquo;中对Kubernetes服务进行了深入的介绍，但我们在这里简单讨论一下适用于单体的部分。一个普通的Service（类型为：ClusterIP）会创建一个虚拟IP，并在其选择器匹配的所有Pod实例中执行负载均衡。但是通过StatefulSet管理的单例Pod只有一个Pod和一个稳定的网络身份。在这种情况下，最好创建一个无头服务(通过设置type: ClusterIP和clusterIP: None)。之所以称为无头，是因为这样的Service没有虚拟IP地址，kube-proxy不处理这些Service，平台也不执行代理。&lt;/p&gt;
&lt;p&gt;然而，这样的服务仍然是有用的，因为带有选择器的无头服务在API服务器中创建端点记录，并为匹配的Pod生成DNS A记录，这样，服务的DNS查询就不会返回它的虚拟IP，而是返回支持Pod的IP地址。这样就可以通过服务的DNS记录直接访问单例Pod，而不需要通过服务的虚拟IP。例如，如果我们创建了一个名为my-singleton的无头服务，我们可以使用my-singleton.default.svc.cluster.local来直接访问Pod的IP地址。&lt;/p&gt;
&lt;p&gt;综上所述，对于非严格的单例来说，一个有一个副本的ReplicaSet和一个普通的Service就足够了。对于严格的单例和性能更好的服务发现，最好使用StatefulSet和无头Service。你可以在后面章Stateful Service中找到一个完整的例子，在这里你必须将副本的数量改为一个，使其成为一个单例。&lt;/p&gt;
&lt;h4 id=&#34;应用内锁&#34;&gt;应用内锁&lt;/h4&gt;
&lt;p&gt;在分布式环境中，控制服务实例数量的方法之一是通过分布式锁，如图1-2所示。每当一个服务实例或实例内部的组件被激活时，它都可以尝试获取一个锁，如果成功了，服务就会成为活动状态。任何后续的服务实例如果未能获取锁，则会等待并不断尝试获取锁，以防当前激活的服务释放锁。&lt;/p&gt;
&lt;p&gt;许多现有的分布式框架使用这种机制来实现高可用性和弹性。例如，消息中间件Apache ActiveMQ可以在一个高可用的主-被拓扑中运行，其中数据源提供共享锁。第一个启动的中间件实例获得锁并成为主，随后启动的其他实例则成为被，等待锁被释放。这种策略可以确保有一个单一的主中间件实例，同时也能抵御故障的发生。
图1-2所示应用内锁&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;application-in-lock.png&#34; alt=&#34;application-in-lock.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;我们可以将这种策略与面向对象中的经典单例进行比较：单例是一个存储在静态类变量中的对象实例。在这个实例中，该类意识到自己是一个单例，而且它的编写方式不允许为同一个进程实例化多个实例。在分布式系统中，这意味着容器化应用程序本身必须以一种不允许同时有多个活动实例的方式来编写，无论启动的Pod实例数量有多少。要在分布式环境中实现这一点，首先，我们需要一个分布式锁的实现，比如Apache ZooKeeper、HashiCorp的Consul、Redis或Etcd提供的锁。&lt;/p&gt;
&lt;p&gt;ZooKeeper的典型实现是使用临时节点，只要有客户端会话就存在，一旦会话结束就会被删除。第一个启动的服务实例在ZooKeeper服务器上发起一个会话，并创建一个临时节点成为活动节点。同一个集群的所有其他服务实例都会变成被的，必须等待临时节点被释放。这就是基于ZooKeeper的实现如何确保整个集群中只有一个主服务实例，确保主/被的故障转移行为。&lt;/p&gt;
&lt;p&gt;在Kubernetes的世界里，与其仅仅为了锁定功能而管理ZooKeeper集群，不如使用通过Kubernetes API暴露的、运行在主节点上的Etcd功能。Etcd是一个分布式键值存储，它使用Raft协议来维护其副本状态。最重要的是，它为实现领导者选举提供了必要的构件，一些客户端库已经实现了这个功能。例如，Apache Camel有一个Kubernetes连接器，它也提供了领导者选举和单人能力。这个连接器更进一步，它没有直接访问Etcd API，而是使用Kubernetes API来利用ConfigMaps作为分布式锁。它依靠Kubernetes乐观的锁定保证来编辑ConfigMaps等资源，一次只能更新一个Pod的ConfigMap。&lt;/p&gt;
&lt;p&gt;Camel的实现使用这个保证来确保只有一个Camel路由实例是活动的，其他实例必须等待并获得锁才能激活。这是对锁的自定义实现，但实现了同样的目标：当有多个Pods使用同一个Camel应用时，只有其中一个成为主单体，其他单体在从模式下等待。&lt;/p&gt;
&lt;p&gt;使用ZooKeeper、Etcd或其他任何分布式锁的实现将与所述的类似：只有一个应用实例成为领导者并激活自己，其他从实例等待锁。这就保证了即使启动了多个Pod副本，并且都是健康的、启动的、运行的，也只有一个服务是主动的，并作为单例执行业务功能，其他实例都在等待获取锁，以防主控失败或关闭。&lt;/p&gt;
&lt;h4 id=&#34;pod中断的安排&#34;&gt;Pod中断的安排&lt;/h4&gt;
&lt;p&gt;单体服务和领导者选举试图限制一个服务同时运行的最大实例数量，而Kubernetes的Pod DisruptionBudget功能则提供了一个互补的、有点相反的功能&amp;ndash;限制同时停机维护的实例数量。&lt;/p&gt;
&lt;p&gt;在它的核心，PodDisruptionBudget确保一定数量或百分比的Pod不会在任何一个时间点上自愿从一个节点上被驱逐。这里的自愿是指可以延迟特定时间的驱逐，例如，当它是由维护或升级的节点耗尽（kubectl drain），或集群缩减触发的，而不是节点变得不健康，这无法预测或控制。&lt;/p&gt;
&lt;p&gt;例1-1中的PodDisruptionBudget适用于与其选择器相匹配的Pod，并确保两个Pod必须一直可用。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;例1-1 PodDisruptionBudget
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: random-generator-pdb
spec:
  selector:
    matchLabels:
      app: reandom-generator
  minAvailable: 2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;除了.spec.minAvailable，还有一个选项是使用.spec.maxUnavailable，它指定了该集的Pods数量，可以在驱逐后不可用。但是你不能同时指定这两个字段，PodDisruptionBudget通常只适用于由控制器管理的Pod。对于不由控制器管理的花苞（也被称为裸露或裸露的Pods），应该考虑围绕PodDisruptionBudget的其他限制。&lt;/p&gt;
&lt;p&gt;该功能对于基于法定人数的应用非常有用，这些应用要求在任何时候都有最少数量的副本运行以确保法定人数。或者当一个应用程序正在服务于关键流量，而这些流量永远不应该低于实例总数的某个百分比。这是Kubernetes另一个控制和影响运行时实例管理的基元，在本章值得一提。&lt;/p&gt;
&lt;h2 id=&#34;讨论&#34;&gt;讨论&lt;/h2&gt;
&lt;p&gt;如果你的用例需要强大的单例保证，你就不能依赖ReplicaSets的应用外锁定机制。Kubernetes ReplicaSets的设计是为了维护其Pod的可用性，而不是为了确保Pod的最多单例语义。因此，有很多故障场景（例如，当运行单体Pod的节点与集群的其他节点分区时，例如用新的Pod实例替换删除的Pod实例时），一个Pod的两个副本在短时间内并发运行。如果不能接受，请使用StatefulSets或研究应用程序中的锁定选项，这些选项可以为您提供更多的控制领导者选举过程，并提供更强的保证。后者还可以防止通过改变副本数量来意外扩展Pod。&lt;/p&gt;
&lt;p&gt;在其他情况下，容器化应用程序中只有一部分应该是单例。例如，可能有一个容器化应用程序提供了一个HTTP端点，该端点可以安全地扩展到多个实例，但也有一个轮询组件必须是一个单例。使用应用外锁定的方法将防止对整个服务进行扩展。同时，作为结果，我们要么在其部署单元中拆分单体人组件，使其保持单体身份（理论上是好的，但并不总是实用的，值得开销），要么使用应用内锁定机制，只锁定必须是单体的组件。这将允许我们透明地扩展整个应用，让HTTP端点进行扩展，并让其他部分作为主-被单体。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>K8s中Daemon服务设计模式</title>
      <link>https://kubesphereio.com/post/daemon-service/</link>
      <pubDate>Sat, 09 Jan 2021 18:30:33 +0800</pubDate>
      
      <guid>https://kubesphereio.com/post/daemon-service/</guid>
      <description>&lt;p&gt;Daemon Service模式允许在目标节点上调度和运行优先级高的、专注于基础设施的Pod。管理员主要使用它来运行特定于节点的Pods，以增强Kubernetes平台的功能。&lt;/p&gt;
&lt;h2 id=&#34;存在问题&#34;&gt;存在问题&lt;/h2&gt;
&lt;p&gt;软件系统中守护进程的概念存在于许多层面。在操作系统的层面上，守护进程是一个长期运行、自我恢复的计算机程序，它作为后台进程运行。在Unix中，守护进程的名称以 &amp;ldquo;d &amp;ldquo;结尾，如httpd、named和sshd。在其他操作系统中，则使用服务启动的任务和幽灵作业等替代术语。&lt;/p&gt;
&lt;p&gt;不管它们被称为什么，这些程序的共同特点是它们作为进程运行，通常不与显示器、键盘和鼠标交互，并且在系统启动时启动。在应用程序层面也存在类似的概念。例如，在JVM中守护线程在后台运行，为用户线程提供支持服务。这些守护线程的优先级较低，在后台运行，在应用程序的生命周期没有发言权，执行任务类似垃圾收集或结束。&lt;/p&gt;
&lt;p&gt;同样，Kubernetes中也有DaemonSet的概念。考虑到Kubernetes是一个分布式平台，分布在多个节点上，以管理应用Pods为主要目标，因此DaemonSet由运行在集群节点上的Pods代表，并为集群的其他节点提供一些后台功能。&lt;/p&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;p&gt;ReplicaSet和它的前身ReplicationController是负责确保特定数量的Pods运行的控制结构。这些控制器不断地监控运行中的Pod的列表，并确保实际的Pod数量总是与期望的数量相匹配。在这方面，DaemonSet是一个类似的结构，负责确保一定数量的Pods始终在运行。不同的是，前两者运行特定数量的Pod，通常是由高可用性和用户负载的应用需求驱动，而不考虑节点数量。&lt;/p&gt;
&lt;p&gt;另一方面，DaemonSet不是由消费者负载驱动决定运行多少个Pod实例和在哪里运行时。它的主要目的是在每个节点或特定节点上保持运行一个Pod。接下来让我们在例1-1中看到这样一个DaemonSet的定义。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1.1 DaemonSet 实例
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata: 
  name: random-refresher
spec:
  selector:
    matchLabels:
      app: random-refresher
  template:
    metadata:
      labels:
        app: random-refresher
    spec: 
      nodeSelector:
        feature: hw-rng 
      containers:
      - image: k8spatterns/random-generator:1.0
        name: random-generator
        command:
        - sh
        - -c
        - &amp;gt;-
          &amp;quot;while true; do
          java -cp / RandomRunner /host_dev/random 100000;
          sleep 30; done&amp;quot;
        volumeMounts:
        - mountPath: /host_dev
          name: devices
        volumes:
        - name: devices
          hostPath:
            path: /dev
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;考虑到这种行为，DaemonSet的主要候选者通常是与基础设施相关的进程，如日志收集器、度量导出器，甚至是kube-proxy，这些进程会执行整个集群的操作。DaemonSet和ReplicaSet的管理方式有很多不同，但主要有以下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;默认情况下，DaemonSet会给每个节点调度一个Pod实例。这可以通过使用nodeSelector字段来控制和限制节点的子集。&lt;/li&gt;
&lt;li&gt;DaemonSet创建的Pod已经指定了nodeName。因此，DaemonSet不需要Kubernetes调度器的存在就可以运行容器。这也允许使用DaemonSet来运行和管理Kubernetes组件。&lt;/li&gt;
&lt;li&gt;由DaemonSet创建的Pod可以在调度器启动之前运行，这使得它们可以在任何其他Pod被调度在节点上之前运行。&lt;/li&gt;
&lt;li&gt;由DaemonSet管理的Pods应该只在目标节点上运行，因此，被许多控制器以更高的优先级和不同的方式对待。例如，descheduler会避免驱逐这类Pod，集群autoscaler会单独管理它们等等。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通常，一个DaemonSet会在每个节点或节点子集上创建一个单一的Pod。鉴于此，DaemonSets管理的Pod有几种方式可以到达。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Service&lt;br&gt;
创建一个与DaemonSet相同的Pod选择器的Service，并使用该Service到达一个守护者Pod负载均衡到随机节点。&lt;/li&gt;
&lt;li&gt;DNS&lt;br&gt;
创建一个无头服务，其Pod选择器与DaemonSet相同，可用于从DNS中检索包含所有Pod IP和端口的多个A记录。&lt;/li&gt;
&lt;li&gt;NodeIP with HostPort&lt;br&gt;
在DaemonSet中的Pod可以指定一个hostPort，并通过节点IP地址和指定的端口进行访问。由于hostIp和hostPort以及协议的组合必须是唯一的，所以Pod可以被调度的地方数量是有限的。&lt;/li&gt;
&lt;li&gt;Push&lt;br&gt;
DaemonSets Pods中的应用可以将数据推送到Pods外部的指定位置或服务。不需要消费者到达DaemonSets Pods。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另一种类似于DaemonSet运行容器的方式是通过静态Pods机制。Kubelet除了与Kubernetes APIServer对话并获取Pod清单外，还可以从本地目录获取资源定义。这样定义的Pod只由Kubelet管理，并且只在一个节点上运行。API服务并不观察这些Pod，也没有控制器，也没有对它们进行健康检查。Kubelet观察这样的Pod，并在它们崩溃时重新启动它们。同样，Kubelet也会定期扫描配置的目录，查看Pod定义的变化，并相应地添加或删除Pod。&lt;/p&gt;
&lt;p&gt;静态Pods可以用来分拆Kubernetes系统进程或其他容器的容器化版本。但DaemonSets与平台的其他部分集成度更好，推荐使用DaemonSets，而不是静态Pods。&lt;/p&gt;
&lt;h2 id=&#34;讨论&#34;&gt;讨论&lt;/h2&gt;
&lt;p&gt;我们描述的模式和Kubernetes特性主要是由开发者而不是平台管理员使用的。DaemonSet介于两者之间，更倾向于管理员工具箱，但我们把它包括在这里，因为它对应用开发者也有适用性。DaemonSets和CronJobs也是Kubernetes如何将Crontab和守护脚本等单节点概念转化为管理分布式系统的多节点集群基元的完美例子。这些都是开发人员也必须熟悉的新的分布式概念。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>K8s中定时Job设计模式</title>
      <link>https://kubesphereio.com/post/cronjob/</link>
      <pubDate>Sat, 09 Jan 2021 17:44:37 +0800</pubDate>
      
      <guid>https://kubesphereio.com/post/cronjob/</guid>
      <description>&lt;p&gt;定时Job模式通过添加时间维度扩展了批次Job模式，并允许由当时事件触发执行一个工作单元。&lt;/p&gt;
&lt;h2 id=&#34;存在问题&#34;&gt;存在问题&lt;/h2&gt;
&lt;p&gt;在分布式系统和微服务的世界里，使用HTTP和轻量级消息传递的实时和事件驱动的应用交互是一个明显的趋势。然而，无论软件开发的最新趋势如何，Job调度有着悠久的历史，而且它仍然具有相关性。==定时Job通常用于自动化系统维护或管理任务==。它们也适用于需要定期执行特定任务的业务应用，这里的典型例子是通过文件传输进行业务对业务的集成，通过数据库轮询进行应用集成，发送新闻信邮件，以及清理和归档旧文件。&lt;/p&gt;
&lt;p&gt;对于系统维护而言，传统方法处理周期性Job是使用专门的调度软件或Cron。然而，对于简单的用例来说，专门的软件可能会很昂贵，而且在单一服务器上运行的Cron作业很难维护，而且是一个单一的故障点。这就是为什么，很多时候，开发人员倾向于实现既能处理调度方面的问题，又能处理需要执行的业务逻辑的解决方案。例如，在Java世界中，Quartz、Spring Batch等库以及带有ScheduledThreadPoolExecutor类的自定义实现都可以运行时间性任务。但与Cron类似，这种方法的主要难点是使调度能力具有弹性和高可用，从而导致资源的高消耗。另外，采用这种方法，基于时间的任务调度器是应用程序的一部分，要使调度器高度可用，整个应用程序必须高度可用。通常情况下，这涉及到运行多个应用实例，同时要保证只有一个实例是活跃的，并调度作业&amp;ndash;这就涉及到领导者选举和其他分布式系统的挑战。&lt;/p&gt;
&lt;p&gt;最后，一个简单的服务，每天要复制几个文件一次，最后可能需要多个节点，分布式的领导选举机制等等。Kubernetes CronJob的实现解决了这些问题，它允许使用著名的Cron格式对Job资源进行调度，让开发者只专注于实现要执行的工作，而不是时间调度方面。&lt;/p&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;p&gt;在Batch Job中，我们看到了Kubernetes Jobs的用例和功能。所有这些也适用于本章，因为CronJob基元建立在Job之上。CronJob实例类似于Unix crontab（cron表）的一行，管理Job的时间方面。它允许在指定的时间点周期性地执行一个Job。见例1-1的示例定义。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1-1. CronJob
apiVersion: batch/v1beta1
kind: CronJob
metadata: 
  name: random-generator
spec: 
  # Every three minutes
  schedule:&amp;quot;*/3****&amp;quot;
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - image: k8spatterns/random-generator:1.0
            name: random-generator command: [ &amp;quot;java&amp;quot;, &amp;quot;-cp&amp;quot;, &amp;quot;/&amp;quot;, &amp;quot;RandomRunner&amp;quot;, &amp;quot;/numbers.txt&amp;quot;, &amp;quot;10000&amp;quot; ]
          restartPolicy: OnFailure
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;除了Job规格外，CronJob还有额外的字段来定义它的时间方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;.spec.schedule&lt;br&gt;
Crontab 条目用于指定Job的计划（例如，0 * * * * 表示每小时运行）。&lt;/li&gt;
&lt;li&gt;.spec.startingDeadlineSeconds&lt;br&gt;
如果任务错过了预定时间，启动任务的最后期限（以秒为单位）。在某些使用情况下，一个任务只有在一定的时间范围内执行才有效，如果执行晚了就没有用了。例如，如果一个Job因为缺乏计算资源或其他缺失的依赖关系而没有在预期的时间内执行，那么最好跳过一次执行，因为它应该处理的数据已经过时了。&lt;/li&gt;
&lt;li&gt;.spec.concurrencyPolicy&lt;br&gt;
指定如何管理同一 CronJob 创建的作业的并发执行。默认行为Allow会创建新的Job实例，即使之前的Job还没有完成。如果这不是所需的行为，可以在当前作业尚未完成的情况下，使用 Forbidor 跳过下一次运行，取消当前正在运行的作业，并使用 Replace 启动一个新的作业。&lt;/li&gt;
&lt;li&gt;.spec.suspend&lt;br&gt;
暂停所有后续执行，但不影响已经开始的执行。&lt;/li&gt;
&lt;li&gt;.spec.successfulJobsHistoryLimit and .spec.failedJobsHistoryLimit&lt;br&gt;
指定应保留多少个已完成和未完成的Job的字段，以便进行审计。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CronJob是一个非常专业的基元，它只适用于工作单元具有时间维度的情况。即使CronJob不是一个通用的基元，它也是一个很好的例子，说明Kubernetes的能力是如何建立在彼此之上，并且也支持非云原生用例。&lt;/p&gt;
&lt;h2 id=&#34;讨论&#34;&gt;讨论&lt;/h2&gt;
&lt;p&gt;正如您所看到的，CronJob是一个非常简单的基元，它向现有的Job定义添加了集群、类似Cron的行为。但当它与其他基元（如 Pods、容器资源隔离）和其他 Kubernetes 特性（如 ，Automated Placement、Health Probe）相结合时，它往往会成为一个非常强大的 Job 调度系统。这使得开发者可以只关注问题域，实现一个只负责要执行的业务逻辑的容器化应用。调度是在应用之外进行的，作为平台的一部分，它具有所有的附加优势，如高可用性、弹性、容量和策略驱动的Pod调度。当然，与Job的实现类似，在实现CronJob容器时，你的应用必须考虑重复运行、不运行、并行运行或取消运行的所有角落和故障情况。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>K8s中批量Job设计模式</title>
      <link>https://kubesphereio.com/post/batch-job/</link>
      <pubDate>Sun, 03 Jan 2021 16:00:45 +0800</pubDate>
      
      <guid>https://kubesphereio.com/post/batch-job/</guid>
      <description>&lt;p&gt;Batch Job模式适合管理孤立的原子工作单元。它基于Job抽象，在分布式环境中可靠地运行短暂的Pod，直到完成。&lt;/p&gt;
&lt;h2 id=&#34;存在问题&#34;&gt;存在问题&lt;/h2&gt;
&lt;p&gt;Kubernetes中管理和运行容器的主要基元是Pod。创建Pod的方式有不同的特点。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bare Pod&lt;br&gt;
可以手动创建一个Pod来运行容器。然而，当这种Pod运行的节点出现故障时，Pod不会被重新启动。不鼓励以这种方式运行Pod，除非出于开发或测试目的。这种机制也被称为非托管或裸露的Pod。&lt;/li&gt;
&lt;li&gt;ReplicaSet&lt;br&gt;
该控制器用于创建和管理预期连续运行的Pod的生命周期（例如，运行一个Web服务器容器）。在任何给定时间中，它维护一组稳定的副本Pods运行，并保证指定数量相同的Pods可用。&lt;/li&gt;
&lt;li&gt;DaemonSet&lt;br&gt;
控制器以单个Pod方式运行在每个节点上。通常用于管理平台功能，如监控、日志收集、存储容器等。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些Pod的一个共同点是，它们代表了长期运行的进程，并不是要在一段时间后停止。然而，在某些情况下，需要可靠地执行一个预定义的限定的工作，然后关闭容器。对于这个任务，Kubernetes提供了Job资源。&lt;/p&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;p&gt;Kubernetes Job类似于ReplicaSet，因为它创建了一个或多个Pod，并确保它们成功运行。然而，不同的是，一旦预期数量的Pod成功终止，该作业就被认为是完成的，不再启动额外的Pod。一个Job定义看起来像例 1-1。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1.1 Job实例
apiVersion: batch/v1
kind: Job
metadata:
  name: random-generator
spec:
  completions: 5
  parallelism: 2
  template:
    metadata:
      name: random-generator
    spec:
      restartPolicy: OnFailure
      containers:
      - image: k8spatterns/random-generator:1.0
        name: random-generator
        command: [ &amp;quot;java&amp;quot;, &amp;quot;-cp&amp;quot;, &amp;quot;/&amp;quot;, &amp;quot;RandomRunner&amp;quot;, &amp;quot;/numbers.txt&amp;quot;, &amp;quot;10000&amp;quot; ]
        
        
Job应该运行五个Pods来完成，这五个Pods必须全部成功。
有两个pod是并行的。
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Job 和 ReplicaSet 定义之间的一个重要区别是 .spec.template.spec.restartPolicy。ReplicaSet的默认值是Always，这对于必须始终保持运行的长期进程来说是有意义的。对于一个Job来说，不允许使用 Always 值，唯一可能的选项是 OnFailure 或Never。&lt;/p&gt;
&lt;p&gt;那么，为什么还要创建一个Job及只运行一次Pod，而不是使用裸Pod呢？使用Job提供了许多可靠性和可扩展性的优势，使其成为首选。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个Job不是一个短暂的内存中的任务，而是一个持久的任务，可以在集群重启后存活下来。&lt;/li&gt;
&lt;li&gt;当一个作业完成时，它不会被删除，但会被保留以用于跟踪。作为Job的一部分创建的Pods也不会被删除，但可用于检查（例如，检查容器日志）。对于裸露的Pods也是如此，但只适用于restartPolicy: OnFailure。&lt;/li&gt;
&lt;li&gt;一个Job可能需要执行多次。使用.spec.completions字段可以指定一个Pod在Job本身完成之前应该成功完成多少次。&lt;/li&gt;
&lt;li&gt;当一个Job必须多次完成时（通过.spec.completions设置），它也可以通过同时启动多个Pod来扩展和执行。这可以通过指定.spec.parallelism字段来实现。&lt;/li&gt;
&lt;li&gt;如果节点出现故障，或者当Pod因某种原因被驱逐，而仍在运行时，调度器会将Pod调度在一个新的健康节点上并重新运行。裸露的Pod将保持在失败的状态，因为现有的Pod永远不会被移动到其他节点上。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有这些都使得Job基元对于那些需要对单位工作的完成进行一些保证的场景具有吸引力。在Job的行为中起主要作用的两个字段是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;.spec.completions&lt;br&gt;
指定应运行多少个Pods来完成一个Job。&lt;/li&gt;
&lt;li&gt;.spec.parallelism&lt;br&gt;
指定多少个Pod副本可以并行运行。设置较高的数字并不能保证较高的并行性，实际的Pod数量可能仍然少于（在某些特殊的情况下，更多）所需的数量（例如，由于节流、资源配额、剩余的完成量不够以及其他原因）。将此字段设置为 0，可以有效地暂停作业。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;图1-1显示了例1-1中定义的完成数为5，并行度为2的Batch Job的处理方式。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kubesphereio.com/img/batch-job.png&#34; alt=&#34;批量job.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;根据这两个参数，有以下几种类型的Job：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Single Pod Job&lt;br&gt;
当您不考虑.spec.completions和.spec.parallelism或将它们设置为默认值1时，就会选择这种类型。这样的Job只启动一个Pod，一旦单个Pod成功终止（退出代码为0），就会完成。&lt;/li&gt;
&lt;li&gt;Fixed completion count Jobs&lt;br&gt;
当你指定.spec.completions的数字大于1时，这个数量的Pod必须成功。您可以选择设置.spec.parallelism，或者将其保留为默认值1。这样的Job在.spec.completions数量的Pods成功完成后，就被认为是完成了。例1-1展示了这种模式的运行情况，当我们事先知道Jobs的数量，并且单个工作项的处理成本证明了使用专用Pod的合理性时，这是最好的选择。&lt;/li&gt;
&lt;li&gt;Work queue Jobs&lt;br&gt;
当您不使用 .spec.completions 并将 .spec.parallelism 设置为大于 1 的整数时，您就拥有了一个并行Job的工作队列。当至少有一个Pod成功终止，并且所有其他Pod也终止时，一个工作队列Job就被认为已经完成。这种设置需要Pods之间相互协调，确定每个Pods正在进行的工作，这样才能以协调的方式完成。例如，当队列中存储了固定但未知数量的工作项目时，并行的Pods可以逐个拾取这些项目进行工作。第一个检测到队列为空并成功退出的Pod表示Job完成。Job控制器也会等待所有其他Pod终止。由于一个Pod处理多个工作项目，这种Job类型是细化工作项目的最佳选择&amp;ndash;当每个工作项目的一个Pod的开销是不合理的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果你有无限的工作项流需要处理，其他控制器（如ReplicaSet）是管理处理这些工作项目的Pod的更好选择。&lt;/p&gt;
&lt;h2 id=&#34;讨论&#34;&gt;讨论&lt;/h2&gt;
&lt;p&gt;Job抽象是一个非常基本但也是基本的基元，其他基元（如 CronJobs）都是基于这个基元。Job有助于将孤立的工作单元转化为可靠和可扩展的执行单元。然而，Job 并不决定如何将可单独处理的工作项映射到 Jobs 或 Pods 中。这是你必须在考虑每个选项的利弊后确定的事情。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One Job per work item&lt;br&gt;
这个选项有创建Kubernetes Jobs的开销，也为平台管理大量消耗资源的Job。当每个工作项目都是一个复杂的任务时，必须记录、跟踪或缩放时，这个选项很有用。&lt;/li&gt;
&lt;li&gt;One Job for all work items&lt;br&gt;
这个选项适用于大量的工作项目，这些项目不需要由平台独立跟踪和管理。在这种情况下，工作项目必须通过批处理框架从应用程序内部进行管理。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Job 基元只为工作项目的调度提供了最基本的基础知识。任何复杂的实现都必须将Job基元与批处理应用框架（例如，在Java生态系统中，我们将Spring Batch和JBeret作为标准实现）结合起来，以达到预期的结果。&lt;/p&gt;
&lt;p&gt;不是所有的服务都必须一直运行。有些服务必须按需运行，有些必须在特定的时间运行，有些必须定期运行。使用Jobs可以只在需要的时候运行Pod，并且只在任务执行期间运行。Jobs被安排在具有所需容量的节点上，满足Pod调度策略和其他容器依赖性考虑。使用Jobs来执行短时任务，而不是使用长期运行的抽象（如ReplicaSet），可以为平台上的其他工作负载节省资源。所有这些都使得Jobs成为一个独特的基元，而Kubernetes则是一个支持多样化工作负载的平台。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>K8s中自动调度设计模式</title>
      <link>https://kubesphereio.com/post/automated-placement/</link>
      <pubDate>Sat, 02 Jan 2021 18:15:51 +0800</pubDate>
      
      <guid>https://kubesphereio.com/post/automated-placement/</guid>
      <description>&lt;p&gt;Automated Placement是Kubernetes调度器的核心功能，用于将新的Pod分配给满足容器资源请求的节点，并遵从调度策略。该模式描述了Kubernetes的调度算法的原理以及从外部影响调度决策的方式。&lt;/p&gt;
&lt;h2 id=&#34;存在问题&#34;&gt;存在问题&lt;/h2&gt;
&lt;p&gt;一个合理规模的基于微服务的系统由几十个甚至几百个独立的进程组成。容器和Pods确实为打包和部署提供了很好的抽象实例，但并不能解决将这些进程调度在合适节点上的问题。随着微服务数量的庞大和不断增长，将它们单独分配和调度到节点上并不是一个可以管理的活动。&lt;/p&gt;
&lt;p&gt;容器之间有依赖性，对节点的依赖性，还有资源需求，所有这些也会随着时间的推移而变化。集群上的可用资源也会随着时间的推移而变化，通过收缩或扩展集群，或者被已经调度的容器消耗掉。我们调度容器的方式也会影响分布式系统的可用性、性能和容量。所有这些都使得将容器调度到节点上成为一个变化的目标，必须在变化中确定下来。&lt;/p&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;p&gt;在Kubernetes中，将Pod分配给节点是由调度器完成的。截至本文写作时，这是一个可配置性很强、仍在不断发展、变化很快的领域。在本章中，我们将介绍主要的调度控制机制、影响调度的驱动力、为什么要选择一种或另一种方案，以及由此产生的后果。Kubernetes调度器是一个有效且省时的工具。它在整个Kubernetes平台中起着基础性的作用，但与其他Kubernetes组件（API Server、Kubelet）类似，它可以单独运行，也可以完全不使用。&lt;/p&gt;
&lt;p&gt;在一个很高的层次上，Kubernetes调度器执行的主要操作是从API Server上监控每个新创建的Pod定义，并将其分配给一个节点。它为每一个Pod找到一个合适的节点（只要有这样的节点），无论是最初的应用调度、扩容，还是将应用从一个不健康的节点转移到一个更健康的节点时。它还考虑运行时的依赖性、资源需求和高可用性的指导策略，通过横向扩展Pod，也通过将附近的Pod进行性能和低延迟交互的方式来实现。然而，为了让调度器正确地完成它的工作，并允许声明式的调度，它需要有可用容量的节点，以及有声明式资源配置文件和指导策略的容器。让我们更详细地看看其中的每一个。&lt;/p&gt;
&lt;h4 id=&#34;available-node-resources&#34;&gt;Available Node Resources&lt;/h4&gt;
&lt;p&gt;首先，Kubernetes集群需要有足够资源容量的节点来运行新的Pod。每个节点都有可用于运行Pod的容量，调度器确保一个Pod所请求的资源之和小于可分配的节点容量。考虑到一个只专用于Kubernetes的节点，其容量使用例1-1中的公式计算。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;实例1-1. node容量
Allocatable[capacity for application pods] = Node Capacity[available capacity on a ndoe] - kube-Reserved[Kubernetes daemons like kubelet, container runtime] - System-Reserved[os system daemons like sshd udev]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果你没有为给操作系统和Kubernetes本身提供动力的系统守护进程预留资源，那么Pods的调度可能会达到节点的全部容量，这可能会导致Pods和系统守护进程争夺资源，导致节点上的资源不足问题。同时要记住，如果容器运行在非Kubernetes管理的节点上，反映在Kubernetes的节点容量计算中。&lt;/p&gt;
&lt;p&gt;这个限制的一个变通方法是运行一个空的Pod，它不做任何事情，只是对CPU和内存的资源请求与未跟踪容器的资源使用量相对应。创建这样的Pod只是为了表示和保留未跟踪容器的资源消耗量，帮助调度器建立更好的节点资源模型。&lt;/p&gt;
&lt;h4 id=&#34;container-resource-demands&#34;&gt;Container Resource Demands&lt;/h4&gt;
&lt;p&gt;高效的Pod调度的另一个重要要求是，容器有其运行时的依赖性和资源需求的定义。归根结底就是要让容器声明它们的资源概况（有请求和限制）和环境依赖性，如存储或端口。只有这样，Pod才会被合理地分配到节点上，并能在高峰期不影响彼此的运行。&lt;/p&gt;
&lt;h4 id=&#34;调度策略&#34;&gt;调度策略&lt;/h4&gt;
&lt;p&gt;最后一块拼图是拥有正确的过滤或优先级策略来满足你的特定应用需求。调度器配置了一套默认的判断和优先级策略，这对大多数应用来说已经足够了。在调度程序启动时，可以用不同的策略来覆盖它，如例1-2所示。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1-2. 调度策略
{ 
    &amp;quot;kind&amp;quot;:&amp;quot;Policy&amp;quot;,
    &amp;quot;apiVersion&amp;quot;:&amp;quot;v1&amp;quot;,
    &amp;quot;predicates&amp;quot;:[ 
        {&amp;quot;name&amp;quot;:&amp;quot;PodFitsHostPorts&amp;quot;},
        {&amp;quot;name&amp;quot;:&amp;quot;PodFitsResources&amp;quot;},
        {&amp;quot;name&amp;quot;:&amp;quot;NoDiskConflict&amp;quot;},
        {&amp;quot;name&amp;quot;:&amp;quot;NoVolumeZoneConflict&amp;quot;},
        {&amp;quot;name&amp;quot;:&amp;quot;MatchNodeSelector&amp;quot;},
        {&amp;quot;name&amp;quot;:&amp;quot;HostName&amp;quot;}
    ],
    &amp;quot;priorities&amp;quot;:[ 
        {&amp;quot;name&amp;quot;:&amp;quot;LeastRequestedPriority&amp;quot;,&amp;quot;weight&amp;quot;:2},
        {&amp;quot;name&amp;quot;:&amp;quot;BalancedResourceAllocation&amp;quot;,&amp;quot;weight&amp;quot;:1},
        {&amp;quot;name&amp;quot;:&amp;quot;ServiceSpreadingPriority&amp;quot;,&amp;quot;weight&amp;quot;:2},
        {&amp;quot;name&amp;quot;:&amp;quot;EqualPriority&amp;quot;,&amp;quot;weight&amp;quot;:1}
    ]
    
}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;判断是过滤掉不合格节点的规则。例如，PodFitsHostsPortsschedules Pods只在那些还有这个端口的节点上请求某些固定的主机端口。&lt;/li&gt;
&lt;li&gt;优先级是根据偏好对可用节点进行排序的规则。例如，LeastRequestedPriority 给予请求资源较少的节点较高的优先级。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;考虑到除了配置默认调度器的策略外，还可以运行多个调度器，并允许Pod指定调度哪个调度器。你可以给它一个唯一的名字来启动另一个配置不同的调度器实例。然后在定义Pod时，只需在Pod规范中添加字段.spec.scheduleName，并将你的自定义调度器名称添加到Pod规范中，Pod就会只被自定义调度器接收。&lt;/p&gt;
&lt;h4 id=&#34;调度过程&#34;&gt;调度过程&lt;/h4&gt;
&lt;p&gt;Pods根据调度策略被分配到具有一定容量的节点上。为了完整起见，图1-1在高层次上直观地展示了这些元素是如何结合在一起的，以及Pod在被调度时经历的主要步骤。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kubesphereio.com/img/scheduling-process.png&#34; alt=&#34;调度过程&#34;&gt;&lt;/p&gt;
&lt;p&gt;一旦创建了一个尚未分配给节点的Pod，它就会被调度器挑选出来，连同所有可用的节点以及过滤和优先级策略集。在第一阶段，调度器应用过滤策略，并根据Pod的标准删除所有不合格的节点。在第二阶段，剩余的节点得到按权重排序。在最后一个阶段，Pod得到一个节点分配，这是调度过程的主要结果。&lt;/p&gt;
&lt;p&gt;在大多数情况下，最好是让调度程序来完成Pod到节点的分配，而不是微观管理调度逻辑。然而，在某些情况下，你可能想强制将一个Pod分配到一个特定的节点或一组节点。这种分配可以使用节点选择器来完成。.spec.nodeSelector是Pod字段，指定了一个键值对的映射，这些键值对必须作为标签存在于节点上，该节点才有资格运行Pod。例如，假设你想强制Pod运行在有SSD存储或GPU加速硬件的特定节点上。在例1-3中的Pod定义中，nodeSelector匹配disktype：ssd，只有标签为disktype=ssd的节点才有资格运行Pod。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1-3. Node基于可用disk类型选择
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
  nodeSelector:
    disktype: ssd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;除了给节点指定自定义标签外，你还可以使用一些每个节点上都有的默认标签。每个节点都有一个唯一的kubernetes.io/hostname标签，可以通过其主机名将Pod调度在节点上。其他表示操作系统、架构和实例类型的默认标签对调度也很有用。&lt;/p&gt;
&lt;h4 id=&#34;node-affinity&#34;&gt;Node Affinity&lt;/h4&gt;
&lt;p&gt;Kubernetes支持许多更灵活的方式来配置调度过程。节点亲和力就是这样一个特性，它是前面介绍的节点选择器方法的泛化，允许将规则指定为必填或优先。必需的规则必须满足，Pod才会被调度到某个节点，而优先的规则只是通过增加匹配节点的权重来暗示偏好，而不是强制性的。此外，节点亲和性功能极大地扩展了你可以表达的约束类型，通过In、NotIn、Exists、DoesNotExist、Gt或Lt等运算符使语言更具表现力，例1-4演示了如何声明节点亲和性。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1.4 节点亲和性
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: numberCores
            operator: Gt
            values: [ &amp;quot;3&amp;quot; ]
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchFields:
          - key: metadata.name
            operator: NotIn
            values: [ &amp;quot;master&amp;quot; ]
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;pod-affinity-and-antiaffinity&#34;&gt;Pod Affinity and Antiaffinity&lt;/h4&gt;
&lt;p&gt;节点亲和力是一种更强大的调度方式，当nodeSelector不够用时，应该优先考虑。这种机制允许根据标签或字段匹配来限制一个Pod可以运行的节点，但它不允许表达Pod之间的依赖关系来决定一个Pod的相对位置。它不允许表达Pod之间的依赖关系，来决定一个Pod相对于其他Pod应该调度在哪里。为了表达Pod应该如何分布以实现高可用性，或被打包并集中在一起以改善延迟，可以使用Pod亲和力和反亲和力。&lt;/p&gt;
&lt;p&gt;节点亲和力在节点粒度上工作，但Pods亲和力不限于节点，可以在多个拓扑层次上表达规则。使用==topologyKey字段和匹配的标==签，可以执行更细粒度的规则，这些规则结合了节点、机架、云提供商区域和区域等域的规则，如例1-5所示。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1-5. Pod亲和性
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            confidential: high
        topologyKey: security-zone
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
        labelSelector:
          matchLabels:
            confidential: none
        topologyKey: kubernetes.io/hostname
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;与节点亲和力类似，Pod亲和力和反亲和力也有硬性要求和软性要求，分别称为requiredDuringSchedulingIgnoredDuringExecution和preferredDuringSchedulingIgnoredDuringExecution。同样，和节点亲和力一样，字段名中也有IgnoredDuringExecution的后缀，这是为了将来的可扩展性而存在的。目前，如果节点变化和亲和力规则上的标签不再有效，Pods就会继续运行，但未来运行时的变化也可能被考虑在内。&lt;/p&gt;
&lt;h4 id=&#34;taints-and-tolerations&#34;&gt;Taints and Tolerations&lt;/h4&gt;
&lt;p&gt;一个更高级的功能是基于污点和容忍来控制Pods可以被调度和允许运行的地方。节点亲和力是Pods的一个属性，它允许Pods选择节点，而污点和容忍则相反。它们允许节点控制哪些Pods应该或不应该被调度在它们上面。污点是节点的一个特性，当它存在时，它阻止Pods调度到节点上，除非Pod对污点有容忍度。从这个意义上说，污点和容忍可以被认为是允许调度到节点上的一种选择，默认情况下，这些节点是不能被调度的，而亲和规则则是一种选择，通过明确选择在哪些节点上运行，从而排除所有非选择的节点。&lt;/p&gt;
&lt;p&gt;通过使用kubectl给一个节点添加污点：kubectl taint nodes master node-role.kubernetes.io/master=&amp;quot;true&amp;rdquo;:NoSchedule，其效果如例1-6所示。如例1-7所示，将匹配的toleration添加到Pod中。注意，例1-6中taints部分的key和effect的值和例1-7中tolerations:部分的值是一样的。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1.6 node污点
apiVersion: v1
kind: node
metadata:
  name: master
spec:
  taints:
  - effect: NoSchedule
  key: node-role.kubernetes.io/master
  
1.7 Pod忍受和node污点
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
  tolerations:
  - key: node-role.kubernetes.io/master
    operator: Exists
    effect: NoSchedule
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;有防止在节点上调度的硬污点(effect=NoSchedule)，有尽量避免在节点上调度的软污点(effect=PreferNoSchedule)，还有可以从节点上驱逐已经运行的Pod的污点(effect=NoExecute)。&lt;/p&gt;
&lt;p&gt;污点和容忍允许复杂的用例，比如为一组专属的Pods设置专用节点，或者通过这些污点节点强制将Pods从有问题的节点驱逐出去。&lt;/p&gt;
&lt;p&gt;你可以根据应用的高可用性和性能需求来影响调度，但尽量不要对调度器限制太多，把自己退到一个角落里，不能再调度Pods，搁浅的资源太多。比如，如果你的容器资源需求粒度太粗，或者节点太小，最后可能会出现节点中的搁浅资源没有被利用的情况。&lt;/p&gt;
&lt;p&gt;在图1-2中，我们可以看到节点A有4GB的内存无法利用，因为没有CPU可以放置其他容器。创建具有较小resourcere quirements的容器可能有助于改善这种情况。另一个解决方案是使用Kubernetes descheduler，它有助于分解节点，提高节点的利用率。&lt;/p&gt;
&lt;p&gt;一旦Pod被分配到一个节点上，调度器的工作就完成了，它不会改变Pod调度的位置，除非在没有节点分配的情况下删除和重新创建Pod。正如你所看到的，随着时间的推移，这可能会导致资源碎片化和集群资源利用率低下。另一个潜在的问题是，当一个新的Pod被调度时，调度器的决策是基于其集群视图的。如果一个集群是动态的，节点的资源情况发生了变化，或者增加了新的节点，调度器就不会纠正之前的Pod调度情况。除了改变节点容量外，还可以改变节点上的标签，影响调度，但过去的调度也不会被修正。&lt;/p&gt;
&lt;p&gt;所有这些都是descheduler可以解决的场景。Kubernetes descheduler是一个可选的功能，通常在集群管理员决定是时候通过重新安排Pods来整理和分解集群时，它就会以Job的形式运行。descheduler带有一些预定义的策略，可以启用、调整或禁用。这些策略以文件的形式传递给descheduler Pod，目前，它们有以下几种。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RemoveDuplicates&lt;br&gt;
该策略可确保只有与 ReplicaSet 或Deployment 相关联的单个 Pod 在单个节点上运行。如果Pod数量超过一个，这些多余的Pod将被驱逐。该策略在节点变得不健康的情况下非常有用，管理控制器在其他健康节点上启动了新的Pod。当不健康的节点恢复并加入集群时，运行中的Pod数量超过了预期，descheduler可以帮助将数量恢复到预期的副本数。当调度策略和集群拓扑结构在初始调度后发生变化时，去除节点上的重复也可以帮助Pods在更多节点上均匀分布。&lt;/li&gt;
&lt;li&gt;LowNodeUtilization&lt;br&gt;
该策略可以找到未被充分利用的节点，并从其他过度利用的节点上驱逐Pod，希望将这些Pod调度在未被充分利用的节点上，从而更好地分散和利用资源。未充分利用的节点被识别为CPU、内存或Pod数量低于配置阈值的节点。同样，过度利用的节点是指那些值大于配置的目标阈值的节点。介于这些值之间的任何节点都被适当利用，不受该策略的影响。&lt;/li&gt;
&lt;li&gt;RemovePodsViolatingInterPodAntiAffinity&lt;br&gt;
这个策略驱逐的Pods违反了Pods间的反亲和规则，这可能发生在Pods被调度在节点上后添加反亲和规则时。&lt;/li&gt;
&lt;li&gt;RemovePodsViolatingNodeAffinity&lt;br&gt;
这个策略是用来驱逐违反节点亲和规则的Pods的。&lt;/li&gt;
&lt;li&gt;Regardless of the policy used, the descheduler avoids evicting the followding：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;标有scheduler.alpha.kubernetes.io/critical-pod注释的临界Pods。
非ReplicaSet，Deployment和Job管理的pod。
DaemonSet管理的pod。
Pods使用本地存储。
有PodDisruptionBudget的pods，驱逐将违反其规则。
Deschedule Pod本身（通过将自身标记为关键Pod实现）。
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当然，所有的驱逐都会尊从Pods的QoS水平，先选择Best-EffortsPods,然后是Burstable Pods，最后是Guaranteed Pods作为驱逐的候选者。&lt;/p&gt;
&lt;h2 id=&#34;讨论&#34;&gt;讨论&lt;/h2&gt;
&lt;p&gt;调度是一个你希望尽可能少干预的领域。可预测的需求，并声明容器的所有有源需求，调度器会做它的工作，并将Pod放置在最合适的节点上。然而，当这还不够的时候，有多种方法可以引导调度器朝向所需的部署拓扑。综上所述，从简单到复杂，以下方法控制了Pod调度（请记住，截至本文撰写时，这个列表会随着Kubernetes的每一个其他版本而改变）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nodeName&lt;br&gt;
最简单硬形式为Pod到节点。这个字段最好由调度器填充，它由策略驱动，而不是手动分配节点。将Pod分配到节点上，极大地限制了Pod的调度范围。这让我们回到了前Kubernetes时代，当时我们明确指定了运行应用的节点。&lt;/li&gt;
&lt;li&gt;nodeSelector&lt;br&gt;
指定键值对的映射。为了使Pod有资格在节点上运行，Pod必须有指定的键值对作为节点上的标签。在Pod和节点上贴上一些有意义的标签后（无论如何你都应该这么做），节点选择器是控制调度器选择的最简单可接受的机制之一。&lt;/li&gt;
&lt;li&gt;Default scheduling alteration&lt;br&gt;
默认的调度器负责将新的Pod调度到集群内的节点上，而且它做得很合理。但是，如果有必要，可以改变这个调度器的过滤和优先策略列表、顺序和权重。&lt;/li&gt;
&lt;li&gt;Pod affinity and antiaffinity&lt;br&gt;
这些规则允许一个Pod表现对其他Pod的依赖性，例如，一个应用的延迟要求、高可用性、安全约束等。&lt;/li&gt;
&lt;li&gt;Node affinity&lt;br&gt;
这个规则允许Pod向节点表现依赖性。例如，考虑节点的硬件、位置等。&lt;/li&gt;
&lt;li&gt;Taints and tolerations&lt;br&gt;
Taints和tolerations允许节点控制哪些Pods应该或不应该被调度在它们上面。例如，为一组Pod致力一个节点，甚至在运行时驱逐Pod。Taints和Tolerations的另一个优点是，如果你通过添加带有新标签的新节点来扩展Kubernetes集群，你不需要在所有Pod上添加新标签，而只需要在应该放在新节点上的Pod上添加。&lt;/li&gt;
&lt;li&gt;Custom scheduler&lt;br&gt;
如果前面的方法都不够好，或者你有复杂的调度需求，你也可以写你的自定义调度器。自定义的调度器可以代替标准的Kubernetes调度器运行，也可以和标准的Kubernetes调度器一起运行。Ahybrid的做法是有一个 &amp;ldquo;调度扩展器 &amp;ldquo;进程，标准Kubernetes调度器在做调度决策时，会调用这个进程作为最后的通道。这样你就不用实现一个完整的调度器，而只需要提供HTTP API来过滤和优先处理节点。拥有自己的调度器的好处是，你可以考虑Kubernetes集群之外的因素，比如硬件成本、网络延迟和更好的利用率，同时将Pod分配给节点。你也可以在使用默认调度器的同时使用多个自定义调度器，并为每个Pod配置使用哪个调度器。每个调度器可以有一套不同的策略，专门用于Pod的子集。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;正如你所看到的，有很多方法可以控制Pod的放置，选择正确的方法或结合多种方法是很有挑战性的。本章的启示是：确定容器资源配置文件的大小和声明，给Pod和节点打上相应的标签，最后，只对Kubernetes调度器做最小的干预。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>K8s中生命周期管理设计模式</title>
      <link>https://kubesphereio.com/post/managed-lifecycle/</link>
      <pubDate>Wed, 30 Dec 2020 20:32:48 +0800</pubDate>
      
      <guid>https://kubesphereio.com/post/managed-lifecycle/</guid>
      <description>&lt;p&gt;由云原生平台管理的容器化应用对其生命周期没有控制权，要想成为优秀的云原生化 ，它们必须监听管理平台发出的事件，并相应地调整其生命周期。托管生命周期模式描述了应用程序如何能够并且应该对这些生命周期事件做出反应。&lt;/p&gt;
&lt;h2 id=&#34;存在问题&#34;&gt;存在问题&lt;/h2&gt;
&lt;p&gt;健康探针我们解释了为什么容器要为不同的健康检查提供API。健康检查API是平台不断探查以获取应用洞察力的只读endpoints。它是平台从应用中提取信息的一种机制。&lt;/p&gt;
&lt;p&gt;除了监控容器的状态外，平台有时可能会发出命令，并期望应用程序对此做出反应。在策略和外部因素的驱动下，云原生平台可能会在任何时刻决定启动或停止其管理的应用程序。容器化应用要决定哪些事件是重要的，要做出反应以及如何反应。但实际上，这是一个API，平台是用来和应用进行通信和发送命令的。另外，如果应用程序不需要这项服务，他们可以自由地从生命周期管理中获益，或者忽略它。&lt;/p&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;p&gt;我们看到，只检查进程状态并不能很好地显示应用程序的健康状况。这就是为什么有不同的API来监控容器的健康状况。同样，只使用进程模型来运行和停止一个进程是不够好的。现实世界中的应用需要更多的细粒度交互和生命周期管理能力。有些应用需要帮助预热，有些应用需要优雅而不带缓存的关闭程序。对于这种和其他用例，一些事件，如图1-1所示，由平台发出，容器可以监听并在需要时做出反应。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kubesphereio.com/img/lifecycle.png&#34; alt=&#34;生命周期.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;应用程序的部署单元是一个Pod。你已经知道，一个Pod由一个或多个容器组成。在Pod层面，还有其他的构造，比如init容器、init Container（以及defer-containers，截止到目前还在提案阶段），可以帮助管理容器的生命周期。我们在本章中描述的事件和钩子都是应用在单个容器级别而不是Pod级别。&lt;/p&gt;
&lt;h4 id=&#34;sigterm-signal&#34;&gt;SIGTERM Signal&lt;/h4&gt;
&lt;p&gt;每当Kubernetes决定关闭一个容器时，无论是因为它所属的Pod正在关闭，还是仅仅是一个失败的liveness探测导致容器重新启动，容器都会收到一个SIGTERM信号。SIGTERM是在Kubernetes发出更突然的SIGKILL信号之前，温柔地戳一下容器，让它干净利落地关闭。一旦收到SIGTERM信号，应用程序应该尽快关闭。对于一些应用来说，这可能是一个快速的终止，而其他一些应用可能必须完成其飞行中的请求，释放开放的连接，并清理临时文件，这可能需要稍长的时间。在所有的情况下，对SIGTERM做出反应是以正确时刻和干净的方式关闭容器。&lt;/p&gt;
&lt;h4 id=&#34;sigkill-signal&#34;&gt;SIGKILL Signal&lt;/h4&gt;
&lt;p&gt;如果容器进程在发出SIGTERM信号后还没有关闭，则会被下面的SIGKILL信号强制关闭。Kubernetes不会立即发送SIGKILL信号，而是在发出SIGTERM信号后默认等待30秒的宽限期。这个宽限期可以使用.spec.terminalGracePeriodSeconds字段为每个Pod定义，但不能保证，因为它可以在向Kubernetes发出命令时被覆盖。这里的目的应该是设计和实现容器化应用，使其具有短暂性的快速启动和关闭进程。&lt;/p&gt;
&lt;h4 id=&#34;poststart-hook&#34;&gt;Poststart Hook&lt;/h4&gt;
&lt;p&gt;只使用过程信号来管理生命周期是有一定局限性的。这就是为什么Kubernetes提供了额外的生命周期钩子，如postStart和preStop。包含postStart钩子的Pod清单看起来像例5-1中的那个。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;实例1-1，容器中配置poststart hook
apiVersion: v1
kind: Pod
metadata:
  name: post-start-hook
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    lifecycle:
      postStart:
        exec:
          command:
          - sh
          - -c
          - sleep 30 &amp;amp;&amp;amp; echo &amp;quot;Wake up!&amp;quot; &amp;gt; /tmp/postStart_done
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;postStart命令在这里等待30秒。sleep只是模拟任何可能在这里运行的冗长启动代码。另外，它在这里使用一个触发器文件与主应用程序同步，主应用程序是并行启动的。&lt;/p&gt;
&lt;p&gt;postStart 命令在容器创建后与主容器的进程异步执行。即使许多应用程序的初始化和预热逻辑可以作为容器启动步骤的一部分来实现，postStart仍然涵盖了一些用例。postStart动作是一个阻塞调用，容器状态保持为Waiting，直到postStart处理程序完成，这又使Pod状态保持为Pending状态。postStart的这种性质可以用来延迟容器的启动状态，同时给容器主进程初始化的时间。&lt;/p&gt;
&lt;p&gt;postStart的另一个用途是当Pod不满足某些前提条件时，防止容器启动。例如，当postStart钩子通过返回一个非零的退出代码来指示错误时，主容器进程会被Kubernetes杀死。&lt;/p&gt;
&lt;p&gt;postStart和preStophook调用机制类似于所述的健康探针，并支持这些处理程序类型。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;exec  直接在容器中运行指令&lt;/li&gt;
&lt;li&gt;httpGet  对一个Pod容器监听的端口执行HTTP GET请求。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;你必须非常小心地执行postStart钩子中的关键逻辑，因为它的执行没有保证。由于钩子是与容器进程并行运行的，所以钩子有可能在容器启动之前就被执行了。另外，钩子的目的是至少有一次语义，所以实现必须照顾到重复的执行。另一个需要注意的方面是，平台不会对没有到达处理程序的HTTP请求失败执行任何重试尝。&lt;/p&gt;
&lt;h4 id=&#34;prestop-hook&#34;&gt;Prestop Hook&lt;/h4&gt;
&lt;p&gt;preStop 钩子是在容器被终止之前向其发送的阻塞调用，它与 SIGTERM 信号具有相同的语义，在无法对 SIGTERM 作出反应时，应使用它来优雅关闭容器。它与SIGTERM信号具有相同的语义，当无法对SIGTERM作出反应时，它应该被用来启动容器的优雅关闭。例 1-2 中的 preStop 动作必须在删除容器的调用被发送到容器运行时之前完成，后者会触发 SIGTERM 通知。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1-2. 容器中配置preStop Hook
apiVersion: v1
kind: Pod
metadata:
  name: pre-stop-hook
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    lifecycle:
      preStop:
        httpGet:
          port: 8080
          path: /shutdown
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;即使preStop正在阻塞，按住它或返回一个不成功的结果并不能阻止容器被删除和进程被杀死。preStop只是一个方便的替代SIGTERM信号的优雅应用，仅此而已。它还提供了与我们之前介绍的postStart钩子相同的处理程序类型和保证。&lt;/p&gt;
&lt;h4 id=&#34;other-lifecycle-controls&#34;&gt;Other Lifecycle Controls&lt;/h4&gt;
&lt;p&gt;在本章中，到目前为止，我们已经关注了当容器生命周期事件发生时允许执行命令的钩子。但另一种机制不是在容器层面，而是在Pod层面，允许执行初始化指令。&lt;/p&gt;
&lt;p&gt;Init容器，深入浅出，但这里我们简单介绍一下，将其与生命周期钩子进行比较。与普通的应用容器不同，init容器按顺序运行，一直运行到完成，并且在Pod中任何一个应用容器启动之前运行。这些保证允许使用init容器进行Pod级初始化任务。生命周期钩子和init容器都以不同的粒度（分别在容器级和Pod级）运行，可以在某些情况下交替使用，或者在其他情况下相互补充。表1-1总结了两者的主要区别。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kubesphereio.com/img/other-lifecycle.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;除了当你需要特定的时间保证时，使用哪种机制没有严格的规定。我们可以完全跳过生命周期钩子和初始化容器，使用bash脚本来执行特定的操作，作为容器启动或关闭命令的一部分。这是有可能的，但它会将容器与脚本紧密耦合，并将其变成维护的噩梦。&lt;/p&gt;
&lt;p&gt;我们还可以使用 Kubernetes 生命周期钩子来执行本章所述的一些动作。另外，我们还可以更进一步，运行使用init容器执行各个动作的容器。在这个序列中，这些选项越来越需要更多的提升，但同时也提供了更强的保障，并实现了重用。&lt;/p&gt;
&lt;p&gt;理解容器和Pod生命周期的阶段和可用钩子对于创建Kubernetes管理的应用来说是至关重要的。&lt;/p&gt;
&lt;h2 id=&#34;讨论&#34;&gt;讨论&lt;/h2&gt;
&lt;p&gt;云原生平台提供的主要好处之一是能够在潜在的不可靠的云基础设施之上可靠和可预测地运行和扩展应用程序。这些平台为在其上运行的应用程序提供了一系列限制和共识。为了有利于应用程序，云原生平台提供的所有功能都遵守这些机制。处理和响应这些事件可以确保您的应用程序可以优雅地启动和关闭，对消费服务的影响最小。目前，在其基本形式下，这意味着容器应该像任何设计良好的POSIX进程一样行为。在未来，可能会有更多的事件给应用程序提示，当它即将被放大，或要求释放资源以防止被关闭。重要的是要进入这样的思维模式：应用程序的生命周期不再由人控制，而是由平台完全自动化。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>K8s中健康检查设计模式</title>
      <link>https://kubesphereio.com/post/health-probe/</link>
      <pubDate>Tue, 29 Dec 2020 13:34:44 +0800</pubDate>
      
      <guid>https://kubesphereio.com/post/health-probe/</guid>
      <description>&lt;p&gt;健康探针模式是关于应用程序如何将其健康状态传达给Kubernetes。为了实现完全自动化，云原生应用必须具有高度的可观察性，允许推断其状态，以便Kubernetes能够检测应用是否已经启动，是否准备好服务的请求。这些观察结果会影响Pods的生命周期管理以及流量被路由到应用程序的方式。&lt;/p&gt;
&lt;h2 id=&#34;存在问题&#34;&gt;存在问题&lt;/h2&gt;
&lt;p&gt;Kubernetes会定期检查容器进程状态，如果发现问题就会重新启动。然而，从实践中我们知道，检查进程状态并不足以决定应用程序的健康状况。在很多情况下，一个应用程序挂起了，但它的进程仍然在运行。例如，一个Java应用程序可能会抛出一个OutOfMemoryError，但JVM进程仍在运行。或者，一个应用程序可能会因为运行到一个无限循环、死锁或一些冲击（缓存、堆、进程）而冻结。为了检测这类情况，Kubernetes需要一种可靠的方法来检查应用程序的健康状况。也就是说，并不是要了解应用的内部工作情况，而是一种检查，表明应用是否按照预期运行，是否能够为消费者提供服务。&lt;/p&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;p&gt;软件业已经接受了这样一个事实，即不可能写出无错误的代码。此外，在使用分布式应用程序时，发生故障的机会就更多了。因此，处理故障的重点已经从避免故障转移到检测故障和恢复上。检测故障并不是一个简单的任务，不能对所有的应用统一执行，因为所有的应用对故障的定义都不同。而且，各种类型的故障需要不同的纠正措施。只要有足够的时间，暂时性的故障可能会自我恢复，而其他一些故障可能需要重新启动应用程序。让我们看看Kubernetes用来检测和纠正故障的检查。&lt;/p&gt;
&lt;h4 id=&#34;进程健康检查&#34;&gt;进程健康检查&lt;/h4&gt;
&lt;p&gt;进程健康检查是Kubelet不断对容器进程进行的最简单的健康检查。如果容器进程没有运行，就会重新启动探测。因此，即使没有任何其他的健康检查，应用程序也会因为这个通用检查而变得更加健壮。如果你的应用程序能够检测到任何类型的故障并关闭自己，那么进程健康检查就是你所需要的全部内容.然而，对于大多数情况下，这还不够，其他类型的健康检查也是必要的。&lt;/p&gt;
&lt;h4 id=&#34;liveness-probes&#34;&gt;Liveness Probes&lt;/h4&gt;
&lt;p&gt;如果你的应用程序运行到一些死锁，从进程健康检查的角度来看，它仍然被认为是健康的。为了根据你的==应用业务逻辑来检测==这种问题和任何其他类型的故障，Kubernetes有==liveness probes==&amp;ndash;由Kubelet代理定期执行检查，询问你的容器确认它仍然是健康的。重要的是要从外部而不是应用程序本身执行健康检查，因为一些故障可能会阻止应用程序看门狗报告其故障。关于纠正措施，这种健康检查类似于进程健康检查，因为如果检测到故障，容器就会重新启动。然而，在使用什么方法检查应用程序健康状况方面，它提供了更多的灵活性，如下所示。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HTTP探针通过容器IP地址执行HTTP GET请求，并期望得到一个介于200和399之间的成功的HTTP响应代码。&lt;/li&gt;
&lt;li&gt;TCP Socket探针假设TCP连接成功。&lt;/li&gt;
&lt;li&gt;Exec探针在容器内核命名空间中执行一个任意命令，并期望有一个成功的退出代码（0）。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;1.1 容器中配置liveness probe
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-liveness-check
spec:
  containers:
  - image: k8spatterns/random-generator:1.0 
    name: random-generator
    env:
    - name: DELAY_STARTUP
      value:&amp;quot;20&amp;quot; 
    ports:
    - containerPort: 8080
    protocol: TCP
    livenessProbe:
      httpGet:
      path: /actuator/health
      port: 8080
    initialDelaySeconds: 30
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;根据您的应用程序的性质，您可以选择最适合您的方法。由您的实现来决定您的应用程序何时被认为是健康的或不健康的。然而，请记住，没有通过健康检查的结果是重启你的容器。如果重启你的容器没有帮助，那么健康检查失败没有任何好处，因为Kubernetes会重启你的容器而不解决根本问题。&lt;/p&gt;
&lt;h4 id=&#34;readiness-probes&#34;&gt;Readiness Probes&lt;/h4&gt;
&lt;p&gt;Liveness检查对于保持应用程序的健康非常有用，它可以杀死不健康的容器，并用新的容器替换它们。但有时一个容器可能并不健康，重启它可能也无济于事。最常见的例子是当一个容器还在启动，还没有准备好处理任何请求。或者是一个容器超载了，它的延迟在增加，你希望它暂时屏蔽掉额外的负载。&lt;/p&gt;
&lt;p&gt;对于这种场景，Kubernetes有Readiness探针。执行就绪性Readiness检查的方法与有效性检查（HTTP、TCP、Exec）相同，但纠正措施不同。失败的Readiness探针不是重启容器，而是导致容器从服务端点中移除，并且不接收任何新的流量。当容器准备就绪时，Readiness针会发出信号，以便它在受到服务请求的冲击之前有一段时间进行热身。它对于在后期阶段屏蔽服务的流量也很有用，因为Readiness探测会定期执行，类似于Liveness检查。例1-2展示了如何通过探测已运行的应用的内部文件是否存在来实现Readiness探测。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1-2 容器中配置readiness probe
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-readiness-check
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    readinessProbe:
      exec:  command: [ &amp;quot;stat&amp;quot;, &amp;quot;/var/run/random-generator-ready&amp;quot; ]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;同样，由你对健康检查的实现来决定你的应用程序什么时候准备好做它的工作，什么时候应该让它自己去做。进程健康检查和liveness检查的目的是通过重启容器从故障中恢复，而readiness检查则是为你的应用程序争取时间，并期望它自己恢复。请记住，Kubernetes试图阻止你的容器接收新的请求（例如，当它正在关闭时），无论readiness检查是否在收到SIGTERM信号后仍然通过。&lt;/p&gt;
&lt;p&gt;在许多情况下，您可以同时执行liveness和readiness探针检查。然而，readiness探针的存在为您的容器提供了启动时间。只有通过了readiness检查，部署才被视为成功，因此，例如，使用旧版本的Pods可以作为滚动更新的一部分被终止。&lt;/p&gt;
&lt;p&gt;liveness和readiness探针是云原生应用程序自动化的基本构件。应用框架，如Spring执行器、WildFly Swarm健康检查、Karaf健康检查或Java的MicroProfile规范都提供了健康探针的实现。&lt;/p&gt;
&lt;h2 id=&#34;讨论&#34;&gt;讨论&lt;/h2&gt;
&lt;p&gt;为了实现完全自动化，云原生应用必须具有高度可观察性，为管理平台提供读取和解释应用健康状况的方法，并在必要时采取纠正措施。健康检查在部署、自愈、扩展等活动的自动化中起着基础作用。然而，对于应用健康，您的应用程序还可以通过其他手段提供更多可见性。&lt;/p&gt;
&lt;p&gt;显而易见的、老的方法是通过日志记录来实现这一目的。对于容器来说，一个好的做法是记录任何重大的系统出错和系统错误事件，并将这些日志收集到一个中心位置以便进一步分析。日志通常不是用来采取自动行动的，而是用来提出告警和进一步调查。日志更有用的方面是对故障的事后分析和检测不明显的错误。&lt;/p&gt;
&lt;p&gt;除了记录到标准流中，将退出容器的原因记录到/dev/termination-log也是一个好的做法。这个位置是容器在永久消失之前陈述其最后意愿的地方。图1-1显示了容器如何与运行时平台通信的可能选项。
&lt;img src=&#34;https://kubesphereio.com/img/health-check.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;容器通过将其视为黑盒，为包装和运行应用程序提供了一种统一的方式。然而，任何旨在成为云原生产品的容器都必须为运行时环境提供API，以观察容器的健康状况并采取相应行动。这种支持是以统一的方式实现容器更新和生命周期自动化的基本前提，从而提高系统的弹性和用户体验。在实际操作中，这意味着，作为最起码的要求，你的容器化应用程序必须为不同类型的健康检查（liveness和readiness）提供API。&lt;/p&gt;
&lt;p&gt;即使是表现更好的应用程序也必须提供其他手段，让管理平台通过集成跟踪和度量收集库（如OpenTracing或Prometheus）来观察容器化应用程序的状态。把你的应用当作一个黑盒子，但要实现所有必要的API，以帮助平台以最好的方式观察和管理你的应用。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>K8s中声明式的部署设计模式</title>
      <link>https://kubesphereio.com/post/declarative-deployment/</link>
      <pubDate>Sun, 27 Dec 2020 19:08:06 +0800</pubDate>
      
      <guid>https://kubesphereio.com/post/declarative-deployment/</guid>
      <description>&lt;p&gt;声明式Deployment模式的核心是Kubernetes 的Deployment资源。这个抽象封装了一组容器的升级和回滚过程，并使其执行成为一种可重复和自动化的活动。&lt;/p&gt;
&lt;h2 id=&#34;存在问题&#34;&gt;存在问题&lt;/h2&gt;
&lt;p&gt;我们可以以自助服务的方式将隔离的环境作为命名空间进行调配，并通过调度器将服务调度在这些环境中，只需最少的人工干预。但是随着微服务的数量越来越多，不断地用新的版本更新和更换也成了越来越大的负担。&lt;/p&gt;
&lt;p&gt;将服务升级到下一个版本涉及的活动包括启动新版本的Pod，优雅地停止旧版本的Pod，等待并验证它已经成功启动，有时在失败的情况下将其全部回滚到以前的版本。这些活动的执行方式有两种，一种是允许有一定的停机时间，但不允许同时运行并发的服务版本，另一种是没有停机时间，但在更新过程中由于两个版本的服务都在运行，导致资源使用量增加。手动执行这些步骤可能会导致人为错误，而正确地编写脚本则需要花费大量的精力，这两点都会使发布过程迅速变成瓶颈。&lt;/p&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;p&gt;幸运的是，Kubernetes也已经自动完成了这项活动。使用Deployment的概念，我们可以描述我们的应用程序应该如何更新，使用不同的策略，并调整更新过程的各个方面。如果您考虑到每次发布周期都要为每个微服务实例进行多次部署的话（根据团队和项目的不同，可能从几分钟到几个月），这是Kubernetes的另一个省力的自动化。&lt;/p&gt;
&lt;p&gt;在第2章中，我们已经看到，为了有效地完成工作，调度器需要主机上有足够的资源、适当的调度策略以及容器充分定义了资源配置文件。同样，为了使部署正确地完成其工作，它希望容器成为良好的云原生。部署的核心是可预测地启动和停止一组Pod的能力。为了达到预期的工作效果，容器本身通常会监听和符合生命周期事件（如SIGTERM；请参见第5章，托管生命周期），并且还提供第4章云原生中所述的健康检查endpoints，即健康探针，以指示它们是否成功启动。&lt;/p&gt;
&lt;p&gt;如果一个容器准确地覆盖了这两个领域，平台就可以干净利落地关闭旧的容器，并通过启动更新的实例来替换它们。然后，更新过程的所有剩余方面都可以以声明的方式定义，并作为一个原子动作执行，具有预定义的步骤和预期的结果。让我们看看容器更新行为的选项吧&lt;/p&gt;
&lt;p&gt;==注意：==&lt;/p&gt;
&lt;h4 id=&#34;使用-kubectl-进行命令式-rolling-updates已被废弃&#34;&gt;使用 kubectl 进行命令式 Rolling Updates已被废弃&lt;/h4&gt;
&lt;p&gt;Kubernetes从一开始就支持滚动更新。最早的实现是势在必行的，客户端kubectl告诉服务器每个更新步骤要做什么。&lt;/p&gt;
&lt;p&gt;虽然kubectl rolling-update 命令仍然存在，但由于这种命令式的方法存在以下缺点，所以它已经被高度废弃。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kubectl rolling-update 不是描述预期的最终状态，而是发布命令让系统进入预期状态。&lt;/li&gt;
&lt;li&gt;替换容器和ReplicationControllers的整个协调逻辑由kubectl执行，它在发生更新过程时，会监控并与API服务器交互，将固有服务器端的责任转移到客户端。&lt;/li&gt;
&lt;li&gt;您可能需要不止一条命令来使系统进入期望状态。这些命令必须是自动的，并且在不同的环境中可以重复使用。&lt;/li&gt;
&lt;li&gt;随着时间的推移，别人可能会覆盖你的修改。&lt;/li&gt;
&lt;li&gt;更新过程必须记录下来，并在服务推进的同时保持更新。&lt;/li&gt;
&lt;li&gt;要想知道我们部署了什么，唯一的方法就是检查系统的状态。有时候，当前系统的状态可能并不是理想的状态，在这种情况下，我们必须使部署文档相互关联。&lt;/li&gt;
&lt;li&gt;取而代之的是，引入Deployment资源对象来支持声明式更新，完全由Kubernetes后端管理。由于声明式更新有如此多的优势，而命令式更新支持终将消失，我们在这个模式中只关注声明式更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;rolling-deployment&#34;&gt;Rolling Deployment&lt;/h4&gt;
&lt;p&gt;Kubernetes中更新应用的声明方式是通过Deployment的概念。在幕后，Deployment创建了一个ReplicaSet，支持基于集合的标签选择器。同时，Deployment抽象允许通过RollingUpdate（默认）和Recreate等策略来塑造更新过程行为。例1-1显示了Deployment配置的滚动更新策略重要部分。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;例1-1，Deployment for a rolling update
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: random-generator
spec: 
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    selector:
      matchLabels:
        app: random-generator
    template:
      metadata:
        labels:
          app: random-generator
      spec: 
        containers:
        - image: k8spatterns/random-generator:1.0
          name: random-generator 
          readinessProbe:
            exec:
              command: [&amp;quot;stat&amp;quot;,&amp;quot;/random-generator-ready&amp;quot; ]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;滚动更新（RollingUpdate）策略行为确保了更新过程中没有停机时间。在幕后，Deployment实现类似这样的动作来执行，通过创建新的ReplicaSets和用新容器替换旧容器。通过Deployment，这里有一个增强是可以控制新容器滚动的速度。Deployment对象允许你通过maxSurge和maxUnavailable字段来控制可用和超出Pod的范围。图1-1展示了滚动更新过程。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kubesphereio.com/img/RollingUpdate1.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;要触发声明式更新，你有三个选项:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用 kubectl replace 将整个部署替换为新版本的部署。&lt;/li&gt;
&lt;li&gt;补丁(kubectl patch)或交互式编辑(kubectl edit)部署，以设置新版本的新容器镜像。&lt;/li&gt;
&lt;li&gt;使用 kubectl set image 来设置部署中的新镜像。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;请参阅我们的示例仓库中的完整示例，其中演示了这些命令的用法，并展示了如何使用kubectl rollout监控或回滚升级。&lt;/p&gt;
&lt;p&gt;除了解决前面提到的命令部署服务方式的弊端外，Deployment还带来了以下好处：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deployment是一个Kubernetes资源对象，其状态完全由Kubernetes内部管理。整个更新过程在服务器端进行，无需客户端交互。&lt;/li&gt;
&lt;li&gt;Deployment的声明性使您可以看到已部署的状态应该是怎样的，而不是到达那里的必要步骤。&lt;/li&gt;
&lt;li&gt;Deployment定义是一个可执行的对象，在上生产之前会在多个环境中进行测试。&lt;/li&gt;
&lt;li&gt;更新过程也会被完全记录下来，版本上有暂停、继续和回滚到以前版本的选项。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;fixed-deployment&#34;&gt;Fixed Deployment&lt;/h4&gt;
&lt;p&gt;滚动更新（RollingUpdate）策略对于在更新过程中确保零停机时间非常有用。然而，这种方法的副作用是，在更新过程中，容器的两个版本同时运行。这可能会给服务消费者带来问题，特别是当更新过程在服务API中引入了向后不兼容的变化，而客户端又无法处理这些变化时。对于这种情况，有Recreate策略，如图1-2所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kubesphereio.com/img/RollingUpdate2.png&#34; alt=&#34;fixed Deployment.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Recreate策略的效果是将maxUnavailable设置为已声明的复制数。这意味着它首先杀死当前版本的所有容器，然后在旧容器被驱逐时同时启动所有新容器。这意味着它首先杀死当前版本的所有容器，然后在旧容器被驱逐时同时启动所有新容器。这一系列操作的结果是，当所有拥有旧版本的容器被停止时，会有一些停机时间，而且没有新的容器准备好处理传入的请求。从积极的一面来看，不会有两个版本的容器同时运行，简化了服务消费者的生活，一次只处理一个版本。&lt;/p&gt;
&lt;h4 id=&#34;蓝绿发布&#34;&gt;蓝绿发布&lt;/h4&gt;
&lt;p&gt;蓝绿部署是一种用于在最小化停机时间和降低风险的生产环境中部署软件的发布策略。Kubernetes&amp;rsquo;Deployment抽象是一个基本概念，它可以让你定义Kubernetes如何将不可变容器从一个版本过渡到另一个版本。我们可以将Deployment基元作为一个构件，与其他Kubernetes基元一起，实现这种更高级的蓝绿部署的发布策略。&lt;/p&gt;
&lt;p&gt;如果没有使用Service Mesh或Knative等扩展，则需要手动完成蓝绿部署。技术上，它的工作原理是通过创建第二个Deployment，容器的最新版本（我们称它为绿色）还没有服务于任何请求。在这一阶段，原始Deployment中的旧Pod副本（称为蓝色）仍在运行并服务于实时请求。&lt;/p&gt;
&lt;p&gt;一旦我们确信新版本的Pods是健康的，并且准备好处理实时请求，我们就会将流量从旧的Pod副本切换到新的副本。Kubernetes中的这个活动可以通过更新服务选择器来匹配新容器（标记为绿色）来完成。如图1-3所示，一旦绿色容器处理了所有的流量，就可以删除蓝色容器，释放资源用于未来的蓝绿部署。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kubesphereio.com/img/Blue-Green-Release.png&#34; alt=&#34;蓝绿发布.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;蓝绿方式的一个好处是，只有一个版本的应用在服务请求，这降低了Service消费者处理多个并发版本的复杂性。缺点是它需要两倍的应用容量，同时蓝色和绿色容器都在运行。另外，在过渡期间，可能会出现长时间运行的进程和数据库状态漂移的重大并发症。&lt;/p&gt;
&lt;h4 id=&#34;金丝雀发布&#34;&gt;金丝雀发布&lt;/h4&gt;
&lt;p&gt;金丝雀发布是一种通过用新的实例替换一小部分旧的实例来软性地将一个应用程序的新版本部署到生产中的方法。这种技术通过只让部分消费者达到更新的版本来降低将新版本引入生产的风险。当我们对新版本的服务以及它在小样本用户中的表现感到满意时，我们就用新版本替换所有的旧实例。图1-4显示了一个金丝雀版本的运行情况。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kubesphereio.com/img/Canary-Release.png&#34; alt=&#34;金丝雀发布.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;在Kubernetes中，可以通过为新的容器版本（最好使用Deployment）创建一个新的ReplicaSet来实现这一技术，该ReplicaSet的副本数量较少，可以作为Canary实例使用。在这个阶段，服务应该将一些消费者引导到更新的Pod实例上。一旦我们确信使用新 ReplicaSet 的一切都能按预期工作，我们就会将新的 ReplicaSet 规模化，旧的 ReplicaSet 则降为零。从某种程度上来说，我们是在进行一个可控的、经过用户测试的增量式推广。&lt;/p&gt;
&lt;h2 id=&#34;讨论&#34;&gt;讨论&lt;/h2&gt;
&lt;p&gt;Deoloyment本质是 Kubernetes 将手动更新应用程序的繁琐过程转化为可重复和自动化的声明式活动的一个例子。开箱即用的部署策略（rolling和recreate）控制用新容器替换旧容器，而发布策略（bluegreen和金丝雀）则控制新版本如何提供给服务消费者。后两种发布策略是基于人对过渡触发器的决定，因此不是完全自动化的，而是需要人的交互。图1-5显示了部署和发布策略的摘要，显示了过渡期间的实例数。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kubesphereio.com/img/Release.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;每个软件都是不同的，部署复杂的系统通常需要额外的步骤和检查。本章讨论的技术涵盖了Pod更新过程，但不包括更新和回滚其他Pod依赖项，如ConfigMaps、Secrete或其他依赖服务。&lt;/p&gt;
&lt;p&gt;截至目前，Kubernetes有一个建议，允许在部署过程中使用钩子。Pre和Post钩子将允许在Kubernetes执行部署策略之前和之后执行自定义命令。这些命令可以在部署进行时执行额外的操作，另外还可以中止、重试或继续部署。这些命令是向新的自动化部署和发布策略迈出的良好一步。目前，一种行之有效的方法是用脚本化去更高层次上编写更新过程，本节中讨论的Deployment和其他属性来管理服务及其依赖关系的更新过程。&lt;/p&gt;
&lt;p&gt;无论你使用何种部署策略，Kubernetes都必须知道你的应用Pod何时启动并运行，以执行所需的步骤序列达到定义的目标部署状态。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>K8s中可预测的需求设计模式</title>
      <link>https://kubesphereio.com/post/predictable-demands/</link>
      <pubDate>Sat, 26 Dec 2020 15:38:59 +0800</pubDate>
      
      <guid>https://kubesphereio.com/post/predictable-demands/</guid>
      <description>&lt;p&gt;在共享云环境中成功部署、管理和共存应用的基础，取决于识别和声明应用资源需求和运行时依赖性。这个Predictable Demands模式是关于你应该如何声明应用需求，无论是硬性的运行时依赖还是资源需求。声明你的需求对于Kubernetes在集群中为你的应用找到合适的位置至关重要。&lt;/p&gt;
&lt;h2 id=&#34;存在问题&#34;&gt;存在问题&lt;/h2&gt;
&lt;p&gt;Kubernetes可以管理用不同编程语言编写的应用，只要该应用可以在容器中运行。然而，不同的语言有不同的资源需求。通常情况下，编译后的语言运行速度更快，而且经常是
与即时运行时或解释语言相比，需要更少的内存。考虑到很多同类别的现代编程语言对资源的要求都差不多，从资源消耗的角度来看，更重要的是领域、应用的业务逻辑和实际实现细节。&lt;/p&gt;
&lt;p&gt;很难预测容器可能需要多少资源才能发挥最佳功能，而知道服务运行的预期资源是开发人员（通过测试发现）。有些服务的CPU和内存消耗情况是固定的，有些服务则是瞬间的。有些服务需要持久性存储来存储数据；有些传统服务需要在主机上固定端口号才能正常工作。定义所有这些应用特性并将其传递给管理平台是云原生应用的基本前提。&lt;/p&gt;
&lt;p&gt;除了资源需求外，应用运行时还对平台管理的能力有依赖性，如数据存储或应用配置。&lt;/p&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;p&gt;了解容器的运行时要求很重要，主要有两个原因。首先，在定义了所有的运行时依赖和资源需求设想后，Kubernetes可以智能地决定在集群上的哪里运行容器以获得最有效的硬件利用率。在大量优先级不同的进程共享资源的环境中，要想成功共存，唯一的办法就是提前了解每个进程的需求。然而，智能投放只是硬币的一面。&lt;/p&gt;
&lt;p&gt;容器资源配置文件必不可少的第二个原因是容量规划。根据具体的服务需求和服务总量，我们可以针对不同的环境做一些容量规划，得出性价比最高的主机配置文件，来满足整个集群的需求。服务资源配置文件和容量规划相辅相成，才能长期成功地进行集群管理。&lt;/p&gt;
&lt;p&gt;在深入研究资源配置文件之前，我们先来看看如何声明运行时依赖关系。&lt;/p&gt;
&lt;h4 id=&#34;运行时依赖&#34;&gt;运行时依赖&lt;/h4&gt;
&lt;p&gt;最常见的运行时依赖之一是用于保存应用程序状态的文件存储。容器文件系统是短暂的，当容器关闭时就会丢失。Kubernetes提供了volume作为Pod级的存储实用程序，可以在容器重启后幸存。&lt;/p&gt;
&lt;p&gt;最直接的卷类型是emptyDir，只要Pod存活，它就会存活，当Pod被删除时，它的内容也会丢失。卷需要有其他类型的存储机制支持，才能有一个在Pod重启后仍能存活的卷。如果你的应用程序需要向这种长时间的存储设备读写文件，你必须在容器定义中使用volumes明确声明这种依赖性。
如例1-1所示。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;如例1-1，依赖于PV
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    volumeMounts:
    - mountPath:&amp;quot;/logs&amp;quot;
      name: log-volume
  volumes:
  - name: log-volume
    persistentVolumeClaim:
      claimName: random-generator-log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;调度器会评估Pod所需要的卷类型，这将影响Pod的调度位置。如果Pod需要的卷不是由集群上的任何节点提供的，那么Pod根本不会被调度。卷是运行时依赖性的一个例子，它影响Pod可以运行什么样的基础设施，以及Pod是否可以被调度。&lt;/p&gt;
&lt;p&gt;当你要求Kubernetes通过hostPort方式暴露容器端口为主机上特定端口时，也会发生类似的依赖关系。hostPort的使用在节点上创建了另一个运行时依赖性，并限制了Pod的调度位置。 hostPort在集群中的每个节点上保留了端口，并限制每个节点最多调度一个Pod。由于端口冲突，你可以扩展到Kubernetes集群中有多少节点就有多少Pod。&lt;/p&gt;
&lt;p&gt;另一种类型的依赖是配置。几乎每个应用程序都需要一些配置信息，Kubernetes提供的推荐解决方案是通过ConfigMaps。你的服务需要有一个消耗设置的策略&amp;ndash;无论是通过环境变量还是文件系统。无论是哪种情况，这都会引入你的容器对名为ConfigMaps的运行时依赖性。如果没有创建所有预期的 ConfigMaps，则容器被调度在节点上，但它们不会启动。ConfigMaps和Secrets在第19章Configuratio资源中进行了更详细的解释，例1-2展示了如何将这些资源用作运行时依赖。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;实例1-2所示，依赖于ConfigMap
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    env:
    - name: PATTERN
      valueFrom:
        configMapKeyRef:
          name: random-generator-config
          key: pattern
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;与ConfigMaps类似的概念是Secrets，它提供了一种略微更安全的方式将特定环境的配置分发到容器中。使用Secret的方式与使用ConfigMap的方式相同，它引入了从容器到namespace的相同依赖性。&lt;/p&gt;
&lt;p&gt;虽然ConfigMap和Secret对象的创建是我们必须执行的简单管理任务，但集群节点提供了存储和端口。其中一些依赖性限制了Pod被调度的位置（如果有的话），而其他依赖性则限制了Pod的运行。
可能会阻止Pod的启动。在设计带有这种依赖关系的容器化应用程序时，一定要考虑它们创建之后运行时的约束。&lt;/p&gt;
&lt;h4 id=&#34;资源配置文件&#34;&gt;资源配置文件&lt;/h4&gt;
&lt;p&gt;指定容器的依赖性，如ConfigMap、Secret和卷，是很直接的。我们需要更多的思考和实验来确定容器的资源需求。在Kubernetes的上下文中，计算资源被定义为可以被容器请求、分配给容器并从容器中获取的东西。资源分为可压缩的（即可以节制的，如CPU，或网络带宽）和不可压缩的（即不能节制的，如内存）。&lt;/p&gt;
&lt;p&gt;区分可压缩资源和不可压缩资源很重要。如果你的容器消耗了太多的可压缩资源（如CPU），它们就会被节流，但如果它们使用了太多的不可压缩资源（如内存），它们就会被杀死（因为没有其他方法可以要求应用程序释放分配的内存）。&lt;/p&gt;
&lt;p&gt;根据你的应用程序的性质和实现细节，你必须指定所需资源的最小量（称为请求）和它可以增长到的最大量（限制）。每个容器定义都可以以请求和限制的形式指定它所需要的CPU和内存量。在一个高层次上，请求/限制的概念类似于软/硬限制。例如，同样地，我们通过使用-Xms和-Xmx命令行选项来定义Java应用程序的堆大小。&lt;/p&gt;
&lt;p&gt;调度器将Pod调度到节点时，使用的是请求量（但不是限制）。对于一个给定的Pod，调度器只考虑那些仍有足够能力容纳Pod及其所有请求资源量相加容器的节点。从这个意义上说，每个容器的请求字段会影响到Pod可以被调度或不被调度的位置。例1-3显示了如何为Pod指定这种限制。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;实例1-3，资源限制
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec: 
  containers: 
  - image: k8spatterns/random-generator:1.0   name: random-generator 
    resources:
      requests:  
        cpu: 100m 
        memory: 100Mi 
      limits:  
        cpu: 200m 
        memory: 200Mi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;根据您是指定请求、限制，还是两者都指定，平台提供不同的服务质量（QoS）。&lt;/p&gt;
&lt;h5 id=&#34;best-effort&#34;&gt;Best-Effort&lt;/h5&gt;
&lt;p&gt;没有为其容器设置任何请求和限制的Pod。这样的Pod被认为是最低优先级的，当Pod的节点用完不可压缩资源时，会首先被干掉。&lt;/p&gt;
&lt;h5 id=&#34;burstable&#34;&gt;Burstable&lt;/h5&gt;
&lt;p&gt;已定义请求和限制的Pod，但它们并不相等（而且限制比预期的请求大）。这样的Pod有最小的资源保证，但也愿意在可用的情况下消耗更多的资源，直至其极限。当节点面临不可压缩的资源压力时，如果没有Best-Effort Pods剩余，这些Pod很可能被干掉。&lt;/p&gt;
&lt;h5 id=&#34;guaranteed&#34;&gt;Guaranteed&lt;/h5&gt;
&lt;p&gt;拥有同等数量请求和限制资源的Pod。这些是优先级最高的Pod，保证不会在Best-Effort和Burstable Pods之前被干掉。&lt;/p&gt;
&lt;p&gt;所以你为容器定义的资源特性或省略资源特性会直接影响到它的QoS，并定义了Pod在资源不足时的相对重要性。在定义你的Pod资源需求时，要考虑到这个后果。&lt;/p&gt;
&lt;h4 id=&#34;pod优先级&#34;&gt;Pod优先级&lt;/h4&gt;
&lt;p&gt;我们解释了容器资源声明如何也定义了Pod的QoS，并影响Kubelet在资源不足时干掉Pod中容器的顺序。另一个相关的功能，在写这篇文章的时候还在测试阶段，就是Pod优先和优先权。Pod优先级允许表明一个Pod相对于其他Pod的重要性，这影响了Pod的调度顺序。让我们在例子1-4中看到它的作用。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;实例1-4，pod优先级
apiVersion: scheduling.k8s.io/v1beta1
kind: PriorityClass
metadata: 
  name: high-priority 
value: 1000 
globalDefault: false
description: This is a very high priority Pod class
---
apiVersion: v1
kind: Pod
metadata: 
  name: random-generator 
  labels: 
    env: random-generator
spec: 
  containers: 
  - image: k8spatterns/random-generator:1.0 
    name: random-generator   
  priorityClassName: high-priority
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们创建了一个PriorityClass，这是一个非命名空间的对象，用于定义一个基于整数的优先级。我们的PriorityClass被命名为high-priority，优先级为1,000。现在我们可以通过它的名字将这个优先级分配给Pods，如priorityClassName：high-riority。PriorityClass是一种表示Pods相对重要性的机制，数值越高表示Pods越重要。&lt;/p&gt;
&lt;p&gt;启用Pod Priority功能后，它会影响调度器将Pod调度在节点上的顺序。首先，优先权进入许可控制器使用priorityClass Name字段来填充新Pod的优先权值。当有多个Pod等待调度时，调度器按最高优先级对待放Pod队列进行排序。在调度队列中，任何待定的Pod都会被选在其他优先级较低的待定Pod之前，如果没有阻止其调度的约束条件，该Pod就会被调度。&lt;/p&gt;
&lt;p&gt;下面是关键部分。如果没有足够容量的节点来调度Pod，调度器可以从节点上抢占（移除）优先级较低的Pod，以释放资源，调度优先级较高的Pod。因此，如果满足其他所有调度要求，优先级较高的Pod可能比优先级较低的Pod更早被调度。这种算法有效地使集群管理员能够控制哪些Pod是更关键的工作负载，并通过允许调度器驱逐优先级较低的Pod，以便在工作节点上为优先级较高的Pod腾出空间，将它们放在第一位。如果一个Pod不能被调度，调度器就会继续调度其他优先级较低的Pod。&lt;/p&gt;
&lt;p&gt;Pod QoS（前面已经讨论过了）和Pod优先级是两个正交的特性，它们之间没有联系，只有一点点重叠。QoS主要被Kubelet用来在可用计算资源较少时保持节点稳定性。==Kubelet在驱逐前首先考虑QoS，然后考虑Pods的PriorityClass。另一方面，调度器驱逐逻辑在选择抢占目标时完全忽略了Pods的QoS==。调度器试图挑选一组优先级最低的Pod，满足优先级较高的Pod等待调度的需求。&lt;/p&gt;
&lt;p&gt;当Pod具有指定的优先级时，它可能会对其他被驱逐的Pod产生不良影响。例如，当一个Pod的优雅终止策略受到重视，第10章中讨论的PodDisruptionBudget，单服务没有得到保证，这可能会打破一个依赖多数Pod数的较低优先级集群应用。&lt;/p&gt;
&lt;p&gt;另一个问题是恶意或不知情的用户创建了优先级最高的Pods，并驱逐了所有其他Pods。为了防止这种情况发生，ResourceQuota已经扩展到支持PriorityClass，较大的优先级数字被保留给通常不应该被抢占或驱逐的关键系统Pods。&lt;/p&gt;
&lt;p&gt;总而言之，Pod优先级应谨慎使用，因为用户指定的数字优先级，指导调度器和Kubelet调度或干掉哪些Pod，会受到用户的影响。任何改变都可能影响许多Pod，并可能阻止平台提供可预测的服务级别协议。&lt;/p&gt;
&lt;h4 id=&#34;项目资源&#34;&gt;项目资源&lt;/h4&gt;
&lt;p&gt;Kubernetes是一个自助服务平台，开发者可以在指定的隔离环境上运行他们认为合适的应用。然而，在一个共享的多租户平台中工作，也需要存在特定的边界和控制单元，以防止一些用户消耗平台的所有资源。其中一个这样的工具是ResourceQuota，它为限制命名空间中的聚合资源消耗提供了约束。通过ResourceQuotas，集群管理员可以限制消耗的计算资源（CPU、内存）和存储的总和。它还可以限制命名空间中创建的对象（如ConfigMaps、Secrets、Pods或Services）的总数。&lt;/p&gt;
&lt;p&gt;这方面的另一个有用的工具是LimitRange，它允许为每种类型的资源设置资源使用限制。除了指定不同资源类型的最小和最大允许量以及这些资源的默认值外，还可以控制请求和限制之间的比例，也就是所谓的超额承诺水平。表1-1给出了如何选择请求和限额的可能值的例子。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;表1-1. Limit和request ranges
Type       Resource  Min    Max  Default limit  Default request  Lim/req ratio  
Container  CPU       500m   2    500m           250m             4
Container  Memory    250Mi  2Gi  500Mi          250Mi            4
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;LimitRanges对于控制容器资源配置非常有用，这样就不会出现需要的资源超过集群节点所能提供的资源的容器。它还可以防止集群用户创建消耗大量资源的容器，使节点不能为其他容器分配资源。考虑到请求(而不是限制)是调度器用来调度的主要容器特性，LimitRequestRatio允许你控制容器的请求和限制之间的差距有多大。在请求和限制之间有很大的综合差距，会增加节点上超负荷的机会，并且当许多容器同时需要比最初请求更多的资源时，可能会降低应用性能。&lt;/p&gt;
&lt;h4 id=&#34;容量规划&#34;&gt;容量规划&lt;/h4&gt;
&lt;p&gt;考虑到容器在不同的环境中可能会有不同的资源情况，以及不同数量的实例，显然，多用途环境的容量规划并不简单。例如，为了获得最佳的硬件利用率，在一个非生产集群上，你可能主要拥有Best-Effort和Burstable容器。在这样的动态环境中，很多容器都是同时启动和关闭的，即使有容器在资源不足的时候被平台干掉，也不会致命。在生产集群上，我们希望事情更加稳定和可预测，容器可能主要是Guaranteed类型，还有一些Burstable。如果一个容器被杀死，那很可能是一个信号，说明集群的容量应该增加。&lt;/p&gt;
&lt;p&gt;当然，在现实生活中，你使用Kubernetes这样的平台，更可能的原因是还有很多服务需要管理，有些服务即将退出，有些服务还在设计开发阶段。即使是一个不断移动的目标，根据前面描述的类似方法，我们可以计算出每个环境中所有服务所需要的资源总量。&lt;/p&gt;
&lt;h2 id=&#34;讨论&#34;&gt;讨论&lt;/h2&gt;
&lt;p&gt;容器不仅对隔离进程和作为打包方式有用。在确定了资源概况后，它们也是成功进行产能规划的基石。进行一些早期测试，以发现每个容器的资源需求，并将该信息作为未来产能规划和预测的基础。&lt;/p&gt;
&lt;p&gt;然而，更重要的是，资源配置文件是应用程序与Kubernetes沟通的方式，以协助调度和管理决策。如果你的应用不提供任何请求或限制，Kubernetes能做的就是把你的容器当作不透明的盒子，当集群满了的时候就会丢掉。所以，每一个应用或多或少都要考虑和提供这些资源声明。&lt;/p&gt;
&lt;p&gt;现在你已经知道了如何确定我们应用的大小，在第3章 &amp;ldquo;声明式部署 &amp;ldquo;中，你将学习多种策略来让我们的应用在Kubernetes上安装和更新。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>存储总结</title>
      <link>https://kubesphereio.com/post/storage-summary/</link>
      <pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/storage-summary/</guid>
      <description>&lt;h2 id=&#34;ceph介绍&#34;&gt;ceph介绍&lt;/h2&gt;
&lt;p&gt;ceph为块存储，以下通过cephadm安装ceph集群，ceph主要有三个基本角色：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OSD 用于集群中所有数据与对象的存储，处理集群数据的复制、恢复、回填、再均衡，并向其他osd守护进程发送心跳，然后向 Monitor 提供一些监控信息。&lt;/li&gt;
&lt;li&gt;Monitor 监控整个集群的状态，维护集群的 cluster MAP 二进制表，保证集群数据的一致性。&lt;/li&gt;
&lt;li&gt;MDS (可选) 为 Ceph 文件系统提供元数据计算、缓存与同步。MDS 进程并不是必须的进程，只有需要使用 CephFS 时，才需要配置 MDS 节点。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1环境准备&#34;&gt;1、环境准备&lt;/h2&gt;
&lt;h3 id=&#34;11节点规划&#34;&gt;1.1、节点规划&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;节点  &lt;/th&gt;
&lt;th&gt;os  &lt;/th&gt;
&lt;th&gt;IP   &lt;/th&gt;
&lt;th&gt;磁盘&lt;/th&gt;
&lt;th&gt;角色&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ceph1&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.13&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;cephadm，mgr, mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph2&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.14&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph3&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.16&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;12修改hosts文件&#34;&gt;1.2、修改hosts文件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hostnamectl set-hostname ceph1  #节点1上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph2  #节点2上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph3  #节点3上执行&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;13配置hosts&#34;&gt;1.3、配置hosts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt; /etc/hosts &amp;lt;&amp;lt;EOF  
192.168.0.13 ceph1
192.168.0.14 ceph2
192.168.0.16 ceph3
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;14关闭防火墙&#34;&gt;1.4、关闭防火墙&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都要执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld
setenforce 0
sed -i &#39;s/^SELINUX=.*/SELINUX=disabled/&#39; /etc/selinux/config
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;15时间同步&#34;&gt;1.5、时间同步&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;yum install -y chrony
systemctl enable --now chronyd

以ceph1作为时间参考，以下在三个节点都操作，添加server ceph1 prefer
vi /etc/chrony.conf
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
server ceph1 prefer

重启chrony
systemctl restart chronyd
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;16dockerlvm2和python3安装&#34;&gt;1.6、docker、lvm2和python3安装&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;三个节点都需要执行：
curl -fsSL get.docker.com -o get-docker.sh 
sh get-docker.sh
systemctl enable docker
systemctl restart docker

yum install -y python3

yum -y install lvm2
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2cephadm安装ceph1&#34;&gt;2、cephadm安装(ceph1)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;官方下载方法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;git下载&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;git clone --single-branch --branch octopus https://github.com/ceph/ceph.git&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cp ceph/src/cephadm/cephadm ./&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cephadm安装(ceph1)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;./cephadm add-repo --release octopus
./cephadm install

mkdir -p /etc/ceph
cephadm bootstrap --mon-ip 192.168.0.13

安装完成提示：

             URL: https://ceph1:8443/
            User: admin
        Password: 86yvswzdzd

INFO:cephadm:You can access the Ceph CLI with:

        sudo /usr/sbin/cephadm shell --fsid 3861dbf4-fd47-11ea-a373-5254228af870 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

INFO:cephadm:Please consider enabling telemetry to help improve Ceph:

        ceph telemetry on

For more information see:

        https://docs.ceph.com/docs/master/mgr/telemetry/

INFO:cephadm:Bootstrap complete.
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;配置ceph.pub
将ceph1的/etc/ceph/ceph.pub放入到ceph2和ceph3的.ssh/authorized_keys,ceph2和ceph3下root目录下没有.ssh，得先创建，最低权限为700，authorized_keys最低权限600。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ceph指令生效&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;cephadm shell&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;添加节点&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph3&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3osd部署&#34;&gt;3、OSD部署&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cephadm shell下执行&lt;/li&gt;
&lt;li&gt;若块设备没显示，可加&amp;ndash;refresh参数&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[ceph: root@ceph1 /]# ceph orch device ls
HOST   PATH      TYPE   SIZE  DEVICE_ID     MODEL  VENDOR  ROTATIONAL  AVAIL  REJECT REASONS
ceph1  /dev/vdc  hdd   50.0G  vol-e801hi3v  n/a    0x1af4  1           True
ceph1  /dev/vda  hdd   20.0G  i-2iwis9yr    n/a    0x1af4  1           False  locked
ceph1  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
ceph2  /dev/vdc  hdd   50.0G  vol-lqcchpox  n/a    0x1af4  1           True
ceph2  /dev/vda  hdd   20.0G  i-r53b2flp    n/a    0x1af4  1           False  locked
ceph2  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  locked, Insufficient space (&amp;lt;5GB)
ceph3  /dev/vdc  hdd    100G  vol-p0eksa5m  n/a    0x1af4  1           True
ceph3  /dev/vda  hdd   20.0G  i-f6njdmtq    n/a    0x1af4  1           False  locked
ceph3  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
[ceph: root@ceph1 /]#

devcice确认true后，执行
[ceph: root@ceph1 /]# ceph orch apply osd --all-available-devices
Scheduled osd.all-available-devices update...
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;查看集群状态,ceph方式及运行容器方式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;ceph方式：
[ceph: root@ceph1 /]# ceph -s
  cluster:
    id:     af1f33f0-fd69-11ea-8530-5254228af870
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 66s)
    mgr: ceph1.rbarkj(active, since 12m), standbys: ceph2.uzdnki
    osd: 3 osds: 3 up (since 14s), 3 in (since 14s)

  data:
    pools:   1 pools, 1 pgs
    objects: 1 objects, 0 B
    usage:   3.0 GiB used, 197 GiB / 200 GiB avail
    pgs:     1 active+clean

容器方式：
[root@ceph1 ceph]# docker ps -a
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES
b417841153a9        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-osd -…&amp;quot;   5 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-osd.2
9dbd720f4a6f        prom/prometheus:v2.18.1      &amp;quot;/bin/prometheus --c…&amp;quot;   6 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-prometheus.ceph1
f860282adf85        prom/alertmanager:v0.20.0    &amp;quot;/bin/alertmanager -…&amp;quot;   6 minutes ago       Up 6 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-alertmanager.ceph1
a6cb4b354f46        ceph/ceph-grafana:6.6.2      &amp;quot;/bin/sh -c &#39;grafana…&amp;quot;   16 minutes ago      Up 16 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-grafana.ceph1
b745f57895f3        prom/node-exporter:v0.18.1   &amp;quot;/bin/node_exporter …&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-node-exporter.ceph1
b00c1a2c7ef6        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-crash…&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-crash.ceph1
99c703f18f67        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mgr -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mgr.ceph1.rbarkj
e972d3d3f13a        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mon -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mon.ceph1

&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.ceph.com/en/latest/cephadm/install/&#34;&gt;https://docs.ceph.com/en/latest/cephadm/install/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>k8s分布式存储总结</title>
      <link>https://kubesphereio.com/post/k8s-storage-summary/</link>
      <pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/k8s-storage-summary/</guid>
      <description>&lt;h2 id=&#34;1pvpvc和sc来源&#34;&gt;1、pv、pvc和sc来源&lt;/h2&gt;
&lt;p&gt;pv引入解耦了pod与底层存储；pvc引入分离声明与消费，分离开发与运维责任，存储由运维系统人员管理，开发人员只需要通过pvc声明需要存储的类型、大小和访问模式即可；sc引入使pv自动创建或删除，开发人员定义的pvc中声明stroageclassname以及大小等需求自动创建pv；运维人员只需要声明好sc以及quota配额即可，无需维护pv。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;早期pod使用volume方式，每次pod都需要配置存储，volume都需要配置存储插件的一堆配置，如果是第三方存储，配置非常复杂；强制开发人员需要了解底层存储类型和配置。从而引入了pv。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    volumeMounts:
    - mountPath: /data
      name: data
  volumes:
  - name: data
    capacity:
      storage: 10Gi
    cephfs:
      monitors:
      - 192.168.0.1:6789
      - 192.168.0.2:6789
      - 192.168.0.3:6789
      path: /opt/eshop_dir/eshop
      user: admin
      secretRef:
        name: ceph-secret

&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;pv的yaml文件，pv其实就是把volume的配置声明从pod中分离出来。存储系统由运维人员管理，开发人员不知道底层配置，所以引入了pvc。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: cephfs
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  cephfs:
    monitors:
    - 192.168.0.1:6789
    - 192.168.0.2:6789
    - 192.168.0.3:6789
    path: /opt/eshop_dir/eshop
    user: admin
    secretRef:
      name: ceph-secret
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;pvc的yaml文件，pvc会根据声明的大小、存储类型和accessMode等关键字查找pv，如果找到了匹配的pv，则会与之关联,而pod直接关联pvc。运维人员需要维护一堆pv，如果pv不够还需要手工创建新的pv，pv空闲还需要手动回收，所以引入了sc。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: cephfs
spec:
  accessModes:
      - ReadWriteMany
  resources:
      requests:
        storage: 8Gi
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;storageclass类似声明了一个非常大的存储池，其中一个最重要参数是provisioner，这个provisioner可以aws-ebs，ceph和nfs等。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: aws-gp2
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  fsType: ext4
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2存储发展过程&#34;&gt;2、存储发展过程&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;最初通过volume plugin实现，又称in-tree。&lt;/li&gt;
&lt;li&gt;1.8开始，新的插件形式支持外部存储系统，即FlexVolume,通过外部脚本集成外部存储接口。&lt;/li&gt;
&lt;li&gt;1.9开始，csi接入，存储厂商需要实现三个服务接口Identity Service、Controller Service、Node Service。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Identity service用于返回一些插件信息。
Controller Service实现Volume的curd操作。
Node Service运行在所有的Node节点，用于实现把volume挂载在当前Node节点的指定目录，该服务会监听一个socket，controller通过这个socket进行通信。
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;云原生分布式存储（Container Attached Storage）CAS&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;1、重新设计一个分布式存储，像openebs/longhorn/PortWorx/StorageOS。
2、已有的分布式存储包装管理，像Rook。
3、CAS：每个volume都由一个轻量级的controller来管理，这个controller可以是一个单独的pod；这个controller与使用该volume的应用pod在同一个node；不同的volume的数据使用多个独立的controller pod进行管理。
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3分布式存储分类&#34;&gt;3、分布式存储分类&lt;/h2&gt;
&lt;h4 id=&#34;31-块存储&#34;&gt;3.1 块存储&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;比如我们使用的ceph，ceph通过rbd实现块存储&lt;a href=&#34;https://kubesphereio.com/post/k8s-rook-install/&#34;&gt;rook搭建&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;32-共享存储&#34;&gt;3.2 共享存储&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;共享文件系统存储即提供文件系统存储接口，我们最常用的共享文件系统存储如NFS、CIFS、GlusterFS等，Ceph通过CephFS实现共享文件系统存储。&lt;a href=&#34;https://kubesphereio.com/post/linux-nfs-install/&#34;&gt;nfs搭建&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;33-对象存储&#34;&gt;3.3 对象存储&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Ceph通过RGW实现对象存储接口，RGW兼容AWS S3 API，因此Pod可以和使用S3一样使用Ceph RGW，比如Python可以使用boto3 SDK对桶和对象进行操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4各个存储fio性能测试供参考&#34;&gt;4、各个存储fio性能测试，供参考。&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://kubesphereio.com/img/storage-fio.png&#34; alt=&#34;存储性能对比&#34;&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Pmem与块存储性能对比</title>
      <link>https://kubesphereio.com/post/pmem-versus-block-storage-performance/</link>
      <pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/pmem-versus-block-storage-performance/</guid>
      <description>&lt;h2 id=&#34;1pmem介绍&#34;&gt;1、Pmem介绍&lt;/h2&gt;
&lt;p&gt;PMEM是硬件产品，Intel Optane DC持久存储模块，是一种具有大容量和数据持久性的创新存储技术。有2种运行模式。两级内存模式无需软件更改，DCPMM被视为更大的内存，并使用DRAM作为其缓存层。AppDirect模式将设备暴露为持久内存，支持软件栈，可用于加速不同的应用程序。在本文中，我们将使用AppDirect模式。&lt;/p&gt;
&lt;h2 id=&#34;2环境信息&#34;&gt;2、环境信息&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;一台master和一台node构成的K8s1.18.6集群&lt;/li&gt;
&lt;li&gt;redis镜像为根据源码编译为pmem-redis:4.0.0&lt;/li&gt;
&lt;li&gt;ceph/neonsan块存储&lt;/li&gt;
&lt;li&gt;centos7.7/内核5.8.7&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3测试两种模式&#34;&gt;3、测试两种模式&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;测试目的是体验PMEM对REDIS的加速效果。先在没有PMEM加速AOF模式，再有PMEM加速PBA模式。&lt;/li&gt;
&lt;li&gt;AOF模式是append only file的意思，通常REDIS是一种内存数据库，数据掉电就丢失了。AOF模式可以把数据库记录随时备份到分布式存储里，这样可以使得REDIS具有掉电恢复的功能。&lt;/li&gt;
&lt;li&gt;PBA模式是pointer based AOF模式，它是使用PMEM对AOF做了加速，原理是备份写盘时只把指针写到磁盘里，数据还在内存或PMEM里，使用PMEM作为缓存。这样既可以掉电恢复，又提升了性能。充分发挥了PMEM AD模式的优势。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4pmem-redis镜像构建&#34;&gt;4、pmem-redis镜像构建&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;git clone &lt;a href=&#34;https://github.com/clayding/opencloud_benchmark.git&#34;&gt;https://github.com/clayding/opencloud_benchmark.git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;cd opencloud_benchmark/k8s/redis/docker&lt;/li&gt;
&lt;li&gt;docker build -t pmem-redis:latest &amp;ndash;network host .&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;5pba模式的设置&#34;&gt;5、PBA模式的设置&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ipmctl安装&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;cd /etc/yum.repos.d/
wget https://copr.fedorainfracloud.org/coprs/jhli/ipmctl/repo/epel-7/jhli-ipmctl-epel-7.repo
wget https://copr.fedorainfracloud.org/coprs/jhli/safeclib/repo/epel-7/jhli-safeclib-epel-7.repo
yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
el-ha-for-rhel-*-server-rpms&amp;quot;
yum install ndctl ndctl-libs ndctl-devel libsafec rubygem-asciidoctor
yum install ipmctl
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;app direct模式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sudo ipmctl delete -goal
sudo ipmctl create -goal PersistentMemoryType=AppDirect

A reboot is required to process new memory allocation goals:
sudo reboot
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;检测Pmem能正常工作且为ad模式,ad是否有值&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;ipmctl show -memoryresources
 MemoryType   | DDR         | PMemModule  | Total
========================================================
 Volatile     | 191.000 GiB | 0.000 GiB   | 191.000 GiB
 AppDirect    | -           | 504.000 GiB | 504.000 GiB
 Cache        | 0.000 GiB   | -           | 0.000 GiB
 Inaccessible | 1.000 GiB   | 1.689 GiB   | 2.689 GiB
 Physical     | 192.000 GiB | 505.689 GiB | 697.689 GiB

 当ad没有值时，ipmctl start -diagnostic诊断是否有错误消息
 刚开始遇到这样的一个问题： 
 The platform configuration check detected that PMem module 0x0001 is not configured.
 分析为：新版Ipmctl有问题，用1.x的版本把pcd delete以后重新provision就可以了
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;6pmem-csi安装&#34;&gt;6、Pmem-csi安装&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;下载， https://github.com/intel/pmem-csi/blob/devel/docs/install.md#install-pmem-csi-driver
cd pmem-CSI

Setting up certificates for securities
# curl -L https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o _work/bin/cfssl --create-dirs
# curl -L https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o _work/bin/cfssljson --create-dirs
# chmod a+x _work/bin/cfssl _work/bin/cfssljson
# export PATH=$PATH:$PWD/_work/bin
# ./test/setup-ca-kubernetes.sh

Deploying the driver to K8s using LVM mode, please choose yaml files corresponding to your kubernetes version
# kubectl create -f deploy/kubernetes-1.18/pmem-csi-lvm.yaml
Applying a storage class
# kubectl apply -f deploy/kubernetes-1.18/pmem-storageclass-ext4.yaml
pod状态
kubectl get pod
NAME                    READY   STATUS        RESTARTS   AGE
pmem-csi-controller-0   2/2     Running       0          22s
pmem-csi-node-tw4mw     2/2     Running       2          33h
sc状态
kubectl get sc
NAME               PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
pmem-csi-sc-ext4   pmem-csi.intel.com         Delete          Immediate           false                  3d2h
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;7benchmark安装&#34;&gt;7、benchmark安装&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;yum install autoconf automake make gcc-c++&lt;/li&gt;
&lt;li&gt;yum install pcre-devel zlib-devel libmemcached-devel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Remove system libevent and install new version:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sudo yum remove libevent&lt;/li&gt;
&lt;li&gt;wget &lt;a href=&#34;https://github.com/downloads/libevent/libevent/libevent-2.0.21-stable.tar.gz&#34;&gt;https://github.com/downloads/libevent/libevent/libevent-2.0.21-stable.tar.gz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;tar xfz libevent-2.0.21-stable.tar.gz&lt;/li&gt;
&lt;li&gt;pushd libevent-2.0.21-stable&lt;/li&gt;
&lt;li&gt;./configure&lt;/li&gt;
&lt;li&gt;make&lt;/li&gt;
&lt;li&gt;sudo make install&lt;/li&gt;
&lt;li&gt;popd&lt;/li&gt;
&lt;li&gt;export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:${PKG_CONFIG_PATH}&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Build and install memtier:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;git clone &lt;a href=&#34;https://github.com/RedisLabs/memtier_benchmark&#34;&gt;https://github.com/RedisLabs/memtier_benchmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;cd memtier_benchmark&lt;/li&gt;
&lt;li&gt;autoreconf -ivf&lt;/li&gt;
&lt;li&gt;./configure &amp;ndash;disable-tls&lt;/li&gt;
&lt;li&gt;make&lt;/li&gt;
&lt;li&gt;sudo make install&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;8测试对比&#34;&gt;8、测试对比&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;aof的yanl文件&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language----&#34; data-lang=&#34;---&#34;&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 2Gi
  storageClassName: csi-neonsan
---
kind: Pod
apiVersion: v1
metadata:
  name: redis-aof
  labels:
    app: redis-aof
spec:
  containers:
    - name: redis-aof
      image: pmem-redis:latest
      imagePullPolicy: IfNotPresent
      args: [&amp;quot;no&amp;quot;]
      ports:
      - containerPort: 6379
      resources:
        limits:
          cpu: &amp;quot;1000m&amp;quot;
          memory: &amp;quot;16Gi&amp;quot;
        requests:
          cpu: &amp;quot;1000m&amp;quot;
          memory: &amp;quot;16Gi&amp;quot;
      volumeMounts:
      - mountPath: &amp;quot;/ceph&amp;quot;
        name: ceph-csi-volume
  volumes:
  - name: ceph-csi-volume
    persistentVolumeClaim:
      claimName: rbd-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: redis
  labels:
    app: redis-aof
spec:
  type: NodePort
  ports:
  - name: redis
    port: 6379
    nodePort: 30379
    targetPort: 6379
  selector:
    app: redis-aof
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;pba的yanl文件&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pmem-csi-pvc-ext4
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  storageClassName: pmem-csi-sc-ext4 # defined in pmem-storageclass-ext4.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 2Gi
  storageClassName: csi-neonsan
---
kind: Pod
apiVersion: v1
metadata:
  name: redis-with-pba
  labels:
    app: redis-with-pba
spec:
  containers:
    - name: redis-with-pba
      image: pmem-redis:latest
      imagePullPolicy: IfNotPresent
      args: [&amp;quot;yes&amp;quot;]
      ports:
      - containerPort: 6379
      resources:
        limits:
          cpu: &amp;quot;1000m&amp;quot;
          memory: &amp;quot;16Gi&amp;quot;
        requests:
          cpu: &amp;quot;1000m&amp;quot;
          memory: &amp;quot;16Gi&amp;quot;
      volumeMounts:
      - mountPath: &amp;quot;/data&amp;quot;
        name: my-csi-volume
      - mountPath: &amp;quot;/ceph&amp;quot;
        name: ceph-csi-volume
  volumes:
  - name: ceph-csi-volume
    persistentVolumeClaim:
      claimName: rbd-pvc
  - name: my-csi-volume
    persistentVolumeClaim:
      claimName: pmem-csi-pvc-ext4
---
apiVersion: v1
kind: Service
metadata:
  name: redis-pba
  labels:
    app: redis-with-pba
spec:
  type: NodePort
  ports:
  - name: redis
    port: 6379
    nodePort: 31379
    targetPort: 6379
  selector:
    app: redis-with-pba
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;先执行aof的yaml文件，kubectl apply -f aof.yaml，然后再执行memtier_benchmark&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;memtier_benchmark -s 172.30.35.9  -p 30379 -R -d 1024 --key-maximum=1000000 -n 10000  --ratio=1:9 | grep Totals&amp;gt;&amp;gt;aof_1024
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 100%,  37 secs]  0 threads:     2000000 ops,   53289 (avg:   53544) ops/sec, 7.23MB/sec (avg: 7.29MB/sec),  3.75 (avg:  3.73) msec latency



[root@neonsan-10 scripts]# cat aof_1024
Totals      53508.29        26.75     48130.71         3.73350         3.27900         8.31900        18.43100      7456.87
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;先执行pba的yaml文件，kubectl apply -f pba.yaml，然后再执行memtier_benchmark,注意yaml文件里面同时挂载不同存储。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;memtier_benchmark -s 172.30.35.9  -p 31379 -R -d 1024 --key-maximum=1000000 -n 10000  --ratio=1:9 | grep Totals&amp;gt;&amp;gt;pba_1024
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 100%,  24 secs]  0 threads:     2000000 ops,   81619 (avg:   81294) ops/sec, 11.07MB/sec (avg: 11.10MB/sec),  2.45 (avg:  2.46) msec latency


[root@neonsan-10 scripts]# cat pba_1024
Totals          0.00         0.00         0.00            -nan         0.00000         0.00000         0.00000         0.00
Totals      82856.35        82.86     74487.86         2.45929         2.38300         4.79900         6.30300     11588.37
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;通过速度和延迟性比较两种存储的性能。&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>CkA考试经验</title>
      <link>https://kubesphereio.com/post/cka-test-experience/</link>
      <pubDate>Sat, 11 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/cka-test-experience/</guid>
      <description>&lt;h1 id=&#34;cka考试经验&#34;&gt;CKA考试经验&lt;/h1&gt;
&lt;p&gt;CKA: Kubernetes管理员认证（CKA）旨在确保认证持有者具备履行Kubernetes管理员职责的技能，知识和能力。如果企业想要申请 KCSP ，条件之一是：至少需要三名员工拥有CKA认证。
&lt;img src=&#34;https://kubesphereio.com/img/cka.jpg&#34; alt=&#34;CKA图片.jpg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-考试报名&#34;&gt;1. 考试报名&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;%5Bhttps://www.cncf.io/certification/cka/%5D&#34;&gt;CKA报名地址&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;注意事项：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;1、报名成功之后，可在12个月之内进行考试，考试不过有一次补考机会。&lt;/p&gt;
&lt;p&gt;2、CKA：74分或以上可以获得证书。&lt;/p&gt;
&lt;p&gt;3、每年Cyber Monday（网络星期一，也就是黑五后的第一个星期一）有优惠或者某些辅导架构有优惠券。&lt;/p&gt;
&lt;h2 id=&#34;2-备考&#34;&gt;2. 备考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;考试时你可以打开两个浏览器Tab，一个是考试窗口，一个用来查阅官方文档（仅允许访问https://kubernetes.io/docs/、https://github.com/kubernetes/ 和https://kubernetes.io/blog/ ）&lt;/li&gt;
&lt;li&gt;查询文档的浏览器Tab可以弄成标签。&lt;/li&gt;
&lt;li&gt;考试前一周看下官网k8s版本，然后部署一个同样版本的K8s练习。&lt;/li&gt;
&lt;li&gt;可以参考考试大纲复习。&lt;/li&gt;
&lt;li&gt;怎么复习，可以在自己搭建的环境下，操作指令，生成对应的。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3-考前检查及考试环境&#34;&gt;3. 考前检查及考试环境&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;考试形式: 在线监控，需要共享桌面和摄像头&lt;/li&gt;
&lt;li&gt;考试环境: 在一个密闭空间，例如书房、卧室、会议室等，电脑屏幕不能对着窗户，房间里除了考生不能存在第二个人，考试的桌面不能放其它东西，水杯也不行&lt;/li&gt;
&lt;li&gt;考试时间及题目: CKA-3小时-24道题&lt;/li&gt;
&lt;li&gt;选择考试时间: 报名成功之后可以在12个月之内进行考试，考试之前需要选择考试时间，选择考试时间的时候记得先选择北京时区，默认是0时区时间。&lt;/li&gt;
&lt;li&gt;电脑要求: 可以在这里WebDelivery Compatibility Check检测自己的电脑环境和网络速度等&lt;/li&gt;
&lt;li&gt;选择的是Linux-Foundation&amp;mdash;&amp;gt;CKA-English&lt;/li&gt;
&lt;li&gt;考试前考官检查:
考试可以提前15分钟进入考试界面
考官会以发消息的方式和你交流（没有语音交流）
看不懂考官发的英文怎么办：可以在chrome浏览器右键翻译
考官会让你共享摄像头，共享桌面 考官会让你出示能确认你身份ID的证件，我当时用的是罗技C310摄像头，无法对焦，护照看上去模糊到不行，后来考官又叫我给护照打光还是不行，后面又叫我打开我的手机，用手机相机当作放大镜用，这样才能看清楚。（我考CKAD的时候，我护照还没举稳，考官就说可以了，应该是考过CKA，他们系统里面已经有我的信息了，就随便瞄了一眼而已）
考官会让你用摄像头环视房间一周，确认你的考试环境（当时我房间门开了一个小缝也要求我去把门关好，还是比较严格）
考官会让你用摄像头看你的整个桌面和桌子底下
考官会让你打开任务管理器，点击左下角简略信息，是否已关闭了其它后台服务。
考官会让你再次点一下桌面共享，然后你叫你点击取消，然后就开始进入考试了&lt;/li&gt;
&lt;li&gt;考试的界面:
左边是题目
右边是终端
终端上面是共享摄像头、共享屏幕、考试信息等按钮（可以唤出记事本）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-考试心得&#34;&gt;4. 考试心得&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;有Notepad记事本，可以记录下自己环境信息，哪道题还没做，题目中的信息等。&lt;/li&gt;
&lt;li&gt;一定要记得用鼠标，拷贝和粘贴特别方便。&lt;/li&gt;
&lt;li&gt;尽量在官网中拷贝yaml文件到答题环境中。&lt;/li&gt;
&lt;li&gt;很多指令记得不清楚，请使用-h，比如etcdctl,node不能调度等。&lt;/li&gt;
&lt;li&gt;特别重要，根据个人考试，然后在浏览器中收藏的记录为：
&lt;img src=&#34;https://kubesphereio.com/img/content.png&#34; alt=&#34;CKA考试相关内容.png&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>iptables指令将容器内部端口映射到外部宿主机端口指南</title>
      <link>https://kubesphereio.com/post/the-iptables-directive-is-a-guide-to-mapping-ports-inside-containers-to-ports-on-external-hosts/</link>
      <pubDate>Tue, 07 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/the-iptables-directive-is-a-guide-to-mapping-ports-inside-containers-to-ports-on-external-hosts/</guid>
      <description>&lt;h4 id=&#34;背景&#34;&gt;背景：&lt;/h4&gt;
&lt;p&gt;docker run 某个容器，忘记了-p/-P 映射端口操作时，怎么把容器端口映射到主机上呢？以下描述的是如何通过iptables指令把容器端口映射到外部宿主机端口操作，防止容器重新创建。
宿主机docker启了一个容器，在容器里面又部署了一个pod，而部署pod这个服务是后续操作的，宿主机docker启容器时没有把端口映射出来，如何通过宿主机去访问pod服务。&lt;/p&gt;
&lt;h2 id=&#34;1相关联的认知&#34;&gt;1、相关联的认知&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;docker run -p: 指令中的小p为具体的宿主机端口映射到容器内部开放的网络端口上。&lt;/li&gt;
&lt;li&gt;docker run -P: 指令中的大P为随机选择一个宿主机端口映射到容器内部开放的网络端口上。&lt;/li&gt;
&lt;li&gt;docker run -p: 可以绑定多IP和端口（跟多个-p）。&lt;/li&gt;
&lt;li&gt;kubectl expose &amp;ndash;type=nodePort: 指令将容器内部端口映射到主机上(宿主机为随机端口，范围30000-32767/在service中编辑修改为具体端口)。&lt;/li&gt;
&lt;li&gt;kubectl expose &amp;ndash;type=lb: 指令，为直接将容器服务暴露出去。&lt;/li&gt;
&lt;li&gt;kubectl ingress: 它允许你基于路径或者子域名来路由流量到后端服务,7层协议http/https。&lt;/li&gt;
&lt;li&gt;kubectl port-forward: 将容器端口转发至本地端口，也可以转发TCP流量。&lt;/li&gt;
&lt;li&gt;kubectl kube-proxy: 只能转发HTTP流量。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2docker与iptables关系&#34;&gt;2、docker与iptables关系&lt;/h2&gt;
&lt;p&gt;源地址变换规则、目标地址变换规则、自定义限制外部ip规则、docker容器间通信iptables规则、docker网络与ip-forward和具体的用iptables指令将容器内部端口映射到外部宿主机端口操作指令。&lt;/p&gt;
&lt;h3 id=&#34;21源ip地址变换规则&#34;&gt;2.1、源ip地址变换规则&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Docker安装完成后，将默认在宿主机系统上增加一些iptables规则，以用于Docker容器和容器之间以及和外界的通信，可以使用iptables-save命令查看。&lt;/li&gt;
&lt;li&gt;其中nat表中的POSTROUTING链有这么一条规则&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE

参数说明：
-s ：源地址172.17.0.0/16
-o：指定数据报文流出接口为docker0
-j ：动作为MASQUERADE（地址伪装）
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;上面这条规则关系着Docker容器和外界的通信，含义是：
判断源地址为172.17.0.0/16的数据包（即Docker容器发出的数据），当不是从docker0网卡发出时做SNAT（源地址转换）。
这样一来，从Docker容器访问外网的流量，在外部看来就是从宿主机上发出的，外部感觉不到Docker容器的存在。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;22目标地址变换规则&#34;&gt;2.2、目标地址变换规则&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;从Docker容器访问外网的流量，在外部看来就是从宿主机上发出的，外部感觉不到Docker容器的存在。那么，外界想到访问Docker容器的服务时，同样需要相应的iptables规则.&lt;/li&gt;
&lt;li&gt;以启动tomcat容器，将其8080端口映射到宿主机上的8080端口为例,然后通过iptables-save查看：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;docker run -itd --name  tomcat -p 8080:8080 tomcat:latest
#iptables-save
*nat
-A POSTROUTING -s 172.18.0.2/32 -d 172.18.0.2/32 -p tcp -m tcp --dport 8080 -j MASQUERADE
...
*filter
-A DOCKER -d 172.18.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 8080 -j ACCEPT
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;可以看到，在nat、filter的Docker链中分别增加了一条规则&lt;/li&gt;
&lt;li&gt;这两条规则将访问宿主机8080端口的流量转发到了172.17.0.4的8080端口上（即真正提供服务的Docker容器IP和端口）。所以外界访问Docker容器是通过iptables做DNAT（目的地址转换）实现的。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;23自定义限制外部ip规则&#34;&gt;2.3、自定义限制外部ip规则&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Docker的forward规则默认允许所有的外部IP访问容器&lt;/li&gt;
&lt;li&gt;可以通过在filter的DOCKER链上添加规则来对外部的IP访问做出限制&lt;/li&gt;
&lt;li&gt;只允许源IP192.168.0.0/16的数据包访问容器，需要添加如下规则：
&lt;code&gt;iptables -I DOCKER -i docker0 ! -s 192.168.0.0/16 -j DROP&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;24docker容器间通信iptables规则&#34;&gt;2.4、docker容器间通信iptables规则&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;不仅仅是与外界间通信，Docker容器之间互个通信也受到iptables规则限制。&lt;/li&gt;
&lt;li&gt;同一台宿主机上的Docker容器默认都连在docker0网桥上，它们属于一个子网，这是满足相互通信的第一步。&lt;/li&gt;
&lt;li&gt;Docker daemon启动参数&amp;ndash;icc(icc参数表示是否允许容器间相互通信)设置为false时会在filter的FORWARD链中增加一条ACCEPT的规则（&amp;ndash;icc=true）：
&lt;code&gt;-A FORWARD -i docker0 -o docker0 -j ACCEPT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;当Docker datemon启动参数&amp;ndash;icc设置为false时，以上规则会被设置为DROP，Docker容器间的相互通信就被禁止,默认是ACCEPT。&lt;/li&gt;
&lt;li&gt;这种情况下，想让两个容器通信就需要在docker run时使用&amp;ndash;link选项。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;25docker网络与ip-forward&#34;&gt;2.5、docker网络与ip-forward&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;在Docker容器和外界通信的过程中，还涉及了数据包在多个网卡间的转发，如从docker0网卡转发到宿主机ens160网卡，这需要内核将ip-forward功能打开&lt;/li&gt;
&lt;li&gt;即将ip_forward系统参数设1：echo 1 &amp;gt; /proc/sys/net/ipv4/ip_forward&lt;/li&gt;
&lt;li&gt;Docker daemon启动的时候默认会将其设为1（&amp;ndash;ip-forward=true）&lt;/li&gt;
&lt;li&gt;永久生效的ip转发
&lt;code&gt;vim /etc/sysctl.conf&lt;/code&gt;
&lt;code&gt;net.ipv4.ip_forward = 1&lt;/code&gt;
&lt;code&gt;sysctl -p /etc/sysctl.conf&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;26iptables指令映射&#34;&gt;2.6、iptables指令映射&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;需要执行三条指令,其中就修改两个参数:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;iptables -t nat -A DOCKER -p tcp --dport ${YOURPORT} -j DNAT --to-destination ${CONTAINERIP}:${YOURPORT}

iptables -t nat -A POSTROUTING -j MASQUERADE -p tcp --source ${CONTAINERIP} --destination ${CONTAINERIP} --dport ${YOURPORT}

iptables -A DOCKER -j ACCEPT -p tcp --destination ${CONTAINERIP} --dport ${YOURPORT}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;${CONTAINERIP} 就是对应容器的ip地址，比如我的容器ip地址是 172.18.0.2 ，（容器的IP可以通过如下方式查看：a.在容器中：ip addr;b.在宿主机中: docker inspect 容器名 |grep IPAddress ）所以我就把上述的参数换成我的IP地址。&lt;/li&gt;
&lt;li&gt;${YOURPORT} 就是要映射出来的端口，我配置的是一个console平台，其端口是30880&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3注意地方及参考&#34;&gt;3、注意地方及参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如果容器是pod形式启的，上面iptables指令映射不适合，其中有对docker链的操作。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/19335444/how-do-i-assign-a-port-mapping-to-an-existing-docker-container&#34;&gt;映射port至存在的docker容器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/zh/docs/concepts/services-networking/service/&#34;&gt;k8s如何访问&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>chart包实践开发指南</title>
      <link>https://kubesphereio.com/post/chart-package-practice-development-guide/</link>
      <pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/chart-package-practice-development-guide/</guid>
      <description>&lt;h2 id=&#34;chart包文件结构以wordpress包为例&#34;&gt;chart包文件结构，以wordpress包为例&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;wordpress/
  Chart.yaml          # 包含有关chart信息的YAML文件
  LICENSE             # OPTIONAL: 包含chart许可证的纯文本文件
  README.md           # OPTIONAL: 一个可读的README文件
  requirements.yaml   # OPTIONAL: 一个YAML文件，列出了chart的依赖关系
  values.yaml         # 该chart的默认配置值
  charts/             # OPTIONAL: 包含此chart所依赖的任何chart的目录。
  templates/          # OPTIONAL: 一个模板目录，当与values相结合时，
                      # 将生成有效的Kubernetes清单文件
  templates/NOTES.txt # OPTIONAL: 包含简短使用说明的纯文本文件
  templates/_helpers.tpl # OPTIONAL:通过define 函数定义命名模板
  crds/               # OPTIONAL: 自定义资源
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;以下说明chart包内容&#34;&gt;以下说明chart包内容&lt;/h3&gt;
&lt;h2 id=&#34;chartyaml文件内容格式&#34;&gt;Chart.yaml文件内容格式&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;name: chart的名称 (required)
version: 一个SemVer 2(语义化版本)版本(required)
description: 这个项目的单句描述 (optional)
keywords:
  - 关于此项目的关键字列表 (optional)
home: 该项目主页的URL(optional)
sources:
  - 此项目的源代码URL列表 (optional)
dependencies: chart依赖关系 (optional)
  - name: chart名字 (nginx)
    version: chart版本 (&amp;quot;1.2.3&amp;quot;)
    repository: url仓库 (&amp;quot;https://example.com/charts&amp;quot;) 
    condition: (optional) 布尔值的yaml路径，用于启用/禁用图表 
maintainers: # (optional)
  - name: 维护者的名称 (每个维护者都需要)
    email: 维护者的email (optional for each maintainer)
    url: 维护者的url (optional for each maintainer)
engine: gotpl＃模板引擎的名称（可选，默认为gotpl）
icon: 要用作图标的SVG或PNG图像的URL (optional)
appVersion: 包含的应用程序版本（可选）这个不一定是SemVer
deprecated: 此chart是否已被弃用（可选，布尔型）
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;readmemd内容&#34;&gt;README.md内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Introduction&lt;/li&gt;
&lt;li&gt;Prerequisites&lt;/li&gt;
&lt;li&gt;Installing the Chart&lt;/li&gt;
&lt;li&gt;Uninstalling the Chart&lt;/li&gt;
&lt;li&gt;Configuration&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;requirementsyaml介绍&#34;&gt;requirements.yaml介绍&lt;/h2&gt;
&lt;p&gt;在Helm中，一个chart可能取决于任何数量的其他chart。 这些依赖关系可以通过requirements.yaml文件动态链接，或者引入charts/目录并手动管理。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dependencies:
  - name: apache
    version: 1.2.3
    repository: http://example.com/charts
    alias: new-subchart-1
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Name: 你想要的chart的名称。&lt;/li&gt;
&lt;li&gt;Version: 你想要的chart的版本。&lt;/li&gt;
&lt;li&gt;repository字段是图表存储库的完整URL。 请注意，您还必须使用helm repo add在本地添加该repository。&lt;/li&gt;
&lt;li&gt;alias：别名。
一旦有一个依赖关系文件，可以运行helm dependency update，它会使用你的依赖关系文件为你下载所有指定的chart到你的charts/目录中。
可以在values.yaml定义true/false判断依赖包是否被启用，如&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apache:
  enabled: true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;依赖关系可以是chart压缩包（foo-1.2.3.tgz），也可以是未打包的chart目录。
依赖执行顺序：参考k8s负载自启动原理，所以我们可以不关心执行顺利。实际上交叉执行。&lt;/p&gt;
&lt;h4 id=&#34;说明helm2是通过requirementsyaml文件描述依赖关系helm3直接在chartyaml描述&#34;&gt;说明：helm2是通过requirements.yaml文件描述依赖关系，helm3直接在Chart.yaml描述。&lt;/h4&gt;
&lt;h2 id=&#34;templatesk8s资源&#34;&gt;templates/k8s资源&lt;/h2&gt;
&lt;p&gt;templates下有多个deployment对象，可以命名不同名字。
执行顺序：参考k8s负载自启动原理，所以我们可以不关心执行顺利。
实际执行顺序为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var InstallOrder KindSortOrder = []string{
    &amp;quot;Namespace&amp;quot;,
    &amp;quot;NetworkPolicy&amp;quot;,
    &amp;quot;ResourceQuota&amp;quot;,
    &amp;quot;LimitRange&amp;quot;,
    &amp;quot;PodSecurityPolicy&amp;quot;,
    &amp;quot;PodDisruptionBudget&amp;quot;,
    &amp;quot;Secret&amp;quot;,
    &amp;quot;ConfigMap&amp;quot;,
    &amp;quot;StorageClass&amp;quot;,
    &amp;quot;PersistentVolume&amp;quot;,
    &amp;quot;PersistentVolumeClaim&amp;quot;,
    &amp;quot;ServiceAccount&amp;quot;,
    &amp;quot;CustomResourceDefinition&amp;quot;,
    &amp;quot;ClusterRole&amp;quot;,
    &amp;quot;ClusterRoleList&amp;quot;,
    &amp;quot;ClusterRoleBinding&amp;quot;,
    &amp;quot;ClusterRoleBindingList&amp;quot;,
    &amp;quot;Role&amp;quot;,
    &amp;quot;RoleList&amp;quot;,
    &amp;quot;RoleBinding&amp;quot;,
    &amp;quot;RoleBindingList&amp;quot;,
    &amp;quot;Service&amp;quot;,
    &amp;quot;DaemonSet&amp;quot;,
    &amp;quot;Pod&amp;quot;,
    &amp;quot;ReplicationController&amp;quot;,
    &amp;quot;ReplicaSet&amp;quot;,
    &amp;quot;Deployment&amp;quot;,
    &amp;quot;HorizontalPodAutoscaler&amp;quot;,
    &amp;quot;StatefulSet&amp;quot;,
    &amp;quot;Job&amp;quot;,
    &amp;quot;CronJob&amp;quot;,
    &amp;quot;Ingress&amp;quot;,
    &amp;quot;APIService&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;两种方式可以提前执行,一种设置pre-install,另一种是设置权重：
pre-install hooks，如：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: foo
  annotations:
    &amp;quot;helm.sh/hook&amp;quot;: &amp;quot;pre-install&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;定义权重，如：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;annotations:
    &amp;quot;helm.sh/hook-weight&amp;quot;: &amp;quot;5&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;valuesyaml&#34;&gt;values.yaml&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Release.Name: release的名称(不是chart的名称！)&lt;/li&gt;
&lt;li&gt;Release.Namespace: chart release的名称空间。&lt;/li&gt;
&lt;li&gt;Release.Service: 进行release的服务。 通常这是Tiller。&lt;/li&gt;
&lt;li&gt;chart版本可以作为Chart.Version获得。Chart：Chart.yaml 的内容。&lt;/li&gt;
&lt;li&gt;templates下有多个deployment对象，可以命名不同名字，然后在values.yaml以不同名字打头定义值。，如以下格式定义：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;mysql:
  name: 
  image:
    repository: 
    tag: 
    pullPolicy:

redis:
  name: 
  image:
    repository: 
    tag: 
    pullPolicy:
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;helm模板&#34;&gt;helm模板&lt;/h2&gt;
&lt;p&gt;helm模板语法嵌套在{{和}}之间，有三个常见的&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.Values.*
从value.yaml文件中读取或者--set获取（--set优先级最大）。
.Release.*
从运行Release的元数据读取,每次安装均会生成一个新的release
template * .
从_helpers.tpl文件中读取，通过define 函数定义命名模板
.Chart.*
从Chart.yaml文件中读取
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;模板函数和管道&#34;&gt;模板函数和管道&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;* | 管道，类似linux下的管道，以下实例效果是一样的。
{{ quote .Values.favorite.drink }}与 {{ .Values.favorite.drink | quote }}
* default制定默认值
drink: {{ .Values.favorite.drink | default “tea” | quote }}
* indent 模板函数，对左空出空格，左边空出两个空格
{{ include &amp;quot;mychart_app&amp;quot; . | indent 2 }}
include 函数，与 template 类似功能
如实例，在_helpers.tpl中define模板，在资源对象中引用。


{{- define &amp;quot;mychart.labels&amp;quot; }}
  labels:
    generator: helm
    date: {{ now | htmlDate }}
{{- end }}

apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-configmap
  {{- template &amp;quot;mychart.labels&amp;quot; }}
data:
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;在模板中使用文件&#34;&gt;在模板中使用文件&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: conf
data:
{{ (.Files.Glob &amp;quot;foo/*&amp;quot;).AsConfig | indent 2 }}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;chart根目录下foo目录的所有文件配置为configmap的内容&lt;/p&gt;
&lt;h2 id=&#34;模板流程控制&#34;&gt;模板流程控制&lt;/h2&gt;
&lt;p&gt;常用的有
if/else 条件控制
with 范围控制
range 循环控制
如：values.yaml中定义变量，ConfigMap中.Values.favorite循环控制参数。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;favorite:
  drink: coffee
  food: pizza

apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-configmap
data:
  myvalue: &amp;quot;Hello World&amp;quot;
  {{- range $key, $val := .Values.favorite }}
  {{ $key }}: {{ $val | quote }}
  {{- end}}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在deployment.yaml文件中使用if/else语法，如：- end结束标志，双括号都有“-”。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{- if .Values.image.repository -}}
image: {.Values.image.repository}
{{- else -}}
image: &amp;quot;***/{{ .Release.Name }}:{{ .Values.image.version }}&amp;quot;
{{- end -}}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>ipv6地址搭建K8s</title>
      <link>https://kubesphereio.com/post/ipv6-address-setup-k8s/</link>
      <pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/ipv6-address-setup-k8s/</guid>
      <description>&lt;h1 id=&#34;ipv6地址搭建k8s&#34;&gt;ipv6地址搭建K8s&lt;/h1&gt;
&lt;h3 id=&#34;环境&#34;&gt;环境&lt;/h3&gt;
&lt;p&gt;centos: 7.7
k8s: v1.16.0&lt;/p&gt;
&lt;h3 id=&#34;提前准备&#34;&gt;提前准备&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;修改主机名
&lt;code&gt;hostnamectl set-hostname node1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;添加ipv6地址及主机名
&lt;code&gt;vi /etc/hosts&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;添加操作系统的ipv6的参数，且使参数生效&lt;code&gt;sysctl -p&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vi /etc/sysctl.conf
net.ipv6.conf.all.disable_ipv6 = 0
net.ipv6.conf.default.disable_ipv6 = 0
net.ipv6.conf.lo.disable_ipv6 = 0
net.ipv6.conf.all.forwarding=1
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;开启ipv6,添加如下内容&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vi /etc/sysconfig/network
NETWORKING_IPV6=yes
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;开启网卡的ipv6,添加如下内容，最后执行reboot生效。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vi /etc/sysconfig/network-scripts/ifcfg-eth0
IPV6INIT=yes
IPV6_AUTOCONF=yes
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;关闭防火墙&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;systemctl stop firewalld
systemctl disable firewalld
setenforce 0
vi /etc/selinux/config
SELINUX=disabled
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;关闭虚拟内存,添加如下内容，最后通过执行sysctl -p /etc/sysctl.d/k8s.conf生效。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;swapoff -a
vi /etc/sysctl.d/k8s.conf 添加下面一行：
vm.swappiness=0
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;安装docker&#34;&gt;安装docker&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;yum install -y yum-utils device-mapper-persistent-data lvm2
yum install wget -y
wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo
sudo sed -i &#39;s+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+&#39; /etc/yum.repos.d/docker-ce.repo
yum makecache fast
yum install docker-ce -y
systemctl enable docker;systemctl restart docker
docker的配置vi /etc/docker/daemon.json
{
&amp;quot;insecure-registry&amp;quot;:[&amp;quot;0.0.0.0/0&amp;quot;],
&amp;quot;ipv6&amp;quot;: true,
&amp;quot;fixed-cidr-v6&amp;quot;: &amp;quot;2001:db8:1::/64&amp;quot;,
&amp;quot;host&amp;quot;:[&amp;quot;unix:///var/run/docker.sock&amp;quot;,&amp;quot;tcp://:::2375&amp;quot;],
&amp;quot;log-level&amp;quot;:&amp;quot;debug&amp;quot;
}
systemctl restart docker
echo &amp;quot;1&amp;quot; &amp;gt;/proc/sys/net/bridge/bridge-nf-call-iptables
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;安装kubectlkubeadm和kubelet插件&#34;&gt;安装kubectl、kubeadm和kubelet插件&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;添加k8s下载源
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

指定版本的安装
yum install kubelet-1.16.0 kubeadm-1.16.0 kubectl-1.16.0 -y
systemctl enable kubelet &amp;amp;&amp;amp; sudo systemctl start kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;初始化的准备&#34;&gt;初始化的准备&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;查看安装过程需要哪些镜像
&lt;code&gt;kubeadm config images list --kubernetes-version=v1.16.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;通过脚本下载所需的镜像。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vi images.sh 
#!/bin/bash
images=(kube-proxy:v1.16.0 kube-scheduler:v1.16.0 kube-controller-manager:v1.16.0 kube-apiserver:v1.16.0 etcd:3.3.15-0 pause:3.1 coredns:1.6.2)
for imageName in ${images[@]} ; do
docker pull gcr.azk8s.cn/google-containers/$imageName
docker tag gcr.azk8s.cn/google-containers/$imageName k8s.gcr.io/$imageName
docker rmi gcr.azk8s.cn/google-containers/$imageName
done
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;执行如下指令下载：&lt;code&gt;chmod +x images.sh &amp;amp;&amp;amp; ./images.sh&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;拷贝kubeadm.yaml文件，需要注意advertiseAddress参数为本机ipv6地址&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vi kubeadm.yaml 
apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: &amp;quot;2402:e7c0:0:a00:ffff:ffff:fffe:fffb&amp;quot;
  bindPort: 6443
nodeRegistration:
  taints:
  - effect: PreferNoSchedule
    key: node-role.kubernetes.io/master
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.16.0
networking:
  podSubnet: 1100::/52
  serviceSubnet: fd00:4000::/112
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;执行如下安装指令：&lt;code&gt;kubeadm init --config=kubeadm.yaml&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;如果使1.16.0之前版本需要安装指令后面添加如下参数执行：&lt;code&gt;--ignore-preflight-errors=HTTPProxy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;如果以下安装有问题，需要重置，先执行&lt;code&gt;kubeadm reset&lt;/code&gt;,再执行以上&lt;code&gt;kubeadm init&lt;/code&gt;指令，安装成功之后，需要做如下操作：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl taint node node1 node-role.kubernetes.io/master-
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;安装网络插件如calico&#34;&gt;安装网络插件，如calico&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;下载calico.yaml文件&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;curl https://docs.projectcalico.org/v3.11/manifests/calico.yaml -O
需要修改及添加的内容为,总共三处：
 &amp;quot;ipam&amp;quot;: {
     &amp;quot;type&amp;quot;: &amp;quot;calico-ipam&amp;quot;,
     &amp;quot;assign_ipv4&amp;quot;: &amp;quot;false&amp;quot;,
     &amp;quot;assign_ipv6&amp;quot;: &amp;quot;true&amp;quot;,
     &amp;quot;ipv4_pools&amp;quot;: [&amp;quot;172.16.0.0/16&amp;quot;, &amp;quot;default-ipv4-ippool&amp;quot;],
     &amp;quot;ipv6_pools&amp;quot;: [&amp;quot;1100::/52&amp;quot;, &amp;quot;default-ipv6-ippool&amp;quot;]
  },

- name: CALICO_IPV4POOL_CIDR
  value: &amp;quot;172.16.0.0/16&amp;quot;
- name: IP6
  value: &amp;quot;autodetect&amp;quot;
- name: CALICO_IPV6POOL_CIDR
  value: &amp;quot;1100::/52&amp;quot;

# Disable IPv6 on Kubernetes.
- name: FELIX_IPV6SUPPORT
  value: &amp;quot;true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;calico的镜像,可以提前下载
calico/cni:v3.11.1
calico/pod2daemon-flexvol:v3.11.1
calico/node:v3.11.1
calico/kube-controllers:v3.11.1&lt;/li&gt;
&lt;li&gt;执行calico，&lt;code&gt;kubectl apply -f calico.yaml&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;验证&#34;&gt;验证：&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl get pod --all-namespaces -o wide&lt;/code&gt;
&lt;code&gt;kubectl get nodes -o wide&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;部署tomcat应用验证&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl run tomcat  --image=tomcat:8.0  --port=8080
kubectl get pod
kubectl expose deployment tomcat  --port=8080 --target-port=8080 --type=NodePort
# kubectl get svc
NAME         TYPE        CLUSTER-IP        EXTERNAL-IP   PORT(S)          AGE
kubernetes   ClusterIP   fd00:4000::1      &amp;lt;none&amp;gt;        443/TCP          33m
tomcat       NodePort    fd00:4000::bf3e   &amp;lt;none&amp;gt;        8080:30693/TCP   22m

curl -6g [2402:e7c0:0:a00:ffff:ffff:fffe:fffb]:32012
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;参考&#34;&gt;参考&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kubernetes.org.cn/5173.html&#34;&gt;https://www.kubernetes.org.cn/5173.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>netapp存储在kubesphere上的实践</title>
      <link>https://kubesphereio.com/post/netapp-stored-on-kubesphere-practice/</link>
      <pubDate>Fri, 13 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/netapp-stored-on-kubesphere-practice/</guid>
      <description>&lt;p&gt;&lt;strong&gt;NetApp&lt;/strong&gt;是向目前的数据密集型企业提供统一存储解决方案的居世界最前列的公司，其 Data ONTAP是全球首屈一指的存储操作系统。NetApp 的存储解决方案涵盖了专业化的硬件、软件和服务，为开放网络环境提供了无缝的存储管理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ontap&lt;/strong&gt;数据管理软件支持高速闪存、低成本旋转介质和基于云的对象存储等存储配置，为通过块或文件访问协议读写数据的应用程序提供统一存储。
&lt;strong&gt;Trident&lt;/strong&gt;是一个由NetApp维护的完全支持的开源项目。以帮助您满足容器化应用程序的复杂&lt;strong&gt;持久性&lt;/strong&gt;需求。
&lt;strong&gt;KubeSphere&lt;/strong&gt; 是一款开源项目，在目前主流容器调度平台 Kubernetes 之上构建的企业级分布式多租户&lt;strong&gt;容器管理平台&lt;/strong&gt;，提供简单易用的操作界面以及向导式操作方式，在降低用户使用容器调度平台学习成本的同时，极大降低开发、测试、运维的日常工作的复杂度。&lt;/p&gt;
&lt;h3 id=&#34;1整体方案&#34;&gt;1、整体方案&lt;/h3&gt;
&lt;p&gt;在VMware Workstation环境下安装ONTAP;ONTAP系统上创建SVM(Storage Virtual Machine)且对接nfs协议；在已有k8s环境下部署Trident,Trident将使用ONTAP系统上提供的信息（svm、managementLIF和dataLIF）作为后端来提供卷；在已创建的k8s和StorageClass卷下部署kubesphere。&lt;/p&gt;
&lt;h3 id=&#34;2版本信息&#34;&gt;2、版本信息&lt;/h3&gt;
&lt;p&gt;Ontap: 9.5
Trident: v19.07
k8s: 1.15
kubesphere: 2.0.2&lt;/p&gt;
&lt;h3 id=&#34;3步骤&#34;&gt;3、步骤&lt;/h3&gt;
&lt;p&gt;主要描述ontap搭建及配置、Trident搭建和配置和kubesphere搭建及配置等方面。参考&lt;a href=&#34;https://kubesphereio.com/post/netapp-works-with-k8s-in-kubesphere/&#34;&gt;ontap搭建&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;31-ontap搭建及配置&#34;&gt;3.1 ontap搭建及配置&lt;/h4&gt;
&lt;p&gt;在VMware Workstation上Simulate_ONTAP_9.5P6_Installation_and_Setup_Guide运行，ontap启动之后，按下面操作配置，其中以cluster base license、feature licenses for the non-ESX build配置证书、e0c、ip address：192.168.*.20、netmask:255.255.255.0、集群名:cluster1、密码等信息。
https://IP address,以上设置的iP地址，用户名和密码：
&lt;img src=&#34;https://kubesphereio.com/img/netapp.png&#34; alt=&#34;netapp&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;32-trident搭建及配置&#34;&gt;3.2 Trident搭建及配置&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;下载安装包trident-installer-19.07.0.tar.gz，解压进入trident-installer目录，执行trident安装指令:
&lt;code&gt;./tridentctl install -n trident&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;结合ontap的提供的参数创建第一个后端vi backend.json。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;{
    &amp;quot;version&amp;quot;: 1,
    &amp;quot;storageDriverName&amp;quot;: &amp;quot;ontap-nas&amp;quot;,
    &amp;quot;backendName&amp;quot;: &amp;quot;customBackendName&amp;quot;,
    &amp;quot;managementLIF&amp;quot;: &amp;quot;10.0.0.1&amp;quot;,
    &amp;quot;dataLIF&amp;quot;: &amp;quot;10.0.0.2&amp;quot;,
    &amp;quot;svm&amp;quot;: &amp;quot;trident_svm&amp;quot;,
    &amp;quot;username&amp;quot;: &amp;quot;cluster-admin&amp;quot;,
    &amp;quot;password&amp;quot;: &amp;quot;password&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;生成后端卷&lt;code&gt;./tridentctl -n trident create backend -f backend.json&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;创建StorageClass,vi storage-class-ontapnas.yaml&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ontapnasudp
provisioner: netapp.io/trident
mountOptions: [&amp;quot;rw&amp;quot;, &amp;quot;nfsvers=3&amp;quot;, &amp;quot;proto=udp&amp;quot;]
parameters:
  backendType: &amp;quot;ontap-nas&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建StorageClass指令&lt;code&gt;kubectl create -f storage-class-ontapnas.yaml&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;33-kubesphere的安装及配置&#34;&gt;3.3 kubesphere的安装及配置&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;在 Kubernetes 集群中创建名为 kubesphere-system 和 kubesphere-monitoring-system 的 namespace。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;$ cat &amp;lt;&amp;lt;EOF | kubectl create -f -
---
apiVersion: v1
kind: Namespace
metadata:
    name: kubesphere-system
---
apiVersion: v1
kind: Namespace
metadata:
    name: kubesphere-monitoring-system
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;创建 Kubernetes 集群 CA 证书的 Secret。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl -n kubesphere-system create secret generic kubesphere-ca  \
--from-file=ca.crt=/etc/kubernetes/pki/ca.crt  \
--from-file=ca.key=/etc/kubernetes/pki/ca.key
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;若 etcd 已经配置过证书，则参考如下创建（以下命令适用于 Kubeadm 创建的 Kubernetes 集群环境）：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl -n kubesphere-monitoring-system create secret generic kube-etcd-client-certs  \
--from-file=etcd-client-ca.crt=/etc/kubernetes/pki/etcd/ca.crt  \
--from-file=etcd-client.crt=/etc/kubernetes/pki/etcd/healthcheck-client.crt  \
--from-file=etcd-client.key=/etc/kubernetes/pki/etcd/healthcheck-client.key
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;修改kubesphere.yaml中存储的设置参数和对应的参数即可
&lt;code&gt;kubectl apply -f kubesphere.yaml&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;访问 KubeSphere UI 界面。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://kubesphereio.com/img/kubesphere.png&#34; alt=&#34;kubesphere&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;参考文档&#34;&gt;参考文档&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://docs.netapp.com/ontap-9/index.jsp?lang=zh_CN&#34;&gt;http://docs.netapp.com/ontap-9/index.jsp?lang=zh_CN&lt;/a&gt;
&lt;a href=&#34;https://netapp-trident.readthedocs.io/en/stable-v19.07/introduction.html&#34;&gt;https://netapp-trident.readthedocs.io/en/stable-v19.07/introduction.html&lt;/a&gt;
&lt;a href=&#34;https://github.com/kubesphere/ks-installer/blob/master/README_zh.md&#34;&gt;https://github.com/kubesphere/ks-installer/blob/master/README_zh.md&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>netapp联合k8s在kubesphere应用</title>
      <link>https://kubesphereio.com/post/netapp-works-with-k8s-in-kubesphere/</link>
      <pubDate>Fri, 13 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/netapp-works-with-k8s-in-kubesphere/</guid>
      <description>&lt;h1 id=&#34;netapp联合k8s在kubesphere应用&#34;&gt;netapp联合k8s在kubesphere应用&lt;/h1&gt;
&lt;h4 id=&#34;配置说明&#34;&gt;配置说明&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;k8s: 1.13+&lt;/li&gt;
&lt;li&gt;ontap: 9.5&lt;/li&gt;
&lt;li&gt;trident: v19.07&lt;/li&gt;
&lt;li&gt;kubesphere: 2.0.2&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;前期准备&#34;&gt;前期准备&lt;/h4&gt;
&lt;p&gt;需要准备材料在以下链接上，链接：https://pan.baidu.com/s/1q3KugGrz-XWJzqhgD7Ze9g
提取码：rhyw&lt;/p&gt;
&lt;p&gt;包括ontap的安装说明，Workstation上ontap9.5模拟器的ova，ontap使用文档，对接各种存储的协议证书，&lt;/p&gt;
&lt;h4 id=&#34;1-ontap的安装&#34;&gt;1. ontap的安装&lt;/h4&gt;
&lt;p&gt;本次测试的环境是安装在VMware Workstation，具体参考链接上这个文档Simulate_ONTAP_9.5P6_Installation_and_Setup_Guide，大致流程为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;window机器的资源配置&lt;/li&gt;
&lt;li&gt;开启VT&lt;/li&gt;
&lt;li&gt;为模拟ONTAP配置VMware Workstation&lt;/li&gt;
&lt;li&gt;在VMware Workstation上配置网络适配器，选择bridge网络&lt;/li&gt;
&lt;li&gt;开启模拟ONTAP&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-模拟ontap的配置&#34;&gt;2. 模拟ontap的配置&lt;/h4&gt;
&lt;p&gt;以上ontap启动之后，按下面操作配置，其中以cluster base license、feature licenses for the non-ESX build配置证书、e0c、ip address：192.168.*.20、netmask:255.255.255.0、集群名:cluster1、密码等信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2.1 Press Ctrl-C for Boot 菜单消息显示时，按 Ctrl-C&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;md1.uzip: 39168 x 16384 blocks
md2.uzip: 5760 x 16384 blocks
*******************************
* *
* Press Ctrl-C for Boot Menu. *
* *
*******************************
^C
Boot Menu will be available.
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;2.2 选择4配置&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Please choose one of the following:
(1) Normal Boot.
(2) Boot without /etc/rc.
(3) Change password.
(4) Clean configuration and initialize all disks.
(5) Maintenance mode boot.
(6) Update flash from backup config.
(7) Install new software first.
(8) Reboot node.
Selection (1-8)? 4
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;2.3 确认reset 和 确定&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Zero disks, reset config and install a new file system?: y
This will erase all the data on the disks, are you sure?: y
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;2.4 创建集群,填写参数&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Enter the cluster management interface port [e0d]: e0c
Enter the cluster management interface IP address: 192.168.x.20
Enter the cluster management interface netmask: 255.255.255.0
Enter the cluster management interface default gateway: &amp;lt;Enter&amp;gt;
A cluster management interface on port e0c with IP address 192.168.x.
20 has been created.
You can use this address to connect to and manager the cluster.
Do you want to create a new cluster or join an existing cluster?
{create}:
create
Enter the cluster name: cluster1
login: admin
Password: &amp;lt;password you defined&amp;gt;
Enter the cluster base license key:SMKQROWJNQYQSDAAAAAAAAAAAAAA
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;3-登录ontap界面&#34;&gt;3. 登录ontap界面&lt;/h4&gt;
&lt;p&gt;参考链接中的m_SL10537_gui_nas_basic_concepts_v2.1.0文档，这里需要配置的信息为，对接各个存储的协议证书、子网的设置、聚合的创建、svm创建和导出策略的配置。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;https://IP address,以上设置的iP地址，用户名和密码：
&lt;img src=&#34;https://kubesphereio.com/img/netapp.png&#34; alt=&#34;netapp&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对接各个存储的协议证书
登录平台之后，配置&amp;ndash;》许可证&amp;ndash;》添加对应的证书，显示为绿色的勾就添加正确。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;子网的设置
登录平台，网络&amp;ndash;》子网&amp;ndash;》创建&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;聚合的创建
登录平台，存储&amp;ndash;》聚合和磁盘&amp;ndash;》聚合&amp;ndash;》创建&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;svm的创建，此处需要选择对应的存储协议、 存储中的聚合和权限、管理(LIF)和数据（LIF）等信息。
登录平台，存储&amp;ndash;》SVM&amp;ndash;》创建&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;导出策略的设置，svm创建之后，点击svm设置&amp;ndash;》导出策略，在规则索引下添加客户端规范0.0.0.0/0，协议和权限。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;4-trident安装部署&#34;&gt;4. trident安装部署&lt;/h4&gt;
&lt;p&gt;介质在链接中，包括所需要的镜像和trident安装包和想要的配置文件。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果多台机器情形，需要在每台机器上执行&lt;code&gt;docker load -i trident.tar&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;解压trident安装包，&lt;code&gt;tar -xf $BASE_FOLDER/trident-installer-19.07.0.tar.gz&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;进入trident-installer目录，执行trident安装指令：&lt;code&gt;./tridentctl install -n trident&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;检查是否安装成功&lt;code&gt;kubectl get pod -n trident&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;创建并验证第一个后端,注意backend.json填写正确的ontap参数，
&lt;code&gt;./tridentctl -n trident create backend -f backend.json&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;验证后端是否生成：&lt;code&gt;./tridentctl -n trident get backend&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;创建storage class：&lt;code&gt;kubectl create -f sample-input/storage-class-basic.yaml&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;5-kubesphere安装部署&#34;&gt;5. kubesphere安装部署&lt;/h4&gt;
&lt;p&gt;参考官方部署文档为：https://github.com/kubesphere/ks-installer&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kubesphere基于Velero做集群的迁移</title>
      <link>https://kubesphereio.com/post/kubesphere-does-cluster-migration-based-on-velero/</link>
      <pubDate>Wed, 13 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/kubesphere-does-cluster-migration-based-on-velero/</guid>
      <description>&lt;p&gt;#Kubesphere基于Velero做集群的迁移
使用Velero 快速完成云原生应用迁移至备份集群中。&lt;/p&gt;
&lt;h3 id=&#34;环境信息&#34;&gt;环境信息&lt;/h3&gt;
&lt;p&gt;集群A（生产）：
master：192.168.11.6、192.168.11.13、192.168.11.16
lb：192.168.11.252
node：192.168.11.22
nfs：192.168.11.14
集群B（容灾）：
master：192.168.11.8、192.168.11.10、192.168.11.17
lb：192.168.11.253
node：192.168.11.18
nfs：192.168.11.14&lt;/p&gt;
&lt;h3 id=&#34;velero安装部署&#34;&gt;Velero安装部署&lt;/h3&gt;
&lt;p&gt;集群A和集群B都需要安装velero，安装过程参考官方文档&lt;a href=&#34;https://velero.io/docs/v1.2.0/contributions/minio/&#34;&gt;velero安装&lt;/a&gt;,大致流程为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、安装velero客户端安装包，A和B集群都需要。
1.1、wget https://github.com/vmware-tanzu/velero/releases/download/v1.0.0/velero-v1.0.0-linux-amd64.tar.gz
1.2、解压安装包，且将velero拷贝至/usr/local/bin目录下。
2、安装velero服务端，A和B集群都需要。
2.1、本地创建密钥文件，vi credentials-velero
[default]
aws_access_key_id = minio
aws_secret_access_key = minio123
2.2、集群B环境，运下载和运行00-minio-deployment.yaml文件，且需要将其中的ClusterIP改成NodePort，添加nodePort: 31860，集群A环境不需要执行这步。
kubectl apply -f examples/minio/00-minio-deployment.yaml
2.3、集群B环境，启动服务端,需要在密钥文件同级目录下执行：
velero install \
    --provider aws \
    --bucket velero \
    --secret-file ./credentials-velero \
    --use-volume-snapshots=false \
    --backup-location-config region=minio,s3ForcePathStyle=&amp;quot;true&amp;quot;,s3Url=http://minio.velero.svc:9000 \
	--plugins velero/velero-plugin-for-aws:v1.0.0
	
2.4、集群A环境，启动服务端，注意：需要在集群A中获取velero的外部curl：
2.4.1、集群A中，kubectl get svc -n velero,获取9000映射的端口，如：9000:31860，根据情况而定
2.4.2、启动指令：
velero install \
    --provider aws \
    --bucket velero \
    --secret-file ./credentials-velero \
    --use-volume-snapshots=false \
    --backup-location-config region=minio,s3ForcePathStyle=&amp;quot;true&amp;quot;,s3Url=http://192.168.11.8:31860 \
	--plugins velero/velero-plugin-for-aws:v1.0.0

&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;集群a数据的备份及集群b恢复&#34;&gt;集群A数据的备份及集群B恢复&lt;/h3&gt;
&lt;p&gt;具体备份指令，定时备份，参考官方文档&lt;a href=&#34;https://velero.io/docs/v1.2.0/contributions/minio/&#34;&gt;备份指令&lt;/a&gt;
在集群A中模拟了带有持久化的有状态和无状态的应用，备份维度以namespace为基准，为test,将pv的模式改成retain形式。
备份指令为：velero backup create test-backup &amp;ndash;include-namespaces test
集群A所有的机器关机且在机器B恢复验证：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@master1 velero-v1.2.0-linux-amd64]# velero restore create --from-backup test-backup
Restore request &amp;quot;test-backup-20191121141336&amp;quot; submitted successfully.
Run `velero restore describe test-backup-20191121141336` or `velero restore logs test-backup-20191121141336` for more details.
[root@master1 velero-v1.2.0-linux-amd64]# kubectl get ns
NAME                           STATUS   AGE
default                        Active   43h
demo                           Active   42h
kube-node-lease                Active   43h
kube-public                    Active   43h
kube-system                    Active   43h
kubesphere-controls-system     Active   43h
kubesphere-monitoring-system   Active   43h
kubesphere-system              Active   43h
openpitrix-system              Active   23h
test                           Active   12s
velero                         Active   24m
[root@master1 velero-v1.2.0-linux-amd64]# kubectl get pod -n test
NAME                             READY   STATUS    RESTARTS   AGE
mysql-v1-0                       1/1     Running   0          22s
tomcattest-v1-554c8875cd-26fz4   1/1     Running   0          22s
tomcattest-v1-554c8875cd-cmm2z   1/1     Running   0          22s
tomcattest-v1-554c8875cd-dc7mr   1/1     Running   0          22s
tomcattest-v1-554c8875cd-fcgn4   1/1     Running   0          22s
tomcattest-v1-554c8875cd-wqb4t   1/1     Running   0          22s
wordpress-v1-65d58448f8-g5bh8    1/1     Running   0          22s
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>k8s1.15安装</title>
      <link>https://kubesphereio.com/post/k8s1-15-install/</link>
      <pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/k8s1-15-install/</guid>
      <description>&lt;h1 id=&#34;使用kubeadm安装k8s-115版本&#34;&gt;使用kubeadm安装k8s 1.15版本&lt;/h1&gt;
&lt;p&gt;k8s 1.15版本中，kubeadm对HA集群的配置已经达到了beta可用，这一版本更新主要是针对稳定性的持续改善和可扩展性。其中用到的镜像和rpm包在百度云上，链接如下。
&lt;a href=&#34;https://pan.baidu.com/s/1LoKvv86Fs5ilZ-TYQdN35A&#34;&gt;https://pan.baidu.com/s/1LoKvv86Fs5ilZ-TYQdN35A&lt;/a&gt;
cos3&lt;/p&gt;
&lt;h2 id=&#34;1准备&#34;&gt;1.准备&lt;/h2&gt;
&lt;h3 id=&#34;11系统准备&#34;&gt;1.1系统准备&lt;/h3&gt;
&lt;p&gt;需要将主机ip和主机名放在每台机器的&lt;code&gt;vi /etc/hosts&lt;/code&gt;下
&lt;code&gt;192.168.11.21 i-fahx5c7k&lt;/code&gt;
&lt;code&gt;192.168.11.22 i-ouaaujhz&lt;/code&gt;
如果各个主机启用了防火墙，需要开启各个组件所需要的端口，可以查看https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
这里各个节点禁用防火墙
&lt;code&gt;systemctl stop firewalld&lt;/code&gt;
&lt;code&gt;systemctl disable firewalld&lt;/code&gt;
禁用selinux
&lt;code&gt;setenforce 0&lt;/code&gt;
vi /etc/selinux/config
SELINUX=disabled
创建vi /etc/sysctl.d/k8s.conf文件，添加如下内容：
&lt;code&gt;net.bridge.bridge-nf-call-ip6tables = 1&lt;/code&gt;
&lt;code&gt;net.bridge.bridge-nf-call-iptables = 1&lt;/code&gt;
&lt;code&gt;net.ipv4.ip_forward = 1&lt;/code&gt;
执行命令使修改生效
&lt;code&gt;modprobe br_netfilter&lt;/code&gt;
&lt;code&gt;sysctl -p /etc/sysctl.d/k8s.conf&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;12-kube-proxy开启ipvs的前置条件&#34;&gt;1.2 kube-proxy开启ipvs的前置条件&lt;/h3&gt;
&lt;p&gt;在所有的节点上执行如下脚本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt; /etc/sysconfig/modules/ipvs.modules &amp;lt;&amp;lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; bash /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4

各个节点需要安装 ipset ipvsadm
yum install ipset ipvsadm -y
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;13-docker安装&#34;&gt;1.3 docker安装&lt;/h3&gt;
&lt;p&gt;安装docker的yum源,国内寻找清华源
&lt;code&gt;yum install wget -y&lt;/code&gt;
&lt;code&gt;yum install -y yum-utils device-mapper-persistent-data lvm2&lt;/code&gt;
&lt;code&gt;wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo&lt;/code&gt;
&lt;code&gt;sed -i &#39;s+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+&#39; /etc/yum.repos.d/docker-ce.repo&lt;/code&gt;
&lt;code&gt;yum makecache fast&lt;/code&gt;
&lt;code&gt;yum install docker-ce -y&lt;/code&gt;
重启docker：&lt;code&gt;systemctl enable docker;systemctl restart docker&lt;/code&gt;
修改docker cgroup driver为systemd
创建或修改&lt;code&gt;vi /etc/docker/daemon.json&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启docker:&lt;code&gt;systemctl restart docker&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-使用kubeadm部署kubernetes&#34;&gt;2. 使用kubeadm部署kubernetes&lt;/h2&gt;
&lt;h3 id=&#34;21-安装kubeadm和kubelet&#34;&gt;2.1 安装kubeadm和kubelet&lt;/h3&gt;
&lt;p&gt;下面在各节点安装kubeadm和kubelet和kubectl，请在百度云上面下载rpm包，在rmp目录下执行如下指令：
&lt;code&gt;yum install -y cri-tools-1.13.0-0.x86_64.rpm kubernetes-cni-0.7.5-0.x86_64.rpm kubelet-1.15.1-0.x86_64.rpm kubectl-1.15.1-0.x86_64.rpm kubeadm-1.15.1-0.x86_64.rpm &lt;/code&gt;
k8s 1.8开始要求关闭系统的swap,不关闭，kubelet将无法启动，执行指令方法：
&lt;code&gt;swapoff -a&lt;/code&gt;
修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载，使用free -m确认swap已经关闭。 swappiness参数调整，修改&lt;code&gt;vi /etc/sysctl.d/k8s.conf&lt;/code&gt;添加下面一行：
&lt;code&gt;vm.swappiness=0&lt;/code&gt;
执行&lt;code&gt;sysctl -p /etc/sysctl.d/k8s.conf&lt;/code&gt;使修改生效。
因为这里本次测试主机上还运行其他服务，关闭swap可能会对其他服务产生影响，所以这里修改kubelet的配置去掉这个限制。
修改&lt;code&gt;vi /etc/sysconfig/kubelet&lt;/code&gt;，加入：&lt;code&gt;KUBELET_EXTRA_ARGS=--fail-swap-on=false&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;22-使用kubeadm-init初始化集群&#34;&gt;2.2 使用kubeadm init初始化集群&lt;/h3&gt;
&lt;p&gt;在各节点开机启动kubelet服务：&lt;code&gt;systemctl enable kubelet&lt;/code&gt;
使用&lt;code&gt;kubeadm config print init-defaults&lt;/code&gt;可以打印集群初始化默认的使用的配置:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: node1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: v1.14.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从默认的配置中可以看到，可以使用imageRepository定制在集群初始化时拉取k8s所需镜像的地址。advertiseAddress的ip替换本机，基于默认配置定制出本次使用kubeadm初始化集群所需的配置文件且在11.21机器上&lt;code&gt;vi kubeadm.yaml&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.11.21
  bindPort: 6443
nodeRegistration:
  taints:
  - effect: PreferNoSchedule
    key: node-role.kubernetes.io/master
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.15.0
networking:
  podSubnet: 10.244.0.0/16
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;font color=#DC143C &gt;说 明 : &lt;/font&gt;
使用kubeadm默认配置初始化的集群，会在master节点打上node-role.kubernetes.io/master:NoSchedule的污点，阻止master节点接受调度运行工作负载。这里测试环境只有两个节点，所以将这个taint修改为node-role.kubernetes.io/master:PreferNoSchedule。&lt;/p&gt;
&lt;p&gt;在开始初始化集群之前，需要从百度云上面下载tar包镜像下来，tar通过scp分别传到各个节点执行docker load -i解压，
镜像列表:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;k8s.gcr.io/kube-proxy                v1.15.0             d235b23c3570        5 weeks ago         82.4MB
k8s.gcr.io/kube-apiserver            v1.15.0             201c7a840312        5 weeks ago         207MB
k8s.gcr.io/kube-scheduler            v1.15.0             2d3813851e87        5 weeks ago         81.1MB
k8s.gcr.io/kube-controller-manager   v1.15.0             8328bb49b652        5 weeks ago         159MB
gcr.io/kubernetes-helm/tiller        v2.14.1             ac22eb1f780e        7 weeks ago         94.2MB
quay.io/coreos/flannel               v0.11.0-amd64       ff281650a721        6 months ago        52.6MB
k8s.gcr.io/coredns                   1.3.1               eb516548c180        6 months ago        40.3MB
k8s.gcr.io/etcd                      3.3.10              2c4adeb21b4f        8 months ago        258MB
k8s.gcr.io/pause                     3.1                 da86e6ba6ca1        19 months ago       742kB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;接下来使用kubeadm初始化集群，选择node1作为Master Node，在node1上执行下面的命令：&lt;code&gt;kubeadm init --config kubeadm.yaml --ignore-preflight-errors=Swap&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubeadm join 192.168.11.21:6443 --token k6s0kn.nyao1ulwwmlm2org \ --discovery-token-ca-cert-hash sha256:b721d25356668f921c90bf5f554836cb346b827a62b78797d0bcbcba24214725 &lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;其中关键步骤：
* [kubelet-start] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml”
* [certs]生成相关的各种证书
* [kubeconfig]生成相关的kubeconfig文件
* [control-plane]使用/etc/kubernetes/manifests目录中的yaml文件创建apiserver、controller-manager、scheduler的静态pod
* [bootstraptoken]生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到
* 下面的命令是配置常规用户如何使用kubectl访问集群：
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
* 最后给出了将节点加入集群的命令kubeadm join 192.168.99.11:6443 –token 4qcl2f.gtl3h8e5kjltuo0r \ –discovery-token-ca-cert-hash sha256:7ed5404175cc0bf18dbfe53f19d4a35b1e3d40c19b10924275868ebf2a3bbe6e
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;需要在11.21机器上执行：
&lt;code&gt;mkdir -p $HOME/.kube&lt;/code&gt;
&lt;code&gt;sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config&lt;/code&gt;
&lt;code&gt;sudo chown $(id -u):$(id -g) $HOME/.kube/config&lt;/code&gt;
查看集群状态，确认组件都处于healthy状态：
&lt;code&gt;kubectl get cs&lt;/code&gt;
集群初始化如果遇到问题，可以使用下面的命令进行清理：
&lt;code&gt;kubeadm reset&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;23-安装pod-network&#34;&gt;2.3 安装Pod Network&lt;/h3&gt;
&lt;p&gt;接下来安装flannel network add-on：
&lt;code&gt;mkdir -p ~/k8s/&lt;/code&gt;
&lt;code&gt;cd ~/k8s&lt;/code&gt;
&lt;code&gt;curl -O https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml&lt;/code&gt;
&lt;code&gt;kubectl apply -f  kube-flannel.yml&lt;/code&gt;
这里注意kube-flannel.yml这个文件里的flannel的镜像是0.11.0，quay.io/coreos/flannel:v0.11.0-amd64
如果Node有多个网卡的话，参考https://github.com/kubernetes/kubernetes/issues/39701，
目前需要在kube-flannel.yml中使用–iface参数指定集群主机内网网卡的名称，否则可能会出现dns无法解析。需要将kube-flannel.yml下载到本地，flanneld启动参数加上–iface=&lt;iface-name&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        - --iface=eth1
......
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;使用&lt;code&gt;kubectl get pod –-all-namespaces -o wide&lt;/code&gt;确保所有的Pod都处于Running状态。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kube-flannel.yml
[root@i-fahx5c7k k8s]# kubectl get pod --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP              NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5c98db65d4-nbb4w             1/1     Running   0          6m29s   10.244.0.2      i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-5c98db65d4-wtm58             1/1     Running   0          6m29s   10.244.0.3      i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   etcd-i-fahx5c7k                      1/1     Running   0          5m26s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-apiserver-i-fahx5c7k            1/1     Running   0          5m37s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-controller-manager-i-fahx5c7k   1/1     Running   0          5m45s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-flannel-ds-amd64-bqswg          1/1     Running   0          58s     192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-zhzxj                     1/1     Running   0          6m29s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-scheduler-i-fahx5c7k            1/1     Running   0          5m20s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;24-测试集群dns是否可用&#34;&gt;2.4 测试集群DNS是否可用&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl run curl --image=radial/busyboxplus:curl -it&lt;/code&gt;
进入后执行&lt;code&gt;nslookup kubernetes.default&lt;/code&gt;确认解析正常:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[ root@curl-6bf6db5c4f-f8jjn:/ ]$ nslookup kubernetes.default
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;25-向kubernetes集群中添加node节点&#34;&gt;2.5 向Kubernetes集群中添加Node节点&lt;/h3&gt;
&lt;p&gt;在master上查看添加节点指令：&lt;code&gt;kubeadm token create --print-join-command&lt;/code&gt;
下面将11.22这个主机添加到Kubernetes集群中，在11.22机器上执行:
&lt;code&gt;kubeadm join 192.168.11.21:6443 --token k6s0kn.nyao1ulwwmlm2org \ --discovery-token-ca-cert-hash sha256:b721d25356668f921c90bf5f554836cb346b827a62b78797d0bcbcba24214725&lt;/code&gt;
11.22加入集群很是顺利，下面在master节点上执行命令查看集群中的节点：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@i-fahx5c7k k8s]# kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
i-fahx5c7k   Ready    master   13m   v1.15.1
i-ouaaujhz   Ready    &amp;lt;none&amp;gt;   50s   v1.15.1
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;251-如何从集群中移除node&#34;&gt;2.5.1 如何从集群中移除Node&lt;/h4&gt;
&lt;p&gt;如果需要从集群中移除11.22这个Node执行下面的命令：
在master节点上执行：
&lt;code&gt;kubectl drain i-ouaaujhz --delete-local-data --force --ignore-daemonsets&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete node i-ouaaujhz&lt;/code&gt;
在11.22上执行：
&lt;code&gt;kubeadm reset&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;kube-proxy开启ipvs&#34;&gt;kube-proxy开启ipvs&lt;/h3&gt;
&lt;p&gt;修改ConfigMap的kube-system/kube-proxy中的config.conf，mode: “ipvs”
&lt;code&gt;kubectl edit cm kube-proxy -n kube-system&lt;/code&gt;
之后重启各个节点上的kube-proxy pod：
&lt;code&gt;kubectl get pod -n kube-system | grep kube-proxy | awk &#39;{system(&amp;quot;kubectl delete pod &amp;quot;$1&amp;quot; -n kube-system&amp;quot;)}&#39;&lt;/code&gt;
日志查看：&lt;code&gt;kubectl logs kube-proxy-62ntf  -n kube-system&lt;/code&gt;出现ipvs即开启。&lt;/p&gt;
&lt;h2 id=&#34;3kubernetes常用组件部署&#34;&gt;3.Kubernetes常用组件部署&lt;/h2&gt;
&lt;p&gt;使用Helm这个Kubernetes的包管理器，这里也将使用Helm安装Kubernetes的常用组件。&lt;/p&gt;
&lt;h3 id=&#34;31-helm的安装&#34;&gt;3.1 Helm的安装&lt;/h3&gt;
&lt;p&gt;Helm由客户端命helm令行工具和服务端tiller组成，Helm的安装十分简单。 下载helm命令行工具到master节点node1的/usr/local/bin下，这里下载的2.14.1版本：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;curl -O https://get.helm.sh/helm-v2.14.1-linux-amd64.tar.gz&lt;/code&gt;
&lt;code&gt;tar -zxvf helm-v2.14.1-linux-amd64.tar.gz&lt;/code&gt;
&lt;code&gt;cd linux-amd64/&lt;/code&gt;
&lt;code&gt;cp helm /usr/local/bin/&lt;/code&gt;
为了安装服务端tiller，还需要在这台机器上配置好kubectl工具和kubeconfig文件，确保kubectl工具可以在这台机器上访问apiserver且正常使用。 这里的11.21节点已经配置好了kubectl。
因为Kubernetes APIServer开启了RBAC访问控制，所以需要创建tiller使用的service account: tiller并分配合适的角色给它。这里简单起见直接分配cluster-admin这个集群内置的ClusterRole给它。创建&lt;code&gt;vi helm-rbac.yaml&lt;/code&gt;文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f helm-rbac.yaml&lt;/code&gt;
接下来使用helm部署tiller:
&lt;code&gt;helm init --service-account tiller --skip-refresh&lt;/code&gt;
tiller默认被部署在k8s集群中的kube-system这个namespace下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@i-fahx5c7k centosrepo]# kubectl get pod -n kube-system -l app=helm
NAME                             READY   STATUS    RESTARTS   AGE
tiller-deploy-7bf78cdbf7-46bv5   1/1     Running   0          22s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;helm version&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>构建arm-x86架构的docker-image操作指南</title>
      <link>https://kubesphereio.com/post/docker-image-operation-guide-for-building-arm-x86-architecture/</link>
      <pubDate>Sun, 13 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/docker-image-operation-guide-for-building-arm-x86-architecture/</guid>
      <description>&lt;h1 id=&#34;构建armx86架构的docker-image操作指南&#34;&gt;构建arm/x86架构的docker image操作指南&lt;/h1&gt;
&lt;p&gt;由于arm环境越来越受欢迎，镜像不单单满足x86结构的docker镜像，还需要arm操作系统的镜像，以下说明在x86机器上如何build一个arm结构的镜像，使用buildx指令来同时构建arm/x86结构的镜像。&lt;/p&gt;
&lt;h2 id=&#34;1启动一台ubuntu的机器并安装docker-1903&#34;&gt;1.	启动一台ubuntu的机器，并安装docker 19.03&lt;/h2&gt;
&lt;p&gt;在测试过程中发现 Centos7.5 有下面的问题，这里我们直接绕过
&lt;a href=&#34;https://github.com/multiarch/qemu-user-static/issues/38&#34;&gt;issue&lt;/a&gt;
docker安装参考&lt;a href=&#34;https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/&#34;&gt;docker安装&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;2运行下列命令安装并测试qemu&#34;&gt;2.	运行下列命令安装并测试qemu&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;查看机器的架构&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;uname -m
x86_64
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;正常测试docker启动一个arm镜像容器&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -t arm64v8/ubuntu uname -m
standard_init_linux.go:211: exec user process caused &amp;quot;exec format error&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;添加特权模式安装qemu，且启动一个arm镜像容器&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm --privileged multiarch/qemu-user-static --reset -p yes
docker run --rm -t arm64v8/ubuntu uname -m
aarch64
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3--启用docker--buildx-命令&#34;&gt;3.	  启用docker  buildx 命令&lt;/h2&gt;
&lt;p&gt;docker buildx 为跨平台构建 docker 镜像所使用的命令。目前为实验特性，可以设置dokcer cli的配置，将实验特性开启。&lt;/p&gt;
&lt;p&gt;将下面配置添加到CLI配置文件当中~/.docker/config.json&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;experimental&amp;quot;: &amp;quot;enabled&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;4创建新的builder实例默认的docker实例不支持镜像导出&#34;&gt;4.	创建新的builder实例（默认的docker实例不支持镜像导出）&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;docker buildx create --name ks-all&lt;/code&gt;
&lt;code&gt;docker buildx use ks-all&lt;/code&gt;
&lt;code&gt;docker buildx inspect --bootstrap&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;执行下面命令可以看到 builder 已经创建好，并且支持多种平台的构建。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker buildx ls
NAME/NODE DRIVER/ENDPOINT             STATUS  PLATFORMS
ks-all *  docker-container
  ks-all0 unix:///var/run/docker.sock running linux/amd64, linux/arm64, linux/riscv64, linux/ppc64le, linux/s390x, linux/386, linux/arm/v7, linux/arm/v6
default   docker
  default default                     running linux/amd64, linux/386
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;5执行构建命令以ks-installer为例&#34;&gt;5.	执行构建命令（以ks-installer为例）&lt;/h2&gt;
&lt;p&gt;在 ks-installer目录下执行命令可以构建 arm64与amd64的镜像，并自动推送到镜像仓库中。
&lt;code&gt;docker buildx build -f /root/ks-installer/Dockerfile --output=type=registry --platform linux/arm64  -t lilinlinlin/ks-installer:2.1.0-arm64 .&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;(需要注意现在 ks-installer 的 Dockerfile中 go build 命令带有 GOOS GOARCH等，这些要删除)&lt;/p&gt;
&lt;p&gt;构建成功之后，可以在dockerhub下图当中可以看到是支持两种arch&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;DIGEST                       OS/ARCH                          COMPRESSED SIZE
97dd2142cac6                 linux/amd64                       111.13 MB
ce366ad696cb                 linux/arm64                       111.13 MB
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;6-构建并保存为tar-文件&#34;&gt;6.	 构建并保存为tar 文件&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;可以参考 buildx 的官方文档
&lt;a href=&#34;https://github.com/docker/buildx#-o---outputpath-typetypekeyvalue&#34;&gt;https://github.com/docker/buildx#-o---outputpath-typetypekeyvalue&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker buildx build  --output=type=docker,dest=/root/ks-installer.tar --platform  linux/arm64 -t lilinlinlin/ks-installer:2.1.0-arm64 ./pkg/db/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;构建tar包时需要注意output的类型需要是docker，而不是tar&lt;/p&gt;
&lt;h2 id=&#34;7--多架构镜像管理&#34;&gt;7.  多架构镜像管理&lt;/h2&gt;
&lt;p&gt;相同镜像格式代表arm64和amd64的镜像。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;确保docker manifest命令被使能&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;#  ~/.docker/config.json中添加  &amp;quot;experimental&amp;quot;: &amp;quot;enabled&amp;quot;

cat ~/.docker/config.json
{
    &amp;quot;experimental&amp;quot;: &amp;quot;enabled&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;将多架构镜像push到dockerhub中
docker push lilinlinlin/ks-installer-amd64:v3.0.0
docker push lilinlinlin/ks-installer-arm64:v3.0.0&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建对应镜像的 manifest list,特别重要&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;docker manifest create lilinlinlin/ks-installer:v3.0.0  \
                       lilinlinlin/ks-installer-amd64:v3.0.0 \
                       lilinlinlin/ks-installer-arm64:v3.0.0 --amend
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;查看修正manifest list内容,可以查看到arm和amd架构的字样&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt; docker manifest inspect lilinlinlin/ks-installer:v3.0.0

 如果相关信息不正确可使用annotate命令修正 
docker manifest annotate --arch arm64 \
       lilinlinlin/ks-installer:v3.0.0 \
       lilinlinlin/ks-installer-arm64:v3.0.0
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;上传manifest list至dockerhub
docker manifest push lilinlinlin/ks-installer:v3.0.0&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;更多参考&#34;&gt;更多参考&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/docker/buildx&#34;&gt;https://github.com/docker/buildx&lt;/a&gt;
&lt;a href=&#34;https://github.com/multiarch/qemu-user-static&#34;&gt;https://github.com/multiarch/qemu-user-static&lt;/a&gt;
&lt;a href=&#34;https://community.arm.com/developer/tools-software/tools/b/tools-software-ides-blog/posts/getting-started-with-docker-for-arm-on-linux&#34;&gt;https://community.arm.com/developer/tools-software/tools/b/tools-software-ides-blog/posts/getting-started-with-docker-for-arm-on-linux&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>centos、ubuntu和pip离线依赖包的制作和使用方法</title>
      <link>https://kubesphereio.com/post/methods-of-making-and-using-centos-ubuntu-pip-offline-dependent-packages/</link>
      <pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/methods-of-making-and-using-centos-ubuntu-pip-offline-dependent-packages/</guid>
      <description>&lt;p&gt;*介绍centos、ubuntu和pip三大核心系统的离线依赖源的制作方法及制作完成之后如何使用&lt;/p&gt;
&lt;h2 id=&#34;1pip安装离线本地包pip版本1923&#34;&gt;1、pip安装离线本地包,pip版本（19.2.3）&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;导出本地已有的依赖包,需要创建一个空的requirements.txt文件。
&lt;code&gt;pip freeze &amp;gt; requirements.txt&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;下载到某个目录下（提前创建目录/packs），指定pip源。
&lt;code&gt;pip download -r requirements.txt -d /packs -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;安装requirements.txt依赖,可以通过pip &amp;ndash;help获取相关指令。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;在线安装：
pip install -r requirements.txt
离线安装：
将/packs目录下的包拷贝到离线环境的机器某目录上（/packs），
pip install --no-index --find-links=&amp;quot;/packs&amp;quot; -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2centos安装离线本地包&#34;&gt;2、centos安装离线本地包&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;制作离线包&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;新的机器上需要在/etc/yum.conf下打开缓存，keepcache=1;
新建目录存放rpm包，如mkdir -p /root/centos-repo;
安装单个工具，yum install -y iotop --downloaddir=/root/centos-repo;
创建本地源，createrepo /root/centos-repo;
制作iso包：mkisofs -r -o /root/centos-7.5-amd64.iso /root/centos-repo/
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3ubuntu安装离线本地包&#34;&gt;3、ubuntu安装离线本地包&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;制作离线包&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;创建存放目录：mkdir -p /home/ubuntu/packs；
安装软件包dpkg-dev:apt-get install dpkg-dev
拷贝dep包至存放目录：sudo cp -r /var/cache/apt/archives/* /home/ubuntu/packs；
进入packs目录下，生成包的依赖信息：dpkg-scanpackages packs /dev/null |gzip &amp;gt; packs/Packages.gz
制作iso包：mkisofs -r -o /home/ubuntu/ubuntu-16.04.5-amd64.iso /home/ubunut/packs
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;使用离线包,如Ubuntu16.04.5&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;创建目录：mkdir  -p /kubeinstaller/apt_repo/16.04.5/iso
挂载至/etc/fstab下在iso包目录下：sh -c &amp;quot;echo &#39;ubuntu-16.04.5-server-amd64.iso /kubeinstaller/apt_repo/16.04.5/iso iso9660 loop 0  0&#39; &amp;gt;&amp;gt; /etc/fstab&amp;quot;
备份之前源：mv -f /etc/apt/sources.list /etc/apt/sources.list-bak
添加新的源：sh -c &amp;quot;echo &#39;deb [trusted=yes]  file:///kubeinstaller/apt_repo/16.04.5/iso/  /&#39; &amp;gt; /etc/apt/sources.list&amp;quot;
挂载生效：mount -a

# clean the process using apt or dpkg
    apt_process=`ps -aux | grep -E &#39;apt|dpkg&#39; | grep -v &#39;grep&#39; | awk &#39;{print $2}&#39;`
    for process in ${apt_process}
    do
        kill -9 $process
    done
    # remove the apt lock
    sudo rm -f /var/lib/apt/lists/lock
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;参考文献&#34;&gt;参考文献&lt;/h4&gt;
&lt;p&gt;pip: &lt;a href=&#34;https://www.cnblogs.com/zengchunyun/p/9344664.html&#34;&gt;https://www.cnblogs.com/zengchunyun/p/9344664.html&lt;/a&gt;
centos: &lt;a href=&#34;https://blog.csdn.net/hao_rh/article/details/73275071&#34;&gt;https://blog.csdn.net/hao_rh/article/details/73275071&lt;/a&gt;
ubuntu: &lt;a href=&#34;https://www.cnblogs.com/gzxbkk/p/7809296.html&#34;&gt;https://www.cnblogs.com/gzxbkk/p/7809296.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>k8s之etcd数据的备份与恢复</title>
      <link>https://kubesphereio.com/post/backup-and-restore-etcd-data-of-k8s/</link>
      <pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/backup-and-restore-etcd-data-of-k8s/</guid>
      <description>&lt;h1 id=&#34;k8s之etcd备份与恢复&#34;&gt;k8s之etcd备份与恢复&lt;/h1&gt;
&lt;p&gt;etcd是一款开源的分布式一致性键值存储。目前有版本为V3以上，但是它的API又有v2和v3之分，以至于操作指令也不一样。
查看etcd版本&lt;code&gt;etcdctl --version&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;说 明 : &lt;/font&gt;
若使用 v3 备份数据时存在 v2 的数据则不影响恢复&lt;/p&gt;
&lt;p&gt;若使用 v2 备份数据时存在 v3 的数据则恢复失败&lt;/p&gt;
&lt;h3 id=&#34;1对于api2备份与恢复方法&#34;&gt;1、对于API2备份与恢复方法&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;snap: 存放快照数据,etcd防止WAL文件过多而设置的快照，存储etcd数据状态。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;wal: 存放预写式日志,最大的作用是记录了整个数据变化的全部历程。在etcd中，所有数据的修改在提交前，都要先写入到WAL中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;备份指令：
&lt;code&gt;etcdctl backup --data-dir /home/etcd/ --backup-dir /home/etcd_backup&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;恢复指令：
&lt;code&gt;etcd -data-dir=/home/etcd_backup/ -force-new-cluster&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;恢复时会覆盖 snapshot 的元数据(member ID 和 cluster ID)，所以需要启动一个新的集群。&lt;/p&gt;
&lt;h3 id=&#34;2对于api3备份与恢复方法&#34;&gt;2、对于API3备份与恢复方法&lt;/h3&gt;
&lt;p&gt;在使用 API 3 时需要使用环境变量 ETCDCTL_API 明确指定。
&lt;code&gt;export ETCDCTL_API=3&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;21备份数据&#34;&gt;2.1备份数据&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;etcdctl --endpoints localhost:2379 \
   --cert=/etc/ssl/etcd/ssl/node-master.pem \
   --key=/etc/ssl/etcd/ssl/node-master-key.pem \
   --cacert=/etc/ssl/etcd/ssl/ca.pem \snapshot save \
   /var/backups/kube_etcd/snapshot.db
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;font color=#DC143C &gt;说 明 : &lt;/font&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;/var/backups/kube_etcd这个目录是根宿主机的/var/lib/etcd目录相映射的，所以备份在这个目录在对应的宿主机上也是能看见的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这些证书对应文件可以直接在etcd容器内通过ps aux|more看见
其中–cert-file对应–cert，–key对应–key-file –cacert对应–trusted-ca-file&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;22恢复数据&#34;&gt;2.2恢复数据&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;停止三台master节点的kube-apiserver，指令为：
&lt;code&gt;mv /etc/kubernetes/manifests/kube-apiserver.yaml /etc/kubernetes/kube-apiserver.yaml&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在三个master节点停止 etcd 服务,指令为：
&lt;code&gt;systemctl stop etcd&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在三个master节点转移并备份当前 etcd 集群数据,指令为：
&lt;code&gt;mv /var/lib/etcd /var/lib/etcd.bak&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将最新的etcd备份数据恢复至三个master节点，其中master_ip为不同master主机的IP
指令为：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;export ETCDCTL_API=3 和
etcdctl snapshot restore /var/backups/kube_etcd/etcd-******/snapshot.db \
   --endpoints=master_ip:2379 \
   --cert=/etc/ssl/etcd/ssl/node-master.pem \
   --key=/etc/ssl/etcd/ssl/node-master-key.pem \
   --cacert=/etc/ssl/etcd/ssl/ca.pem \
   --data-dir=/var/lib/etcd
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;执行 etcd 恢复命令,指令为：
&lt;code&gt;systemctl restart etcd&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;重启 kube-apiserver,指令为：
&lt;code&gt;mv /etc/kubernetes/kube-apiserver.yaml /etc/kubernetes/manifests/kube-apiserver.yaml &lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;检查是否正常,指令为：
&lt;code&gt;kubectl get pod  --all-namespaces&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;检查etcd集群状态及成员指令为：
&lt;code&gt;etcdctl --endpoints=https://192.168.0.91:2379 --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-ks-allinone.pem --key=/etc/ssl/etcd/ssl/node-ks-allinone-key.pem member list&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>K8s1.16在centos安装</title>
      <link>https://kubesphereio.com/post/k8s1.16-installed-on-centos-system/</link>
      <pubDate>Sun, 22 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/k8s1.16-installed-on-centos-system/</guid>
      <description>&lt;h1 id=&#34;centos系统k8s-116版本安装&#34;&gt;centos系统k8s-1.16版本安装&lt;/h1&gt;
&lt;p&gt;k8s1.16版本相对之前版本变化不小，亮点和升级参看&lt;a href=&#34;http://k8smeetup.com/article/N1lqGc0i8v&#34;&gt;v1.16说明&lt;/a&gt;。相关联的镜像和v1.16二进制包上传至百度云上，链接如下&lt;a href=&#34;https://pan.baidu.com/s/19khl0Hn5ZnZ8TvbO5HZVww&#34;&gt;k8s1.16介质，ftq5&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;1准备&#34;&gt;1.准备&lt;/h2&gt;
&lt;h3 id=&#34;11系统准备&#34;&gt;1.1系统准备&lt;/h3&gt;
&lt;p&gt;需要将主机ip和主机名放在每台机器的&lt;code&gt;vi /etc/hosts&lt;/code&gt;下&lt;/p&gt;
&lt;p&gt;&lt;code&gt;192.168.11.21 i-fahx5c7k&lt;/code&gt;
&lt;code&gt;192.168.11.22 i-ouaaujhz&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;如果各个主机启用了防火墙，需要开启各个组件所需要的端口，可以查看https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
这里各个节点禁用防火墙&lt;/p&gt;
&lt;p&gt;&lt;code&gt;systemctl stop firewalld&lt;/code&gt;
&lt;code&gt;systemctl disable firewalld&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;禁用selinux&lt;/p&gt;
&lt;p&gt;&lt;code&gt;setenforce 0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;vi /etc/selinux/config
SELINUX=disabled
创建vi /etc/sysctl.d/k8s.conf文件，添加如下内容：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;net.bridge.bridge-nf-call-ip6tables = 1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;net.bridge.bridge-nf-call-iptables = 1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;net.ipv4.ip_forward = 1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;执行命令使修改生效&lt;/p&gt;
&lt;p&gt;&lt;code&gt;modprobe br_netfilter&lt;/code&gt;
&lt;code&gt;sysctl -p /etc/sysctl.d/k8s.conf&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;12-kube-proxy开启ipvs的前置条件&#34;&gt;1.2 kube-proxy开启ipvs的前置条件&lt;/h3&gt;
&lt;p&gt;在所有的节点上执行如下脚本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt; /etc/sysconfig/modules/ipvs.modules &amp;lt;&amp;lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; bash /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4

各个节点需要安装 ipset ipvsadm
yum install ipset ipvsadm -y
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;13-docker安装&#34;&gt;1.3 docker安装&lt;/h3&gt;
&lt;p&gt;安装docker的yum源,国内寻找清华源&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install wget -y
yum install -y yum-utils device-mapper-persistent-data lvm2
wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo
sed -i &#39;s+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+&#39; /etc/yum.repos.d/docker-ce.repo
yum makecache fast
yum install docker-ce -y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启docker：systemctl enable docker;systemctl restart docker
修改docker cgroup driver为systemd
创建或修改&lt;code&gt;vi /etc/docker/daemon.json&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;exec-opts&amp;quot;: [&amp;quot;native.cgroupdriver=systemd&amp;quot;]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启docker:&lt;code&gt;systemctl restart docker&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-使用kubeadm部署kubernetes&#34;&gt;2. 使用kubeadm部署kubernetes&lt;/h2&gt;
&lt;h3 id=&#34;21-安装kubeadm和kubelet&#34;&gt;2.1 安装kubeadm和kubelet&lt;/h3&gt;
&lt;p&gt;下面在各节点安装kubeadm和kubelet和kubectl，请在百度云上面下载rpm包，在k8s116目录下执行如下指令：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;yum install -y cri-tools-1.13.0-0.x86_64.rpm cni-0.7.5-0.x86_64.rpm kubelet-1.16.0-0.x86_64.rpm kubectl-1.16.0-0.x86_64.rpm kubeadm-1.16.0-0.x86_64.rpm &lt;/code&gt;&lt;/p&gt;
&lt;p&gt;k8s 1.8开始要求关闭系统的swap,不关闭，kubelet将无法启动，执行指令方法：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;swapoff -a&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载，使用free -m确认swap已经关闭。 swappiness参数调整，修改&lt;code&gt;vi /etc/sysctl.d/k8s.conf&lt;/code&gt;添加下面一行：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;vm.swappiness=0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;执行&lt;code&gt;sysctl -p /etc/sysctl.d/k8s.conf&lt;/code&gt;使修改生效。
因为这里本次测试主机上还运行其他服务，关闭swap可能会对其他服务产生影响，所以这里修改kubelet的配置去掉这个限制。
修改&lt;code&gt;vi /etc/sysconfig/kubelet&lt;/code&gt;，加入：&lt;code&gt;KUBELET_EXTRA_ARGS=--fail-swap-on=false&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;22-使用kubeadm-init初始化集群&#34;&gt;2.2 使用kubeadm init初始化集群&lt;/h3&gt;
&lt;p&gt;在各节点开机启动kubelet服务：&lt;code&gt;systemctl enable kubelet&lt;/code&gt;
使用&lt;code&gt;kubeadm config print init-defaults&lt;/code&gt;可以打印集群初始化默认的使用的配置:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: node1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: v1.14.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从默认的配置中可以看到，可以使用imageRepository定制在集群初始化时拉取k8s所需镜像的地址。advertiseAddress的ip替换本机，基于默认配置定制出本次使用kubeadm初始化集群所需的配置文件且在11.21机器上&lt;code&gt;vi kubeadm.yaml&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.11.21
  bindPort: 6443
nodeRegistration:
  taints:
  - effect: PreferNoSchedule
    key: node-role.kubernetes.io/master
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.16.0
networking:
  podSubnet: 10.244.0.0/16
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;font color=#DC143C &gt;说 明 : &lt;/font&gt;
使用kubeadm默认配置初始化的集群，会在master节点打上node-role.kubernetes.io/master:NoSchedule的污点，阻止master节点接受调度运行工作负载。这里测试环境只有两个节点，所以将这个taint修改为node-role.kubernetes.io/master:PreferNoSchedule。&lt;/p&gt;
&lt;p&gt;在开始初始化集群之前，kubeadm config images pull查看需要哪些镜像,需要从百度云上面下载tar包镜像下来，tar通过scp分别传到各个节点执行docker load -i解压，
镜像列表:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;k8s.gcr.io/kube-apiserver            v1.16.0             b305571ca60a        42 hours ago        217MB
k8s.gcr.io/kube-proxy                v1.16.0             c21b0c7400f9        42 hours ago        86.1MB
k8s.gcr.io/kube-controller-manager   v1.16.0             06a629a7e51c        42 hours ago        163MB
k8s.gcr.io/kube-scheduler            v1.16.0             301ddc62b80b        42 hours ago        87.3MB
k8s.gcr.io/etcd                      3.3.15-0            b2756210eeab        2 weeks ago         247MB
k8s.gcr.io/coredns                   1.6.2               bf261d157914        5 weeks ago         44.1MB
gcr.io/kubernetes-helm/tiller        v2.14.1             ac22eb1f780e        3 months ago        94.2MB
quay.io/coreos/flannel               v0.11.0-amd64       ff281650a721        7 months ago        52.6MB
k8s.gcr.io/pause                     3.1                 da86e6ba6ca1        21 months ago       742kB
radial/busyboxplus                   curl                71fa7369f437        5 years ago         4.23MB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;接下来使用kubeadm初始化集群，选择node1作为Master Node，在node1上执行下面的命令：
&lt;code&gt;kubeadm init --config kubeadm.yaml --ignore-preflight-errors=Swap&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubeadm join 192.168.11.21:6443 --token k6s0kn.nyao1ulwwmlm2org \ --discovery-token-ca-cert-hash sha256:b721d25356668f921c90bf5f554836cb346b827a62b78797d0bcbcba24214725 &lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;其中关键步骤：
* [kubelet-start] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml”
* [certs]生成相关的各种证书
* [kubeconfig]生成相关的kubeconfig文件
* [control-plane]使用/etc/kubernetes/manifests目录中的yaml文件创建apiserver、controller-manager、scheduler的静态pod
* [bootstraptoken]生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到
* 下面的命令是配置常规用户如何使用kubectl访问集群：
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
* 最后给出了将节点加入集群的命令kubeadm join 192.168.99.11:6443 –token 4qcl2f.gtl3h8e5kjltuo0r \ –discovery-token-ca-cert-hash sha256:7ed5404175cc0bf18dbfe53f19d4a35b1e3d40c19b10924275868ebf2a3bbe6e
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;需要在11.21机器上执行：
&lt;code&gt;mkdir -p $HOME/.kube&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo chown $(id -u):$(id -g) $HOME/.kube/config&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;查看集群状态，确认组件都处于healthy状态：
&lt;code&gt;kubectl get cs&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;集群初始化如果遇到问题，可以使用下面的命令进行清理：
&lt;code&gt;kubeadm reset&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;23-安装pod-network&#34;&gt;2.3 安装Pod Network&lt;/h3&gt;
&lt;p&gt;接下来安装flannel network add-on：
&lt;code&gt;mkdir -p ~/k8s/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cd ~/k8s&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;curl -O https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl apply -f  kube-flannel.yml&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这里注意kube-flannel.yml这个文件里的flannel的镜像是0.11.0，quay.io/coreos/flannel:v0.11.0-amd64
注意需要在vi /var/lib/kubelet/kubeadm-flags.env文件配置中去掉&amp;ndash;network-plugin=cni,然后重启kubelet,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl restart kubelet
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果Node有多个网卡的话，参考https://github.com/kubernetes/kubernetes/issues/39701，
目前需要在kube-flannel.yml中使用–iface参数指定集群主机内网网卡的名称，否则可能会出现dns无法解析。需要将kube-flannel.yml下载到本地，flanneld启动参数加上–iface=&lt;iface-name&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        - --iface=eth1
......
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;使用&lt;code&gt;kubectl get pods –-all-namespaces -o wide&lt;/code&gt;确保所有的Pod都处于Running状态。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kube-flannel.yml
[root@i-fahx5c7k k8s]# kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP              NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5c98db65d4-nbb4w             1/1     Running   0          6m29s   10.244.0.2      i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   coredns-5c98db65d4-wtm58             1/1     Running   0          6m29s   10.244.0.3      i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   etcd-i-fahx5c7k                      1/1     Running   0          5m26s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-apiserver-i-fahx5c7k            1/1     Running   0          5m37s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-controller-manager-i-fahx5c7k   1/1     Running   0          5m45s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-flannel-ds-amd64-bqswg          1/1     Running   0          58s     192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-proxy-zhzxj                     1/1     Running   0          6m29s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
kube-system   kube-scheduler-i-fahx5c7k            1/1     Running   0          5m20s   192.168.11.21   i-fahx5c7k   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;24-测试集群dns是否可用&#34;&gt;2.4 测试集群DNS是否可用&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl run curl --image=radial/busyboxplus:curl -it&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;进入后执行&lt;code&gt;nslookup kubernetes.default&lt;/code&gt;确认解析正常:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[ root@curl-6bf6db5c4f-f8jjn:/ ]$ nslookup kubernetes.default
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;25-向kubernetes集群中添加node节点&#34;&gt;2.5 向Kubernetes集群中添加Node节点&lt;/h3&gt;
&lt;p&gt;下面将11.22这个主机添加到Kubernetes集群中，在11.22机器上执行:
&lt;code&gt;kubeadm join 192.168.11.21:6443 --token k6s0kn.nyao1ulwwmlm2org \ --discovery-token-ca-cert-hash sha256:b721d25356668f921c90bf5f554836cb346b827a62b78797d0bcbcba24214725&lt;/code&gt;
11.22加入集群很是顺利，下面在master节点上执行命令查看集群中的节点：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@i-fahx5c7k k8s]# kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
i-fahx5c7k   Ready    master   13m   v1.15.1
i-ouaaujhz   Ready    &amp;lt;none&amp;gt;   50s   v1.15.1
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;251-如何从集群中移除node&#34;&gt;2.5.1 如何从集群中移除Node&lt;/h4&gt;
&lt;p&gt;如果需要从集群中移除11.22这个Node执行下面的命令：
在master节点上执行：
&lt;code&gt;kubectl drain i-ouaaujhz --delete-local-data --force --ignore-daemonsets&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl delete node i-ouaaujhz&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;在11.22上执行：
&lt;code&gt;kubeadm reset&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;kube-proxy开启ipvs&#34;&gt;kube-proxy开启ipvs&lt;/h3&gt;
&lt;p&gt;修改ConfigMap的kube-system/kube-proxy中的config.conf，mode: “ipvs”&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl edit cm kube-proxy -n kube-system&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;之后重启各个节点上的kube-proxy pod：
&lt;code&gt;kubectl get pod -n kube-system | grep kube-proxy | awk &#39;{system(&amp;quot;kubectl delete pod &amp;quot;$1&amp;quot; -n kube-system&amp;quot;)}&#39;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;日志查看：&lt;code&gt;kubectl logs kube-proxy-62ntf  -n kube-system&lt;/code&gt;出现ipvs即开启。&lt;/p&gt;
&lt;h2 id=&#34;3kubernetes常用组件部署&#34;&gt;3.Kubernetes常用组件部署&lt;/h2&gt;
&lt;p&gt;使用Helm这个Kubernetes的包管理器，这里也将使用Helm安装Kubernetes的常用组件。&lt;/p&gt;
&lt;h3 id=&#34;31-helm的安装&#34;&gt;3.1 Helm的安装&lt;/h3&gt;
&lt;p&gt;Helm由客户端命helm令行工具和服务端tiller组成，Helm的安装十分简单。 下载helm命令行工具到master节点node1的/usr/local/bin下，这里下载的2.14.1版本：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;curl -O https://get.helm.sh/helm-v2.14.1-linux-amd64.tar.gz&lt;/code&gt;
&lt;code&gt;tar -zxvf helm-v2.14.1-linux-amd64.tar.gz&lt;/code&gt;
&lt;code&gt;cd linux-amd64/&lt;/code&gt;
&lt;code&gt;cp helm /usr/local/bin/&lt;/code&gt;
为了安装服务端tiller，还需要在这台机器上配置好kubectl工具和kubeconfig文件，确保kubectl工具可以在这台机器上访问apiserver且正常使用。 这里的11.21节点已经配置好了kubectl。
&lt;code&gt;helm init --output yaml &amp;gt; tiller.yaml&lt;/code&gt;
更新 tiller.yaml 两处：apiVersion 版本;增加选择器&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
...
spec:
  replicas: 1
  strategy: {}
  selector:
    matchLabels:
      app: helm
      name: tiller
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;创建：&lt;code&gt;kubectl create -f tiller.yaml&lt;/code&gt;
因为Kubernetes APIServer开启了RBAC访问控制，所以需要创建tiller使用的service account: tiller并分配合适的角色给它。这里简单起见直接分配cluster-admin这个集群内置的ClusterRole给它。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create serviceaccount --namespace kube-system tiller
kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
kubectl patch deploy --namespace kube-system tiller-deploy -p &#39;{&amp;quot;spec&amp;quot;:{&amp;quot;template&amp;quot;:{&amp;quot;spec&amp;quot;:{&amp;quot;serviceAccount&amp;quot;:&amp;quot;tiller&amp;quot;}}}}&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;检查helm是否安装成功&lt;code&gt;helm list&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;安装k8s116脚本的安装&#34;&gt;安装k8s1.16脚本的安装&lt;/h3&gt;
&lt;p&gt;解压包，然后执行脚本install-k8s.sh
&lt;code&gt;tar -xzvf k8s116.tar.gz&lt;/code&gt;
&lt;code&gt;./install-k8s.sh&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;参考文档&#34;&gt;参考文档&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/zh/docs/&#34;&gt;kubeadm安装&lt;/a&gt;
&lt;a href=&#34;https://kubernetes.io/zh/docs/setup/independent/create-cluster-kubeadm/&#34;&gt;kubeadm创建集群&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>k8s1.15.3在ubuntu系统部署</title>
      <link>https://kubesphereio.com/post/k8s1-15-3-install-on-the-ubuntu/</link>
      <pubDate>Mon, 02 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/k8s1-15-3-install-on-the-ubuntu/</guid>
      <description>&lt;h1 id=&#34;k8s1153在ubuntu系统部署&#34;&gt;k8s1.15.3在ubuntu系统部署&lt;/h1&gt;
&lt;p&gt;国内环境下，k8s1.15.3在ubuntu系统部署，相关的镜像以及docker的deb包和k8s核心组件的deb包在以下百度链接下。
&lt;a href=&#34;https://pan.baidu.com/s/1FqfkBiRfa03xaKbmKnqI2w&#34;&gt;k8s介质&lt;/a&gt;
提取码：05ef&lt;/p&gt;
&lt;h4 id=&#34;配置&#34;&gt;配置&lt;/h4&gt;
&lt;p&gt;2核4G
k8s：v1.15.3
ubuntu:18.04&lt;/p&gt;
&lt;h3 id=&#34;1-前期准备&#34;&gt;1. 前期准备&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;关闭ufw防火墙,Ubuntu默认未启用,无需设置。
&lt;code&gt;sudo ufw disable&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;禁用SELINUX （ubuntu19.04默认不安装）
&lt;code&gt;sudo setenforce 0&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;开启数据包转发,修改/etc/sysctl.conf，开启ipv4转发
&lt;code&gt;net.ipv4.ip_forward=1 注释取消&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;防火墙修改FORWARD链默认策略
&lt;code&gt;sudo iptables -P FORWARD ACCEPT&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;禁用swap
&lt;code&gt;sudo swapoff -a&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置iptables参数，使得流经网桥的流量也经过iptables/netfilter防火墙&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sudo tee /etc/sysctl.d/k8s.conf &amp;lt;&amp;lt;-&#39;EOF&#39;
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sudo sysctl --system
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;2docker安装&#34;&gt;2.docker安装&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;安装deb包,通过dpkg指令&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;dpkg -i containerd.io_1.2.5-1_amd64.deb &amp;amp;&amp;amp; \
dpkg -i docker-ce-cli_18.09.5~3-0~ubuntu-bionic_amd64.deb &amp;amp;&amp;amp; \
dpkg -i docker-ce_18.09.5~3-0~ubuntu-bionic_amd64.deb
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;docker使用加速器（阿里云加速器）&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;tee /etc/docker/daemon.json &amp;lt;&amp;lt;- &#39;EOF&#39;
{
&amp;quot;registry-mirrors&amp;quot;: [&amp;quot;https://5xcgs6ii.mirror.aliyuncs.com&amp;quot;]
}
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;设置docker开机自启动
&lt;code&gt;sudo systemctl enable docker &amp;amp;&amp;amp; sudo systemctl start docker&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3安装kubeadmkubeletkubectl&#34;&gt;3.安装kubeadm、kubelet、kubectl&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;通过dpkg -i 来安装k8s核心组件，指令如下&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;dpkg -i cri-tools_1.13.0-00_amd64.deb  kubernetes-cni_0.7.5-00_amd64.deb socat_1.7.3.2-2ubuntu2_amd64.deb conntrack_1%3a1.4.4+snapshot20161117-6ubuntu2_amd64.deb kubelet_1.15.3-00_amd64.deb  kubectl_1.15.3-00_amd64.deb kubeadm_1.15.3-00_amd64.deb
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;设置开机自启动
&lt;code&gt;sudo systemctl enable kubelet &amp;amp;&amp;amp; sudo systemctl start kubelet&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4kubeadm-init初始化集群&#34;&gt;4.kubeadm init初始化集群&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;先要将需要的镜像解压
&lt;code&gt;docker load -i k8s1153.tar&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;查看Kubernetes需要哪些镜像
&lt;code&gt;kubeadm config images list --kubernetes-version=v1.15.3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;注意apiserver-advertise-address要换成本机的IP&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sudo kubeadm init --apiserver-advertise-address=192.168.11.21 --pod-network-cidr=172.16.0.0/16 --service-cidr=10.233.0.0/16 --kubernetes-version=v1.15.3
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;创建kubectl使用的kubeconfig文件
&lt;code&gt;mkdir -p $HOME/.kube&lt;/code&gt;
&lt;code&gt;sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config&lt;/code&gt;
&lt;code&gt;sudo chown $(id -u):$(id -g) $HOME/.kube/config&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建flannel的pod，命令如下
以下两个文件在百度下链接下。
&lt;code&gt;kubectl create -f kube-flannel.yml&lt;/code&gt;
&lt;code&gt;kubectl apply -f weave-net.yml&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;5检查集群及重新添加节点&#34;&gt;5.检查集群及重新添加节点&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;检查node是否ready
&lt;code&gt;kubectl get nodes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;检查pod是否running
&lt;code&gt;kubectl get pod --all-namespaces -o wide&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;添加节点，如果忘记token了，可以在master上面执行如下指令获取
&lt;code&gt;kubeadm token list&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;添加节点，需要提前在新的机器上安装kubelet等服务及需要把相关的镜像拷贝过去解压。最后通过如下指令添加：
&lt;code&gt;kubeadm join –token=4fccd2.b0e0f8918bd95d3e 192.168.11.21:6443&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;参考文档&#34;&gt;参考文档&lt;/h5&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/qq_42346414/article/details/89949380&#34;&gt;参考&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>docker部署mysql5.7</title>
      <link>https://kubesphereio.com/post/docker-deploy-mysql5-7/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/docker-deploy-mysql5-7/</guid>
      <description>&lt;h1 id=&#34;docker部署mysql57&#34;&gt;docker部署mysql5.7&lt;/h1&gt;
&lt;p&gt;越来越多服务容器化，下面以mysql5.7版本为例容器化部署。按三种方式：最简单、配置文件映射和数据映射来展开。&lt;/p&gt;
&lt;h2 id=&#34;准备条件&#34;&gt;准备条件&lt;/h2&gt;
&lt;p&gt;docker官方镜像：mysql:5.7
个人docker账号：lilinlinlin/mysql:5.7&lt;/p&gt;
&lt;h2 id=&#34;1-最简单的部署mysql&#34;&gt;1. 最简单的部署mysql&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=rootroot -d lilinlinlin/mysql:5.7&lt;/code&gt;
Navicat 测试连接 用户名:root 密码:rootroot 端口:3306&lt;/p&gt;
&lt;h2 id=&#34;2-数据映射到本机部署mysql&#34;&gt;2. 数据映射到本机部署mysql&lt;/h2&gt;
&lt;p&gt;先在本机新建一个目录，&lt;code&gt;mkdir -p /var/lib/mysql&lt;/code&gt;
然后run起来,-v前面是宿主机的目录，&amp;ndash;restart always表示重启容器
&lt;code&gt;docker run -p 3306:3306 --restart always -v /var/lib/mysql:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=mysqlroot -d 镜像ID&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3-配置文件映射到本机部署mysql&#34;&gt;3. 配置文件映射到本机部署mysql&lt;/h2&gt;
&lt;p&gt;在本机/etc下新建vi my.cnf配置文件，如果有的话先删除原来的，再加入创建新的&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[mysql]
#设置mysql客户端默认字符集
default-character-set=utf8
socket=/var/lib/mysql/mysql.sock

[mysqld]
#mysql5.7以后的不兼容问题处理
sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES
datadir=/var/lib/mysql
socket=/var/lib/mysql/mysql.sock
# Disabling symbolic-links is recommended to prevent assorted security risks
symbolic-links=0
# Settings user and group are ignored when systemd is used.
# If you need to run mysqld under a different user or group,
# customize your systemd unit file for mariadb according to the
# instructions in http://fedoraproject.org/wiki/Systemd
#允许最大连接数
max_connections=200
#服务端使用的字符集默认为8比特编码的latin1字符集
character-set-server=utf8
#创建新表时将使用的默认存储引擎
default-storage-engine=INNODB
lower_case_table_names=1
max_allowed_packet=16M 
#设置时区
default-time_zone=&#39;+8:00&#39;
[mysqld_safe]
log-error=/var/log/mariadb/mariadb.log
pid-file=/var/run/mariadb/mariadb.pid

#
# include all files from the config directory
#
!includedir /etc/mysql/conf.d/
!includedir /etc/mysql/mysql.conf.d/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行的指令：&amp;ndash;privileged=true 获取临时的selinux的权限。
&lt;code&gt;docker run -p 3306:3306 --privileged=true -v /etc/my.cnf:/etc/mysql/mysql.conf.d/mysqld.cnf -e MYSQL_ROOT_PASSWORD=mysql密码 -d 镜像ID&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;查看容器启动情况&#34;&gt;查看容器启动情况&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;docker ps -a|grep mysql&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>k8s之helm部署及用法</title>
      <link>https://kubesphereio.com/post/helm-deployment-and-guide/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/helm-deployment-and-guide/</guid>
      <description>&lt;h1 id=&#34;k8s之helm部署及用法&#34;&gt;k8s之helm部署及用法&lt;/h1&gt;
&lt;p&gt;Helm是Kubernetes的一个包管理工具，用来简化Kubernetes应用的部署和管理。可以把Helm比作CentOS的yum工具。所以可以把该包在不同环境下部署起来,前提需要部署k8s环境。&lt;/p&gt;
&lt;h2 id=&#34;1-helm部署&#34;&gt;1. helm部署&lt;/h2&gt;
&lt;p&gt;Helm由两部分组成，客户端helm和服务端tiller。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tiller运行在Kubernetes集群上，管理chart安装的release&lt;/li&gt;
&lt;li&gt;helm是一个命令行工具，可在本地运行，一般运行在CI/CD Server上。一般我们用helm操作&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;11-客户端helm和服务端tiller安装以1921681120为例&#34;&gt;1.1 客户端helm和服务端tiller安装，以192.168.11.20为例&lt;/h3&gt;
&lt;p&gt;下载地址：https://github.com/helm/helm/releases
这里可以下载的是helm v2.14.1，解压缩后将可执行文件helm拷贝到/usr/local/bin下。这样客户端helm就在这台机器上安装完成了。
通过&lt;code&gt;helm version&lt;/code&gt;显示客户端安装好了，但是服务端没有好.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm version
Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.14.1&amp;quot;, GitCommit:&amp;quot;5270352a09c7e8b6e8c9593002a73535276507c0&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}
Error: Get http://localhost:8080/api/v1/namespaces/kube-system/pods?labelSelector=app%3Dhelm%2Cname%3Dtiller: dial tcp [::1]:8080: connect: connection refused
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;为了安装服务端tiller，还需要在这台机器上配置好kubectl工具和kubeconfig文件，确保kubectl工具可以在这台机器上访问apiserver且正常使用。
&lt;code&gt;kubectl get cs&lt;/code&gt;
使用helm在k8s上部署tiller：
&lt;code&gt;helm init --service-account tiller --skip-refresh&lt;/code&gt;
&lt;font color=#DC143C &gt;说明:&lt;/font&gt;
如果网络原因不能访问gcr.io，可以通过helm init –service-account tiller –tiller-image &lt;your-docker-registry&gt;/tiller:2.7.2 –skip-refresh使用私有镜像仓库中的tiller镜像。ps:lilinlinlin/tiller:2.7.2
tiller默认被部署在k8s集群中的kube-system这个namespace下。
&lt;code&gt;kubectl get pod -n kube-system -l app=helm&lt;/code&gt;
再次helm version可以打印客户端和服务端的版本：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm version
Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.7.2&amp;quot;, GitCommit:&amp;quot;8
Server: &amp;amp;version.Version{SemVer:&amp;quot;v2.7.2&amp;quot;,
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2-kubernetes-rbac配置&#34;&gt;2. kubernetes RBAC配置&lt;/h2&gt;
&lt;p&gt;因为我们将tiller部署在Kubernetes 1.8上，Kubernetes APIServer开启了RBAC访问控制，所以我们需要创建tiller使用的service account: tiller并分配合适的角色给它。
这里简单起见直接分配cluster-admin这个集群内置的ClusterRole给它。创建&lt;code&gt;vi helm-rbac.yaml&lt;/code&gt;文件：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f helm-rbac.yaml&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3添加国内helm源&#34;&gt;3.添加国内helm源&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;helm repo add stable http://mirror.azure.cn/kubernetes/charts/&lt;/code&gt;
&lt;code&gt;helm repo add apphub https://apphub.aliyuncs.com&lt;/code&gt;
更新chart repo: &lt;code&gt;helm repo update&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;4-helm的基本使用&#34;&gt;4. helm的基本使用&lt;/h2&gt;
&lt;p&gt;下面我们开始尝试创建一个chart，这个chart用来部署一个简单的服务&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm create hello-test
Creating hello-test

tree hello-test/
hello-test/
├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── ingress.yaml
│   ├── NOTES.txt
│   └── service.yaml
└── values.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;charts目录中是本chart依赖的chart，当前是空的&lt;/li&gt;
&lt;li&gt;Chart.yaml这个yaml文件用于描述Chart的基本信息，如名称版本等&lt;/li&gt;
&lt;li&gt;templates是Kubernetes manifest文件模板目录，模板使用chart配置的值生成Kubernetes manifest文件。&lt;/li&gt;
&lt;li&gt;templates/NOTES.txt 纯文本文件，可在其中填写chart的使用说明&lt;/li&gt;
&lt;li&gt;value.yaml 是chart配置的默认值
在 values.yaml 中，可以看到，默认创建的是一个 Nginx 应用。为了方便外网访问测试，将 values.yaml 中 service 的属性修改为:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;service:
  type: NodePort
  port: 30080
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;41-部署应用&#34;&gt;4.1 部署应用&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;helm install ./hello-test&lt;/code&gt;
但实际上可以这样部署为,&lt;strong&gt;.tgz为chart包，&lt;/strong&gt;.yaml类似与values.yaml把变量文件定义出来。
&lt;code&gt;helm upgrade --install &amp;lt;name&amp;gt; **.tgz **.yaml --namespace &amp;lt;namespace-name&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;42-查看部署应用&#34;&gt;4.2 查看部署应用&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;helm list&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;43-删除部署应用&#34;&gt;4.3 删除部署应用&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;helm delete &amp;lt;name&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;44-打包chart&#34;&gt;4.4 打包chart&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;helm package &amp;lt;name&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考：&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kubernetes.org.cn/3435.html&#34;&gt;https://www.kubernetes.org.cn/3435.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>kubekey源码解读</title>
      <link>https://kubesphereio.com/post/kubekey-source-code-interpretation/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/kubekey-source-code-interpretation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>kubernetes一致性认证的提交操作指南</title>
      <link>https://kubesphereio.com/post/kubernetes-compliance-certification-submission-instructions/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/kubernetes-compliance-certification-submission-instructions/</guid>
      <description>&lt;p&gt;软件一致性尤为重要，它可以避免分裂，使众厂商将精力聚焦于共同推动软件发展而不是自成一家。2017年CNCF启动了Kubernetes一致性认证计划，CNCF提供一套测试工具，各厂商按照操作指导进行测试自身的产品，将测试报告上传给CNCF社区，CNCF审核测试报告后，会给符合条件的企业颁发一个证书。
大致流程：&lt;img src=&#34;https://ww1.sinaimg.cn/large/006bbiLEgy1gfsyxpcmmej30mx09zwew.jpg&#34; alt=&#34;操作流程图.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-环境信息&#34;&gt;1. 环境信息&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;亚太机器ubuntu（16.04.6）两台，192.168.0.3/192.168.0.4&lt;/li&gt;
&lt;li&gt;k8s1.18.3（1master+1node）（至少两台机器）&lt;/li&gt;
&lt;li&gt;sonobuoy0.18.3&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2k8s及云平台的部署&#34;&gt;2.k8s及云平台的部署&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;参考官网链接部署&lt;a href=&#34;https://github.com/kubesphere/kubekey&#34;&gt;K8s及云平台的部署&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;K8s版本为1.18.3及KubeSphere版本为v3.0.0，指令如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;curl -O -k https://kubernetes.pek3b.qingstor.com/tools/kubekey/kk
chmod +x kk
./kk create cluster --with-kubernetes v1.18.3 --with-kubesphere
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3-sonobuoy组件部署及运行&#34;&gt;3. sonobuoy组件部署及运行&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;sonobuoy二进制文件下载&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;wget https://github.com/vmware-tanzu/sonobuoy/releases/download/v0.18.3/sonobuoy_0.18.3_linux_amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;解压，&lt;code&gt;tar -xzvf sonobuoy_0.18.3_linux_amd64.tar.gz&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;配置环境变量，sonobuoy执行文件拷贝到/usr/local/bin下 &lt;code&gt;cp sonobuoy /usr/local/bin/&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-sonobuoy执行及相关指令&#34;&gt;4. sonobuoy执行及相关指令&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;在集群中部署一个sonobuoy的pod，&amp;ndash;mode=certified-conformance参数在Kubernetesv1.16(Sonobuoy v0.16)需要添加，指令为：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sonobuoy run --mode=certified-conformance
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;sonobuoy运行状态&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sonobuoy status
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;详细的日志&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sonobuoy logs
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;通过sonobuoy status显示“completed”，可以通过如下指令获取输出结果，需要提交的内容在plugins/e2e/results/global/{e2e.log,junit_01.xml}目录下&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sonobuoy retrieve
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;删除sonobuoy组件&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sonobuoy delete
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;5-提交的pr包含内容&#34;&gt;5. 提交的pr包含内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;5.1 fork k8s-conformance代码到你GitHub账号下，然后git clone到本地。&lt;/li&gt;
&lt;li&gt;5.2 在本地找到对应的k8s版本号，然后在里面建相关的名字即可，比如在v1.18版本下创建KubeSphere目录。&lt;/li&gt;
&lt;li&gt;5.3 自己项目目录下，需要包含以下四个文件，e2e.log、junit_01.xml、PRODUCT.yaml和README.md&lt;/li&gt;
&lt;li&gt;5.4 e2e.log和junit_01.xml两个文件是通过上面四步骤下解压的两个文件。&lt;/li&gt;
&lt;li&gt;5.5 PRODUCT.yaml包含的内容大致为：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;vendor: 组织结构
name: 项目名
version: 版本号
website_url: 项目官方浏览页
repo_url: 项目官方镜像仓库地址
documentation_url: 项目官方文档
product_logo_url: 项目log图标
type: 开源/非开源
description: 项目的描述
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;5.6 README.md包含的内容大致为：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;To reproduce:
本身项目的安装方法
sonobuoy项目的安装方法
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;5.7 提交代码到自己GitHub账号下，然后提pr即可。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;6-参考文章&#34;&gt;6. 参考文章&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cncf/k8s-conformance/blob/master/instructions.md&#34;&gt;https://github.com/cncf/k8s-conformance/blob/master/instructions.md&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>kubesphere2-1-HA环境，一个master或者两个master宕机恢复</title>
      <link>https://kubesphereio.com/post/kubesphere2-1-ha-environment-one-master-or-two-masters-down-to-recover/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/kubesphere2-1-ha-environment-one-master-or-two-masters-down-to-recover/</guid>
      <description>&lt;h1 id=&#34;kubesphere21-ha环境一个master或者两个master宕机恢复&#34;&gt;kubesphere2.1-HA环境，一个master或者两个master宕机恢复&lt;/h1&gt;
&lt;p&gt;kubesphere2.1版本提供了master节点的高可用功能，为生产环境保驾护航。然而很多事情都有意外，当其中一个master或者两个master卡住了，或者重启都不能自动恢复的情况下，那么怎么恢复呢，以下分一个master宕机和两个master宕机的恢复方法。&lt;/p&gt;
&lt;h3 id=&#34;验证环境信息&#34;&gt;验证环境信息&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;os: centos7.5
master1: 192.168.11.6
master2: 192.168.11.8
master3: 192.168.11.13
node1: 192.168.11.14
lb: 192.168.11.253
nfs服务端: 192.168.11.14
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;一个master宕机的模拟及恢复方法&#34;&gt;一个master宕机的模拟及恢复方法&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;正常的环境：nodes都running，etcd服务都正常。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes
NAME      STATUS   ROLES    AGE   VERSION
master1   Ready    master   19m   v1.15.5
master2   Ready    master   16m   v1.15.5
master3   Ready    master   16m   v1.15.5
node1     Ready    worker   14m   v1.15.5

export ETCDCTL_API=3

etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
192.168.11.6:2379, b3487da7c562b17, 3.2.18, 3.8 MB, true, 5, 4434
192.168.11.8:2379, 3d04d74d80b7f07b, 3.2.18, 3.8 MB, false, 5, 4434
192.168.11.13:2379, 3ad323c042cc4c05, 3.2.18, 3.8 MB, false, 5, 4434
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;一个master宕机情况，把master2重置，看nodes和etcd情况。还需在界面创建一些带有存储的pod用例。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes   
NAME      STATUS     ROLES    AGE   VERSION
master1   Ready      master   57m   v1.15.5
master2   NotReady   master   55m   v1.15.5
master3   Ready      master   55m   v1.15.5
node1     Ready      worker   52m   v1.15.5

etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
Failed to get the status of endpoint 192.168.11.8:2379 (context deadline exceeded)
192.168.11.6:2379, b3487da7c562b17, 3.2.18, 11 MB, true, 5, 14953
192.168.11.13:2379, 3ad323c042cc4c05, 3.2.18, 11 MB, false, 5, 14972
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;恢复方法：修改脚本中hosts.ini文件，需要注意顺序
在[all]组里面，master2行整体放在master3下面，[kube-master]和[etcd]组中master2放在master3下面,然后在另一个终端机器上重新执行安装脚本。以下恢复情况，etcd正常，nodes也正常，业务数据存在且业务pod没有中断正常使用。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes
NAME      STATUS     ROLES    AGE   VERSION
master1   Ready      master   83m   v1.15.5
master2   Ready      master   80m   v1.15.5
master3   Ready      master   80m   v1.15.5
node1     Ready      worker   78m   v1.15.5

etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
192.168.11.6:2379, b3487da7c562b17, 3.2.18, 11 MB, true, 11, 20292
192.168.11.8:2379, 3d04d74d80b7f07b, 3.2.18, 11 MB, false, 11, 20297
192.168.11.13:2379, 3ad323c042cc4c05, 3.2.18, 11 MB, false, 11, 20298

docker ps | grep tomcat
6863620b07cf        882487b8be1d                                     &amp;quot;catalina.sh run&amp;quot;        17 minutes ago       Up 17 minutes                           k8s_tomcat-2jl160_tomcat-v1-6c7745494-k64w5_demo_5b406841-df9b-4d5c-b018-998c400a2c3e_1
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;两个master宕机的模拟及恢复方法&#34;&gt;两个master宕机的模拟及恢复方法&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;两个master宕机情况，把master2和master3重置，看nodes和etcd情况，nodes不正常，etcd两个不正常。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes
Unable to connect to the server: EOF
[root@master1 ~]# 
[root@master1 ~]# 
[root@master1 ~]# etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
Failed to get the status of endpoint 192.168.11.8:2379 (context deadline exceeded)
Failed to get the status of endpoint 192.168.11.13:2379 (context deadline exceeded)
192.168.11.6:2379, b3487da7c562b17, 3.2.18, 11 MB, false, 12, 27944
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;恢复方法：
1、同样需要在host.ini文件修改master的顺序，由于我们重置2和3，所有此处不用修改顺序；
2、在已解压的安装介质目录下，进入k8s/roles/kubernetes/preinstall/tasks/main.yml文件，用#注释如下内容，重跑安装脚本，作用说明：在重置的master2和master3机器上安装docker和etcd，但整个集群还需修复。。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;#- import_tasks: 0020-verify-settings.yml
#  when:
#    - not dns_late
#  tags:
#    - asserts
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3、在master2机器上，临时修复master2节点的etcd服务，执行如下指令：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;停止etcd：systemctl stop etcd
备份etcd数据：mv /var/lib/etcd /var/lib/etcd-bak
从master1的/var/backups/kube_etcd/备份目录下拷贝最近时间的snapshot.db至master2机器上
在master2先转为版本3指令令：export ETCDCTL_API=3
在master2恢复指令：etcdctl snapshot restore /root/snapshot.db    --endpoints=192.168.11.8:2379    --cert=/etc/ssl/etcd/ssl/node-master2.pem    --key=/etc/ssl/etcd/ssl/node-master2-key.pem    --cacert=/etc/ssl/etcd/ssl/ca.pem --data-dir=/var/lib/etcd
重启etcd: systemctl restart etcd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;4、修改host.ini文件master2和master3顺序，把[all][kube-master][etcd]三个组的master3放在最后面，再次跑安装脚本。其中的host.ini文件为&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[all]
master1 ansible_connection=local  ip=192.168.11.6
master2  ansible_host=192.168.11.8  ip=192.168.11.8  ansible_ssh_pass=
master3  ansible_host=192.168.11.13  ip=192.168.11.13  ansible_ssh_pass=
node1    ansible_host=192.168.11.14  ip=192.168.11.14  ansible_ssh_pass=

[kube-master]
master1
master2
master3

[kube-node]
node1

[etcd]
master1
master2
master3

[k8s-cluster:children]
kube-node
kube-master 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;5、由第四步只是临时修复master2的etcd，并不完全修复，先全部修复作如下处理：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;第四步执行过程中，z在“wait for etcd up”会报错，先ctrl +c 终止脚本；
在master2机器上，停止etcd服务：systemctl stop etcd
在master2机器上，删除etcd数据：rm -rf /var/lib/etcd
再次修改host.ini文件，master2和master3顺序，把[all][kube-master][etcd]三个组的master2放在最后面，再次跑安装脚本即可。
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;验证结果&#34;&gt;验证结果&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;集群中的nodes、etcd和带有存储数据的业务pod情况：&lt;/li&gt;
&lt;li&gt;集群中的nodes、etcd和带有存储数据的业务pod情况,nodes恢复正常，etcd服务也回复正常，带有存储的pod一直运行，集群恢复成功：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[root@master1 ~]# kubectl get nodes
NAME      STATUS   ROLES    AGE     VERSION
master1   Ready    master   4h56m   v1.15.5
master2   Ready    master   4h53m   v1.15.5
master3   Ready    master   4h53m   v1.15.5
node1     Ready    worker   4h51m   v1.15.5
[root@master1 ~]# 
[root@master1 ~]# 
[root@master1 ~]# 
[root@master1 ~]# etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
192.168.11.6:2379, b3487da7c562b17, 3.2.18, 12 MB, false, 1372, 32116
192.168.11.8:2379, 3d04d74d80b7f07b, 3.2.18, 12 MB, true, 1372, 32116
192.168.11.13:2379, 3ad323c042cc4c05, 3.2.18, 12 MB, false, 1372, 32116

docker ps | grep tomcat
6863620b07cf        882487b8be1d                                     &amp;quot;catalina.sh run&amp;quot;        4 hours ago         Up 4 hours                              k8s_tomcat-2jl160_tomcat-v1-6c7745494-k64w5_demo_5b406841-df9b-4d5c-b018-998c400a2c3e_1
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>kubesphere2-1-HA环境，某台master或者master的etcd宕机，新加机器恢复方法</title>
      <link>https://kubesphereio.com/post/kubesphere2-1-ha-environment-a-master-or-master-etcd-down-a-new-machine-recovery-method/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/kubesphere2-1-ha-environment-a-master-or-master-etcd-down-a-new-machine-recovery-method/</guid>
      <description>&lt;p&gt;kubesphere2.1版本提供了master节点的高可用功能，为生产环境保驾护航。然而在生产环境中，为了业务更正常运行，当以下两种情形发生时，告诉大家怎么恢复。第一种情形是：其中某台master机器的etcd服务不能正常提供服务，而需要在另外一台机器上部署一个etcd服务加入到现etcd集群中；第二种情形是：其中某台master宕机，需要在另外一台机器上部署master，并加入到现master集群中。&lt;/p&gt;
&lt;h3 id=&#34;环境信息&#34;&gt;环境信息&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;os: centos7.5
master1: 192.168.11.6
master2: 192.168.11.16
master3: 192.168.11.13
node1: 192.168.11.14
lb: 192.168.11.253
nfs服务端: 192.168.11.14
新加机器master2: 192.168.11.8
安装介质机器：192.168.11.6
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;一个master宕机在另外一台服务器上恢复方法&#34;&gt;一个master宕机，在另外一台服务器上恢复方法&lt;/h2&gt;
&lt;p&gt;假设为master2（192.168.11.16）宕机了，现以新机器192.168.11.8作为恢复master2的机器。以下操作都在master1机器上执行。
1、在安装介质机器即master1机器上面，进入/etc/ssl/etcd/ssl目录下，将含有master2相关联的文件证书删掉（那个master有问题就出来那个）。
&lt;code&gt;rm -rf admin-master2-key.pem admin-master2.pem member-master2-key.pem member-master2.pem node-master2-key.pem node-master2.pem&lt;/code&gt;
2、将etcd集群中master2的节点移除。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;第一步先转为etcd3版本：export ETCDCTL_API=3
查看etcd集群的成员：
etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem member list
b3487da7c562b17, started, etcd1, https://192.168.11.6:2380, https://192.168.11.6:2379
3ad323c042cc4c05, started, etcd2, https://192.168.11.13:2380, https://192.168.11.13:2379
52a4779150442b41, started, etcd3, https://192.168.11.16:2380, https://192.168.11.16:2379
第三步：移除master2节点，如192.168.11.16
etcdctl member remove 52a4779150442b41 --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem  --key=/etc/ssl/etcd/ssl/node-master1-key.pem
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3、新打开master1机器窗口进入解压包目录下修改conf/hosts.ini,一定要新打开窗口（由于上一步操作使master1机器为etcd3版本指令）。
master2的IP由原来的192.168.11.16改成192.168.11.8；在[all]组里面，master2行整体放在master3下面，[kube-master]和[etcd]组中master2放在master3下面,然后执行安装脚本即可。
4、验证结果：
用&lt;code&gt;kubectl get nodes -o wide&lt;/code&gt;指令看master2IP是否替换；
看etcd集群中是否包含新的master2IP，指令为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、开启etcd3版本：export ETCDCTL_API=3
2、etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem --endpoints=192.168.11.6:2379,192.168.11.8:2379,192.168.11.13:2379 endpoint status
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;一个master中etcd服务不正常在另外一台服务器上恢复etcd方法&#34;&gt;一个master中etcd服务不正常，在另外一台服务器上恢复etcd方法&lt;/h2&gt;
&lt;p&gt;假设为master2（192.168.11.16）宕机了，现以新机器192.168.11.8作为恢复master2的机器。以下操作都在master1机器上执行。
1、在安装介质机器即master1机器上面，进入/etc/ssl/etcd/ssl目录下，将含有master2相关联的文件证书删掉（那个master有问题就出来那个）。
&lt;code&gt;rm -rf admin-master2-key.pem admin-master2.pem member-master2-key.pem member-master2.pem node-master2-key.pem node-master2.pem&lt;/code&gt;
2、将etcd集群中master2的节点移除。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;第一步先转为etcd3版本：export ETCDCTL_API=3
查看etcd集群的成员：
etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem member list
b3487da7c562b17, started, etcd1, https://192.168.11.6:2380, https://192.168.11.6:2379
3ad323c042cc4c05, started, etcd2, https://192.168.11.13:2380, https://192.168.11.13:2379
52a4779150442b41, started, etcd3, https://192.168.11.16:2380, https://192.168.11.16:2379
第三步：移除master2节点，如192.168.11.16
etcdctl member remove 52a4779150442b41 --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem  --key=/etc/ssl/etcd/ssl/node-master1-key.pem
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3、新打开master1机器窗口进入解压包目录下修改conf/hosts.ini,一定要新打开窗口（由于上一步操作使master1机器为etcd3版本指令）。
master2的IP由原来的192.168.11.16改成192.168.11.8；在[all]组里面，master2行整体放在master3下面，[kube-master]和[etcd]组中master2放在master3下面。
4、进入解压包，scripts目录下，编辑install.sh脚本，用#将如下内容注释掉：
&lt;code&gt;ansible-playbook -i $BASE_FOLDER/../k8s/inventory/my_cluster/hosts.ini $BASE_FOLDER/../kubesphere/kubesphere.yml -b&lt;/code&gt;
5、进入解压包，k8s目录下，编辑cluster.yml文件，用#将以下开头的内容至结尾都注释掉&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- hosts: k8s-cluster
  any_errors_fatal: &amp;quot;{{ any_errors_fatal | default(true) }}&amp;quot;
  roles:
    - { role: kubespray-defaults}
    - { role: kubernetes/node, tags: node }
  environment: &amp;quot;{{ proxy_env }}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;6、重新到脚本目录，执行install.sh脚本即可。
7、验证结果：
看etcd集群中是否包含新的master2IP，指令为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、开启etcd3版本：export ETCDCTL_API=3
2、etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-master1.pem --key=/etc/ssl/etcd/ssl/node-master1-key.pem member list
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>tcpdump抓包实战教程</title>
      <link>https://kubesphereio.com/post/tcpdump-package-capture-tutorial/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/tcpdump-package-capture-tutorial/</guid>
      <description>&lt;h1 id=&#34;tcpdump抓包实战教程&#34;&gt;tcpdump抓包实战教程&lt;/h1&gt;
&lt;p&gt;做 web 开发，接口对接过程中，分析 http 请求报文数据包格式是否正确，定位问题，省去无用的甩锅过程，再比如抓取 tcp/udp 报文，分析 tcp 连接过程中的三次握手和四次挥手。windows使用wireshark工具，Linux使用的是tcpdump工具，也可以生成.pcap文件在wireshark图形化工具上分析。&lt;/p&gt;
&lt;h3 id=&#34;命令行&#34;&gt;命令行&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;-i 选择网卡接口&lt;/li&gt;
&lt;li&gt;-n： 不解析主机名&lt;/li&gt;
&lt;li&gt;-nn：不解析端口&lt;/li&gt;
&lt;li&gt;port 80： 抓取80端口上面的数据&lt;/li&gt;
&lt;li&gt;tcp： 抓取tcp的包&lt;/li&gt;
&lt;li&gt;udp：抓取udp的包&lt;/li&gt;
&lt;li&gt;-w： 保存成pcap文件&lt;/li&gt;
&lt;li&gt;dst：目的ip&lt;/li&gt;
&lt;li&gt;src：源ip&lt;/li&gt;
&lt;li&gt;-c：&amp;lt;数据包数目&amp;gt;&lt;/li&gt;
&lt;li&gt;-s0表示可按包长显示完整的包&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tcp标志位&#34;&gt;tcp标志位&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SYN，显示为S，同步标志位，用于建立会话连接，同步序列号；&lt;/li&gt;
&lt;li&gt;ACK，显示为.，确认标志位，对已接收的数据包进行确认；&lt;/li&gt;
&lt;li&gt;FIN，显示为F，完成标志位，表示我已经没有数据要发送了，即将关闭连接；&lt;/li&gt;
&lt;li&gt;RESET，显示为R，重置标志位，用于连接复位、拒绝错误和非法的数据包；&lt;/li&gt;
&lt;li&gt;PUSH，显示为P，推送标志位，表示该数据包被对方接收后应立即交给上层应用，而不在缓冲区排队；&lt;/li&gt;
&lt;li&gt;URGENT，显示为U，紧急标志位，表示数据包的紧急指针域有效，用来保证连接不被阻断，并督促中间设备尽&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;nginx连接问题&#34;&gt;nginx连接问题。&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;抓取nginx(端口30880)交互的包：我们知道nginx交互其实是tcp协议，因此使用如下命令
&lt;code&gt;tcpdump -i eth0 tcp and port 30880 -n -nn -C 20 -W 50 -s 0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;tcp建立连接：先在服务的机器上执行如下命令，接着把multinode.ks.dev.chenshaowen.com:30880的url在浏览器访问即可。以下标志位含义可以参考上面描述。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[root@master ~]#tcpdump -i eth0 tcp and port 30880 -n -nn -C 20 -W 50 -s 0 
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes
10:44:36.391603 IP 139.198.254.12.61476 &amp;gt; 192.168.12.2.30880: Flags [S], seq 3950767038, win 64240, options [mss 1360,nop,wscale 8,nop,nop,sackOK], length 0
10:44:36.391864 IP 192.168.12.2.30880 &amp;gt; 139.198.254.12.61476: Flags [S.], seq 1197638836, ack 3950767039, win 29200, options [mss 1460,nop,nop,sackOK,nop,wscale 7], length 0
10:44:36.447960 IP 139.198.254.12.61478 &amp;gt; 192.168.12.2.30880: Flags [.], ack 1, win 515, length 0
10:44:36.448242 IP 139.198.254.12.61476 &amp;gt; 192.168.12.2.30880: Flags [.], ack 1, win 515, length 0
10:44:36.592856 IP 192.168.12.2.30880 &amp;gt; 139.198.254.12.61476: Flags [P.], seq 5250:6642, ack 1511, win 252, length 1392
10:44:36.647585 IP 139.198.254.12.61476 &amp;gt; 192.168.12.2.30880: Flags [.], ack 6642, win 515, length 0
10:44:41.592442 IP 192.168.12.2.30880 &amp;gt; 139.198.254.12.61476: Flags [F.], seq 6642, ack 1511, win 252, length 0
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;简单分析&#34;&gt;简单分析&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;当url一打开，tcpdump就有数据显示。看到的S标志位，建立会话连接；接着看到.标志位，对接受包进行确认；然后看到P标志位，表示数据包被对方接受上交给上层应用；最后看到F标志位，表示完成，没有数据要发送。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;非服务端tcpdump客户端的数据&#34;&gt;非服务端tcpdump客户端的数据&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;一般情况下，我们都是在服务端使用tcpdump工具抓包的；如果需要在非服务端使用tcpdump抓包可以通过网络流量镜像方式，使服务端的流量到目标地址上。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;某服务不正常tcpdump测试结果&#34;&gt;某服务不正常tcpdump测试结果&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;抓取某不正常的服务(端口30881)交互的包：我们知道nginx交互其实是tcp协议，因此使用如下命令
&lt;code&gt;tcpdump -i eth0 tcp and port 30881 -n -nn -C 20 -W 50 -s 0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;tcp建立连接：先在服务的机器上执行如下命令，接着把multinode.ks.dev.chenshaowen.com:30881的url在浏览器访问即可。以下标志位含义可以参考上面描述。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[root@master ~]#tcpdump -i eth0 tcp and port 30881 -n -nn -C 20 -W 50 -s 0 
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes
10:56:00.569543 IP 139.198.254.12.61608 &amp;gt; 192.168.12.2.30881: Flags [S], seq 1702724216, win 64240, options [mss 1360,nop,wscale 8,nop,nop,sackOK], length 0
10:56:00.569723 IP 192.168.12.2.30881 &amp;gt; 139.198.254.12.61608: Flags [R.], seq 0, ack 1702724217, win 0, length 0
10:56:00.571570 IP 139.198.254.12.61609 &amp;gt; 192.168.12.2.30881: Flags [S], seq 97188145, win 64240, options [mss 1360,nop,wscale 8,nop,nop,sackOK], length 0
10:56:00.571637 IP 192.168.12.2.30881 &amp;gt; 139.198.254.12.61609: Flags [R.], seq 0, ack 97188146, win 0, length 0
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;简单分析-1&#34;&gt;简单分析&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;当url一打开，一开始出现S标志位，表示建立会话连接；接着出现R标志位，表示重置，用于连接复位，拒绝错误和非法的数据包；最后有出现S标志位，再次建立会话连接，一直S与R交替出现，说明该服务不正常。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;参考文章&#34;&gt;参考文章&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://dreamgoing.github.io/tcpdump%E5%AE%9E%E6%88%98.html&#34;&gt;https://dreamgoing.github.io/tcpdump%E5%AE%9E%E6%88%98.html&lt;/a&gt;
&lt;a href=&#34;https://klionsec.github.io/2017/01/31/tcpdump-sniffer-pass/#menu&#34;&gt;https://klionsec.github.io/2017/01/31/tcpdump-sniffer-pass/#menu&lt;/a&gt;
&lt;a href=&#34;https://hubinwei.me/2018/07/25/tcpdump%E6%8A%93%E5%8C%85%E7%BB%83%E4%B9%A0/&#34;&gt;https://hubinwei.me/2018/07/25/tcpdump%E6%8A%93%E5%8C%85%E7%BB%83%E4%B9%A0/&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ubuntu快速安装wecenter</title>
      <link>https://kubesphereio.com/post/ubuntu-quickly-installs-wecenter/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/ubuntu-quickly-installs-wecenter/</guid>
      <description>&lt;h1 id=&#34;ubuntu快速安装wecenter&#34;&gt;ubuntu快速安装wecenter&lt;/h1&gt;
&lt;p&gt;以镜像的形式安装wecenter，简化了nginx和php环境的单独安装。WeCenter（wecenter.com）是一款建立知识社区的开源程序（免费版），专注于企业和行业社区内容的整理、归类、检索和分享，是知识化问答社区的首选软件。后台使用PHP开发，MVC架构，前端使用Bootstrap框架。&lt;/p&gt;
&lt;h2 id=&#34;准备&#34;&gt;准备&lt;/h2&gt;
&lt;p&gt;mysql需要搭建，参考：
&lt;a href=&#34;https://lilinlinlin.github.io/2019/08/13/docker%E9%83%A8%E7%BD%B2mysql5-7/#more&#34;&gt;https://lilinlinlin.github.io/2019/08/13/docker%E9%83%A8%E7%BD%B2mysql5-7/#more&lt;/a&gt;
镜像：wecenter/wecenter:3.3.2&lt;/p&gt;
&lt;h2 id=&#34;wecenter安装&#34;&gt;wecenter安装&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;docker run --name wecenter1 -p 8081:80  -d wecenter/wecenter:3.3.2&lt;/code&gt;
通过外网ip加端口访问,链接为：
&lt;code&gt;eip:8081/install&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;安装成功浏览器打开之后&#34;&gt;安装成功，浏览器打开之后&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/large/006bbiLEgy1g5ygifdatkj30xv0q8abj.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;点击下一步，进入配置系统，正确填入数据库的主机、账号、密码和数据库的名称（默认wecenter），点开始安装。&lt;/li&gt;
&lt;li&gt;上步成功之后，管理员配置，用户名admin，密码自己定义。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考文档&#34;&gt;参考文档：&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://help.websoft9.com/lamp-guide/installation/wecenter/install.html&#34;&gt;http://help.websoft9.com/lamp-guide/installation/wecenter/install.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>基于keepalived-haproxy部署kubesphere高可用方案</title>
      <link>https://kubesphereio.com/post/deploy-kubesphere-high-availability-solution-based-on-keepalived-haproxy/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/deploy-kubesphere-high-availability-solution-based-on-keepalived-haproxy/</guid>
      <description>&lt;h1 id=&#34;基于keepalivedhaproxy部署kubesphere高可用方案&#34;&gt;基于keepalived+haproxy部署kubesphere高可用方案&lt;/h1&gt;
&lt;p&gt;通过keepalived + haproxy实现的，其中keepalived提供一个VIP，通过VIP关联所有的Master节点；然后haproxy提供端口转发功能。由于VIP还是存在Master的机器上的，默认配置API Server的端口是6443，所以我们需要将另外一个端口关联到这个VIP上，一般用8443。&lt;/p&gt;
&lt;h4 id=&#34;环境信息&#34;&gt;环境信息：&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;青云机器操作系统：centos7.5
master1:192.168.0.10
master2:192.168.0.11
master3:192.168.0.12
node:192.168.0.6
vip:192.168.0.200
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;提前说明及遇到过坑&#34;&gt;提前说明及遇到过坑&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;keepalived提供VIP时，需提前规划的vip不能ping通。&lt;/li&gt;
&lt;li&gt;腾讯云服务器上不提供由keepalived方式产生VIP，需要提前在云平台界面HAVIP上创建，创建出的vip再在keepalived.conf中配置。（特别注意这点）。&lt;/li&gt;
&lt;li&gt;通过keepalived服务，vip只能在其中的某一个master中看到，如果ip a方式在每个master都看到，说明keepalived有问题。&lt;/li&gt;
&lt;li&gt;keepalived+haproxy正常安装之后，检查node节点可以和vip通信。&lt;/li&gt;
&lt;li&gt;common.yaml配置文件需要填写正确的ip和转发的端口。&lt;/li&gt;
&lt;li&gt;hosts.ini文件master需要添加master1、master2和master3。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;keepalived安装和配置三台master机器都要安装&#34;&gt;keepalived安装和配置，三台master机器都要安装。&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;安装keepalived, &lt;code&gt;yum install -y keepalived&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;/etc/keepalived/keepalived.conf文件下修改配置,需要修改自己场景的vIP,填写正确服务器的网卡名如：eth0。&lt;/li&gt;
&lt;li&gt;其中killall组件，还需要安装yum install psmisc -y。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;global_defs {
    router_id lb-backup
}
vrrp_script check_haproxy {
  script &amp;quot;/usr/bin/killall -0 haproxy&amp;quot;
  interval 2
  weight 2
}
vrrp_instance VI-kube-master {
    state MASTER
    priority 110
    dont_track_primary
    interface eth0
    virtual_router_id 90
    advert_int 3
    virtual_ipaddress {
        192.168.0.200
    }
	track_script {
    check_haproxy
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启keepalived服务及断电自启：&lt;code&gt;systemctl enable keepalived;systemctl restart keepalived&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;haproxy安装和配置三台master机器都要安装&#34;&gt;haproxy安装和配置，三台master机器都要安装。&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;安装haproyx，&lt;code&gt;yum install -y haproxy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;/etc/haproxy/haproxy.cfg 文件下修改配置，server服务端分别为master的IP值。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;global
        log /dev/log    local0
        log /dev/log    local1 notice
        chroot /var/lib/haproxy
        #stats socket /run/haproxy/admin.sock mode 660 level admin
        stats timeout 30s
        user haproxy
        group haproxy
        daemon
        nbproc 1

defaults
        log     global
        timeout connect 5000
        timeout client  50000
        timeout server  50000

listen kube-master
        bind 0.0.0.0:8443
        mode tcp
        option tcplog
        balance roundrobin
        server master1 192.168.0.10:6443  check inter 10000 fall 2 rise 2 weight 1
        server master2 192.168.0.11:6443  check inter 10000 fall 2 rise 2 weight 1
        server master3 192.168.0.12:6443  check inter 10000 fall 2 rise 2 weight 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重启haproxy服务及断电自启：&lt;code&gt;systemctl enable haproxy;systemctl restart haproxy&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;hostsini配置和commonyaml配置&#34;&gt;hosts.ini配置和common.yaml配置。&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hosts.ini配置实例如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[all]
master1 ansible_connection=local  ip=192.168.0.10
master2  ansible_host=192.168.0.11  ip=192.168.0.11  ansible_ssh_pass=****
master3  ansible_host=192.168.0.12  ip=192.168.0.12  ansible_ssh_pass=****
node1  ansible_host=192.168.0.6  ip=192.168.0.6  ansible_ssh_pass=****

[kube-master]
master1
master2
master3

[kube-node]
node1


[etcd]
master1
master2
master3

[k8s-cluster:children]
kube-node
kube-master 
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;common.yaml的lb配置，注意此处填写VIP，及转发的端口8443。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# apiserver_loadbalancer_domain_name: &amp;quot;lb.kubesphere.local&amp;quot;
loadbalancer_apiserver:
  address: 192.168.0.200
  port: 8443
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;安装kubesphere及结果&#34;&gt;安装kubesphere及结果&lt;/h3&gt;
&lt;p&gt;kubesphere-all-v2.1.0/scripts目录下，执行./install.sh，选择2+yes即可，然后等待脚本的安装。
node正常的结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes -o wide
NAME      STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION          CONTAINER-RUNTIME
master1   Ready    master   95m   v1.15.5   192.168.0.10   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-862.el7.x86_64   docker://18.9.7
master2   Ready    master   88m   v1.15.5   192.168.0.11   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-862.el7.x86_64   docker://18.9.7
master3   Ready    master   88m   v1.15.5   192.168.0.12   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-862.el7.x86_64   docker://18.9.7
node1     Ready    worker   86m   v1.15.5   192.168.0.6    &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-862.el7.x86_64   docker://18.9.7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;apiserver中vip生效的结果&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;telnet 192.168.0.200 8443
Trying 192.168.0.200...
Connected to 192.168.0.200.
Escape character is &#39;^]&#39;.
^CConnection closed by foreign host.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>排查linux服务器的工具</title>
      <link>https://kubesphereio.com/post/tools-to-troubleshoot-linux-servers/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/tools-to-troubleshoot-linux-servers/</guid>
      <description>&lt;p&gt;当服务器遇到问题，或者提前查看服务器质量怎么样，一般可以通过以下几点分析：服务器整体情况，CPU使用情况，内存，磁盘，磁盘io，网络io等。&lt;/p&gt;
&lt;h3 id=&#34;不同环境插件安装&#34;&gt;不同环境插件安装&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;centos&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;yum install sysstat -y&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ubuntu&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;apt install sysstat -y&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;1整体分析之top&#34;&gt;1、整体分析之top&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;执行top指令&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# top
top - 10:41:30 up 6 days, 11:23,  1 user,  load average: 0.98, 0.66, 0.57
Tasks: 304 total,   1 running, 200 sleeping,   0 stopped,   2 zombie
%Cpu(s):  3.1 us,  0.7 sy,  0.0 ni, 95.5 id,  0.6 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem : 32937840 total, 25930000 free,  2666144 used,  4341696 buff/cache
KiB Swap:        0 total,        0 free,        0 used. 30586896 avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
28484 root      20   0 1468632 1.280g  68832 S  39.1  4.1 260:25.96 kube-apiserver
25442 root      20   0 10.277g 308160  80692 S   9.3  0.9 166:39.40 etcd
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;第 1 行：系统时间、运行时间、登录终端数、系统负载（三个数值分别为1分钟、5分钟、15分钟内的平均值，数值越小意味着负载越低）。&lt;/li&gt;
&lt;li&gt;第 2 行：进程总数、运行中的进程数、睡眠中的进程数、停止的进程数、僵死的进程数。一般情况下，只要没有僵死的进程，就没啥大问题。&lt;/li&gt;
&lt;li&gt;第 3 行：用户占用资源百分比、系统内核占用资源百分比、改变过优先级的进程资源百分比、空闲的资源百分比等。&lt;/li&gt;
&lt;li&gt;第 4 行：物理内存总量、内存空闲量、内存使用量、作为内核缓存的内存量。&lt;/li&gt;
&lt;li&gt;第 5 行：虚拟内存总量、虚拟内存空闲量、虚拟内存使用量、已被提前加载的内存量。&lt;/li&gt;
&lt;li&gt;第 6 行里面主要看 PID 和 COMMAND 这两个参数，其中 PID 就是进程 ID ， COMMAND 就是执行的命令，能够看到比较靠前的两个进程都是k8s进程。&lt;/li&gt;
&lt;li&gt;干掉僵尸进程的指令，&lt;code&gt;ps -A -ostat,ppid | grep -e &#39;^[Zz]&#39; | awk &#39;{print $2}&#39; | xargs kill -HUP &amp;gt; /dev/null 2&amp;gt;&amp;amp;1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;在当前这个界面，按下数字键盘 1 能够看到各个 CPU 的详细利用率&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;top - 10:46:53 up 6 days, 11:29,  1 user,  load average: 0.15, 0.53, 0.57
Tasks: 303 total,   1 running, 200 sleeping,   0 stopped,   2 zombie
%Cpu0  :  1.7 us,  1.3 sy,  0.0 ni, 96.3 id,  0.7 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu1  :  2.0 us,  0.7 sy,  0.0 ni, 97.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu2  :  0.7 us,  0.3 sy,  0.0 ni, 99.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;2cpu分析之vmstat&#34;&gt;2、cpu分析之vmstat&lt;/h3&gt;
&lt;p&gt;一般 vmstat 工具的使用是通过两个数字参数来完成的，第一个参数是采样的时间间隔，单位是秒，第二个参数是采样的次数，这次的命令是：vmstat -n 3 2 意思就是隔 3 秒取样一次，一共取样 2 次。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;执行vmstat指令&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# vmstat -n 3 2
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
0  0      0 25910152 684436 3669440    0    0     1    38    4    0  5  2 89  1  3
3  0      0 25910008 684436 3669488    0    0     0   441 8905 15261  1  1 97  1  0
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;21procs&#34;&gt;2.1、procs:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;r ：运行和等待 CPU 时间片的进程数，一般来说整个系统的运行队列不要超过总核数的 2 倍，要不然系统压力太大了。&lt;/li&gt;
&lt;li&gt;b : 等待资源的进程数，比如正在等待磁盘 IO ，网络 IO 这种。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;22cpu&#34;&gt;2.2、cpu：&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;us  ：用户进程消耗 CPU 时间百分比， us 值高的话，说明用户进程消耗 CPU 时间比较长，如果长期大于 50% 的话，那就说明程序还有需要优化的地方。&lt;/li&gt;
&lt;li&gt;sy ：内核进程消耗的 CPU 时间百分比。&lt;/li&gt;
&lt;li&gt;us + sy 参考值为 80% ，如果大于 80% 的话，说明可能存在 CPU 不足。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3内存分析之free&#34;&gt;3、内存分析之free&lt;/h3&gt;
&lt;p&gt;一般我们使用free -m即可&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# free -m
              total        used        free      shared  buff/cache   available
Mem:          32165        2608       25354          13        4203       29861
Swap:             0           0           0
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;如果应用程序可用内存/系统物理内存大于 70% 的话，说明内存是充足的，没啥问题，但是如果小于 20% 的话，就要考虑增加内存了。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4磁盘分析之df&#34;&gt;4、磁盘分析之df&lt;/h3&gt;
&lt;p&gt;排查磁盘问题，首先要排查磁盘空间够不够，df和du就可以。df查看磁盘使用情况；du查看目录占用磁盘情况及子文件占用情况。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# df -hT
Filesystem     Type      Size  Used Avail Use% Mounted on
udev           devtmpfs   16G     0   16G   0% /dev
tmpfs          tmpfs     3.2G   14M  3.2G   1% /run
/dev/vda2      ext4       99G  9.4G   84G  11% /
tmpfs          tmpfs      16G     0   16G   0% /dev/shm
tmpfs          tmpfs     5.0M     0  5.0M   0% /run/lock
tmpfs          tmpfs      16G     0   16G   0% /sys/fs/cgroup

root@master2:~# du -sh
516M    .
root@master2:~# du -sh *
16K     1.sh
434M    etcd
82M     etcd.tar.gz
4.0K    etcd.txt
root@master2:~# du -h --max-depth=1
4.0K    ./.cache
8.0K    ./.gnupg
8.0K    ./.ansible
434M    ./etcd
948K    ./.kube
8.0K    ./.ssh
8.0K    ./.vim
516M    .
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;5磁盘io分析之iostat&#34;&gt;5、磁盘io分析之iostat&lt;/h3&gt;
&lt;p&gt;在对数据库进行操作时，第一要考虑就是磁盘io操作，因为相对来说，如果在某个时间段给磁盘进行大量的写入操作会造成程序等待时间长，导致客户端那边好久都没啥反应。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# iostat -xdk 3 2
Linux 4.15.0-115-generic (master2)      09/11/2020      _x86_64_        (16 CPU)

Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
loop0            0.00    0.00      0.02      0.00     0.00     0.00   0.00   0.00    1.75    0.00   0.00    18.07     0.00   0.16   0.00
loop1            0.05    0.00      0.07      0.00     0.00     0.00   0.00   0.00    1.66    0.00   0.00     1.49     0.00   0.06   0.00
vda              0.74   19.38     18.68    154.10     0.00    10.14   0.07  34.35    6.74   17.52   0.28    25.39     7.95   0.79   1.60

Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
loop0            0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00
loop1            0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00
vda              0.00    7.00      0.00     61.33     0.00     6.00   0.00  46.15    0.00    1.33   0.00     0.00     8.76   0.00   0.00
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;rkB/s ：每秒读取数据量 kB 。&lt;/li&gt;
&lt;li&gt;wkB/s ：每秒写入数据量 kB 。&lt;/li&gt;
&lt;li&gt;svctm ：I/O 请求的平均服务时间，单位毫秒。&lt;/li&gt;
&lt;li&gt;util ：一秒中有百分之几的时间用于 I/O 操作，如果接近 100% 说明磁盘带宽跑满了，这个时候就要优化程序或者增加磁盘了。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;6网络io分析之sar&#34;&gt;6、网络io分析之sar&lt;/h3&gt;
&lt;p&gt;网络 IO 的话，可以通过 sar -n DEV 3 2 这条命令来看，和上面的差不多，意思就是每隔 3 秒取样一次，一共取样 2 次。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@master2:~# sar -n DEV 3 2
Linux 4.15.0-115-generic (master2)      09/11/2020      _x86_64_        (16 CPU)

11:06:46 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
11:06:49 AM      eth0    854.33    859.33    199.93    210.99      0.00      0.00      0.00      0.00
11:06:49 AM     tunl0    113.67    108.33     10.92      9.86      0.00      0.00      0.00      0.00
11:06:49 AM calide035c655d8     98.33    108.33     11.04     12.63      0.00      0.00      0.00      0.00
11:06:49 AM        lo     58.00     58.00     13.18     13.18      0.00      0.00      0.00      0.00
11:06:49 AM cali3d31c61f18c      6.00      3.33      4.41      2.08      0.00      0.00      0.00      0.00
11:06:49 AM nodelocaldns      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
11:06:49 AM kube-ipvs0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
11:06:49 AM cali7d0b83575fc      7.00      8.00      2.46      0.81      0.00      0.00      0.00      0.00
11:06:49 AM cali056accc6554     24.33     21.00      2.93     10.81      0.00      0.00      0.00      0.00
11:06:49 AM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00

11:06:49 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
11:06:52 AM      eth0    623.67    620.00    109.84    127.33      0.00      0.00      0.00      0.00
11:06:52 AM     tunl0     97.33     90.00      6.64      8.74      0.00      0.00      0.00      0.00
11:06:52 AM calide035c655d8     93.67    104.00     10.52      8.68      0.00      0.00      0.00      0.00
11:06:52 AM        lo     65.67     65.67     11.60     11.60      0.00      0.00      0.00      0.00
11:06:52 AM cali3d31c61f18c      6.00      3.67      5.62      2.78      0.00      0.00      0.00      0.00
11:06:52 AM nodelocaldns      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
11:06:52 AM kube-ipvs0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
11:06:52 AM cali7d0b83575fc      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
11:06:52 AM cali056accc6554      3.67      2.33      0.24      0.39      0.00      0.00      0.00      0.00
11:06:52 AM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00

Average:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
Average:         eth0    739.00    739.67    154.89    169.16      0.00      0.00      0.00      0.00
Average:        tunl0    105.50     99.17      8.78      9.30      0.00      0.00      0.00      0.00
Average:    calide035c655d8     96.00    106.17     10.78     10.65      0.00      0.00      0.00      0.00
Average:           lo     61.83     61.83     12.39     12.39      0.00      0.00      0.00      0.00
Average:    cali3d31c61f18c      6.00      3.50      5.02      2.43      0.00      0.00      0.00      0.00
Average:    nodelocaldns      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:    kube-ipvs0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:    cali7d0b83575fc      3.50      4.00      1.23      0.40      0.00      0.00      0.00      0.00
Average:    cali056accc6554     14.00     11.67      1.58      5.60      0.00      0.00      0.00      0.00
Average:      docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;IFACE ：LAN 接口&lt;/li&gt;
&lt;li&gt;rxpck/s ：每秒钟接收的数据包&lt;/li&gt;
&lt;li&gt;txpck/s ：每秒钟发送的数据包&lt;/li&gt;
&lt;li&gt;rxKB/s ：每秒接收的数据量，单位 KByte&lt;/li&gt;
&lt;li&gt;txKB/s ：每秒发出的数据量，单位 KByte&lt;/li&gt;
&lt;li&gt;rxcmp/s ：每秒钟接收的压缩数据包&lt;/li&gt;
&lt;li&gt;txcmp/s ：每秒钟发送的压缩数据包&lt;/li&gt;
&lt;li&gt;rxmcst/s：每秒钟接收的多播数据包&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>提前预防K8s集群资源不足的处理方式配置</title>
      <link>https://kubesphereio.com/post/prevent-the-configuration-of-the-k8s-cluster-from-under-resourcing-in-advance/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/prevent-the-configuration-of-the-k8s-cluster-from-under-resourcing-in-advance/</guid>
      <description>&lt;h1 id=&#34;提前预防k8s集群资源不足的处理方式配置&#34;&gt;提前预防K8s集群资源不足的处理方式配置&lt;/h1&gt;
&lt;p&gt;在管理集群的时候我们常常会遇到资源不足的情况，在这种情况下我们要保证整个集群可用，并且尽可能减少应用的损失。根据该问题提出以下两种方案：一种为优化kubelet参数，另一种为脚本化诊断处理。&lt;/p&gt;
&lt;h2 id=&#34;概念解释&#34;&gt;概念解释&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;CPU 的使用时间是可压缩的，换句话说它本身无状态，申请资源很快，也能快速正常回收。&lt;/li&gt;
&lt;li&gt;内存（memory）大小是不可压缩的，因为它是有状态的（内存里面保存的数据），申请资源很慢（需要计算和分配内存块的空间），并且回收可能失败（被占用的内存一般不可回收）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;优化kubelet参数&#34;&gt;优化kubelet参数&lt;/h2&gt;
&lt;p&gt;优化kubelet参数通过k8s资源表示、节点资源配置及kubelet参数设置、应用优先级和资源动态调整这几个方面来介绍。k8s资源表示为yaml文件中如何添加requests和limites参数。节点资源配置及kubelet参数设置描述为一个node上面资源配置情况，从而来优化kubelet参数。应用优先级描述为当资源不足时，优先保留那些pod不被驱逐。资源动态调整描述为运算能力的增减，如：HPA 、VPA和Cluster Auto Scaler。&lt;/p&gt;
&lt;h3 id=&#34;k8s资源表示&#34;&gt;k8s资源表示&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;在k8s中，资源表示配置字段是 spec.containers[].resource.limits/request.cpu/memory。yaml格式如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;spec:
  template:
    ...
    spec:
      containers:
        ...
        resources:
          limits:
            cpu: &amp;quot;1&amp;quot;
            memory: 1000Mi
          requests:
            cpu: 20m
            memory: 100Mi
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;资源动态调整&#34;&gt;资源动态调整&lt;/h3&gt;
&lt;p&gt;动态调整的思路：应用的实际流量会不断变化，因此使用率也是不断变化的，为了应对应用流量的变化，我们应用能够自动调整应用的资源。比如在线商品应用在促销的时候访问量会增加，我们应该自动增加 pod 运算能力来应对；当促销结束后，有需要自动降低 pod 的运算能力防止浪费。
运算能力的增减有两种方式：改变单个 pod 的资源，已经增减 pod 的数量。这两种方式对应了 kubernetes 的 HPA 和 VPA和Cluster Auto Scaler。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HPA: 横向 pod 自动扩展的思路是这样的：kubernetes 会运行一个 controller，周期性地监听 pod 的资源使用情况，当高于设定的阈值时，会自动增加 pod 的数量；当低于某个阈值时，会自动减少 pod 的数量。自然，这里的阈值以及 pod 的上限和下限的数量都是需要用户配置的。&lt;/li&gt;
&lt;li&gt;VPA: VPA 调整的是单个 pod 的 request 值（包括 CPU 和 memory）VPA 包括三个组件：
（1）Recommander：消费 metrics server 或者其他监控组件的数据，然后计算 pod 的资源推荐值
（2）Updater：找到被 vpa 接管的 pod 中和计算出来的推荐值差距过大的，对其做 update 操作（目前是 evict，新建的 pod 在下面 admission controller 中会使用推荐的资源值作为 request）
（3）Admission Controller：新建的 pod 会经过该 Admission Controller，如果 pod 是被 vpa 接管的，会使用 recommander 计算出来的推荐值&lt;/li&gt;
&lt;li&gt;CLuster Auto Scaler：能够根据整个集群的资源使用情况来增减节点。Cluster Auto Scaler 就是监控这个集群因为资源不足而 pending 的 pod，根据用户配置的阈值调用公有云的接口来申请创建机器或者销毁机器。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;节点资源配置及kubelet参数设置&#34;&gt;节点资源配置及kubelet参数设置&lt;/h3&gt;
&lt;p&gt;节点资源的配置一般分为 2 种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;资源预留：为系统进程和 k8s 进程预留资源&lt;/li&gt;
&lt;li&gt;pod 驱逐：节点资源到达一定使用量，开始驱逐 pod&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Node Capacity&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;kube-reserved&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;system-reserved&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;eviction-threshold&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Allocatable&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Node Capacity：Node的所有硬件资源&lt;/li&gt;
&lt;li&gt;kube-reserved：给kube组件预留的资源：kubelet,kube-proxy以及docker等&lt;/li&gt;
&lt;li&gt;system-reserved：给system进程预留的资源&lt;/li&gt;
&lt;li&gt;eviction-threshold：kubelet eviction的阈值设定&lt;/li&gt;
&lt;li&gt;Allocatable：真正scheduler调度Pod时的参考值（保证Node上所有Pods的request resource不超过Allocatable）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;allocatable的值即对应 describe node 时看到的allocatable容量，pod 调度的上限&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;计算公式：节点上可配置值 = 总量 - 预留值 - 驱逐阈值

Allocatable = Capacity - Reserved(kube+system) - Eviction Threshold
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以上配置均在kubelet 中添加，涉及的参数有：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--kube-reserved=cpu=200m,memory=250Mi \
--system-reserved=cpu=200m,memory=250Mi \
--eviction-hard=memory.available&amp;lt;5%,nodefs.available&amp;lt;10%,imagefs.available&amp;lt;10% \
--eviction-soft=memory.available&amp;lt;10%,nodefs.available&amp;lt;15%,imagefs.available&amp;lt;15% \
--eviction-soft-grace-period=memory.available=2m,nodefs.available=2m,imagefs.available=2m \
--eviction-max-pod-grace-period=120 \
--eviction-pressure-transition-period=30s \
--eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=500Mi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以上配置均为百分比，举例：&lt;/p&gt;
&lt;p&gt;以2核4GB内存40GB磁盘空间的配置为例，Allocatable是1.6 CPU，3.3Gi 内存，25Gi磁盘。当pod的总内存消耗大于3.3Gi或者磁盘消耗大于25Gi时，会根据相应策略驱逐pod。
Allocatable = 4Gi - 250Mi -250Mi - 4Gi*5% = 3.3Gi&lt;/p&gt;
&lt;p&gt;（1） 配置 k8s组件预留资源的大小，CPU、Mem&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;指定为k8s系统组件（kubelet、kube-proxy、dockerd等）预留的资源量，
如：--kube-reserved=cpu=1,memory=2Gi,ephemeral-storage=1Gi。
这里的kube-reserved只为非pod形式启动的kube组件预留资源，假如组件要是以static pod（kubeadm）形式启动的，那并不在这个kube-reserved管理并限制的cgroup中，而是在kubepod这个cgroup中。
（ephemeral storage需要kubelet开启feature-gates，预留的是临时存储空间（log，EmptyDir），生产环境建议先不使用）
ephemeral-storage是kubernetes1.8开始引入的一个资源限制的对象，kubernetes 1.10版本中kubelet默认已经打开的了,到目前1.11还是beta阶段，主要是用于对本地临时存储使用空间大小的限制，如对pod的empty dir、/var/lib/kubelet、日志、容器可读写层的使用大小的限制。
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（2）配置 系统守护进程预留资源的大小（预留的值需要根据机器上容器的密度做一个合理的值）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;含义：为系统守护进程(sshd, udev等)预留的资源量，
如：--system-reserved=cpu=500m,memory=1Gi,ephemeral-storage=1Gi。
注意，除了考虑为系统进程预留的量之外，还应该为kernel和用户登录会话预留一些内存。
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（3）配置 驱逐pod的硬阈值&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;含义：设置进行pod驱逐的阈值，这个参数只支持内存和磁盘。
通过--eviction-hard标志预留一些内存后，当节点上的可用内存降至保留值以下时，
kubelet 将会对pod进行驱逐。
配置：--eviction-hard=memory.available&amp;lt;5%,nodefs.available&amp;lt;10%,imagefs.available&amp;lt;10%
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（4）配置 驱逐pod的软阈值&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--eviction-soft=memory.available&amp;lt;10%,nodefs.available&amp;lt;15%,imagefs.available&amp;lt;15%
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（5）定义达到软阈值之后，持续时间超过多久才进行驱逐&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--eviction-soft-grace-period=memory.available=2m,nodefs.available=2m,imagefs.available=2m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（6）驱逐pod前最大等待时间=min(pod.Spec.TerminationGracePeriodSeconds, eviction-max-pod-grace-period)，单位为秒&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--eviction-max-pod-grace-period=120
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（7）至少回收的资源量&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=500Mi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（8）防止波动,kubelet 多久才上报节点的状态&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--eviction-pressure-transition-period=30s
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;应用优先级&#34;&gt;应用优先级&lt;/h3&gt;
&lt;p&gt;当资源不足时，配置了如上驱逐参数，pod之间的驱逐顺序是怎样的呢？以下描述设置不同优先级来确保集群中核心的组件不被驱逐还正常运行，OOM 的优先级如下,pod oom 值越低，也就越不容易被系统杀死。：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;BestEffort Pod &amp;gt; Burstable Pod &amp;gt; 其它进程（内核init进程等） &amp;gt; Guaranteed Pod &amp;gt; kubelet/docker 等 &amp;gt; sshd 等进程
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;kubernetes 把 pod 分成了三个 QoS 等级，而其中和limits和requests参数有关：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Guaranteed：oom优先级最低，可以考虑数据库应用或者一些重要的业务应用。除非 pods 使用超过了它们的 limits，或者节点的内存压力很大而且没有 QoS 更低的 pod，否则不会被杀死。&lt;/li&gt;
&lt;li&gt;Burstable：这种类型的 pod 可以多于自己请求的资源（上限有 limit 指定，如果 limit 没有配置，则可以使用主机的任意可用资源），但是重要性认为比较低，可以是一般性的应用或者批处理任务。&lt;/li&gt;
&lt;li&gt;Best Effort：oom优先级最高，集群不知道 pod 的资源请求情况，调度不考虑资源，可以运行到任意节点上（从资源角度来说），可以是一些临时性的不重要应用。pod 可以使用节点上任何可用资源，但在资源不足时也会被优先杀死。
Pod 的 requests 和 limits 是如何对应到这三个 QoS 等级上的，可以用下面一张表格概括：&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;request是否配置&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;limits是否配置&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;两者的关系&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Qos&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;requests=limits&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Guaranteed&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;所有容器的cpu和memory都必须配置相同的requests和limits&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;request&amp;lt;limit&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Burstable&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;只要有容器配置了cpu或者memory的request和limits就行&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;否&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Burstable&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;只要有容器配置了cpu或者memory的request就行&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;否&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;是&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Guaranteed/Burstable&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;如果配置了limits，k8s会自动把对应资源的request设置和limits一样。如果所有容器所有资源都配置limits，那就是Guaranteed;如果只有部分配置了limits，就是Burstable&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;否&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;否&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Best Effort&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;所有的容器都没有配置资源requests或limits&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;request和limits相同，可以参考资源动态调整中的VPA设置合理值。&lt;/li&gt;
&lt;li&gt;如果只配置了limits，没有配置request，k8s会把request值和limits值一样。&lt;/li&gt;
&lt;li&gt;如果只配置了request，没有配置limits，该pod共享node上可用的资源，实际上很反对这样设置。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;
&lt;p&gt;动态地资源调整通过 kubelet 驱逐程序进行的，但需要和应用优先级配合使用才能达到很好的效果，否则可能驱逐集群中核心组件。&lt;/p&gt;
&lt;h2 id=&#34;脚本化诊断处理&#34;&gt;脚本化诊断处理&lt;/h2&gt;
&lt;p&gt;什么叫脚本化诊断处理呢？它的含义为：当集群中的某台机器资源（一般指memory）用到85%-90%时，脚本自动检查到且该节点为不可调度。缺点为：背离了资源动态调整中CLuster Auto Scaler特点。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;集群中每台机器都可以执行kubectl指令：
如果没有设置，可将master机器上的$HOME/.kube/config文件拷贝到node机器上。&lt;/li&gt;
&lt;li&gt;可以通过&lt;a href=&#34;https://github.com/Forest-L/shell-operator&#34;&gt;shell-operator&lt;/a&gt;自动诊断机器资源且做cordon操作处理&lt;/li&gt;
&lt;li&gt;脚本中关键说明
（1）获取本地IP：ip a | grep &amp;lsquo;state UP&amp;rsquo; -A2| grep inet | grep -v inet6 | grep -v 127 | sed &amp;rsquo;s/^[ \t]*//g&amp;rsquo; | cut -d &#39; &#39; -f2 | cut -d &amp;lsquo;/&amp;rsquo; -f1
（2）获取本地ip对应的node名：kubectl get nodes -o  wide | grep &amp;ldquo;本地ip&amp;rdquo; | awk &amp;lsquo;{print $1}&amp;rsquo;
（3）不可调度：kubectl cordon node &amp;lt;node名&amp;gt;
（4）获取总内存： free -m | awk &amp;lsquo;NR==2{print $2}&amp;rsquo;
（5）获取使用内存： free -m | awk &amp;lsquo;NR==2{print $3}&amp;rsquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考文档&#34;&gt;参考文档&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/zh/docs/tasks/administer-cluster/out-of-resource/&#34;&gt;https://kubernetes.io/zh/docs/tasks/administer-cluster/out-of-resource/&lt;/a&gt;
&lt;a href=&#34;https://cizixs.com/2018/06/25/kubernetes-resource-management/&#34;&gt;https://cizixs.com/2018/06/25/kubernetes-resource-management/&lt;/a&gt;
&lt;a href=&#34;https://segmentfault.com/a/1190000021402192&#34;&gt;https://segmentfault.com/a/1190000021402192&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>测量网络速度工具之iperf认知</title>
      <link>https://kubesphereio.com/post/iperf-cognition-as-a-network-speed-measurement-tool/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/iperf-cognition-as-a-network-speed-measurement-tool/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;iperf工具是测量服务器网络速度工具，它通过测量服务器可以处理的最大网络吞吐量来测试网络速度，在遇到网络问题时特别有用。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;1下载源代码服务端和客户端都要安装&#34;&gt;1、下载源代码（服务端和客户端都要安装）&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;wget https://iperf.fr/download/source/iperf-2.0.8-source.tar.gz&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;安装编译环境
&lt;code&gt;yum install gcc-c++ -y&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;解压并安装iperf&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;tar -xvf iperf-2.0.8-source.tar.gz

cd iperf-2.0.8/

./configure &amp;amp;&amp;amp; make &amp;amp;&amp;amp; make install
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;2测试&#34;&gt;2、测试&lt;/h3&gt;
&lt;h4 id=&#34;21服务端执行iperf指令&#34;&gt;2.1、服务端执行iperf指令&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;iperf -s -p 12345 -i 1&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;-s表示以服务器模式运行。&lt;/li&gt;
&lt;li&gt;-p设置服务监听端口，测试时该端口在服务上没有被占用即可。&lt;/li&gt;
&lt;li&gt;-i设置每次报告之间的时间间隔，单位为s。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;iperf -s -p 12345 -i 1
------------------------------------------------------------
Server listening on TCP port 12345
TCP window size: 85.3 KByte (default)
------------------------------------------------------------
[  4] local 192.168.0.8 port 12345 connected with 192.168.0.10 port 52648
[ ID] Interval       Transfer     Bandwidth
[  4]  0.0- 1.0 sec   119 MBytes   999 Mbits/sec
[  4]  1.0- 2.0 sec  60.3 MBytes   506 Mbits/sec
[  4]  2.0- 3.0 sec  61.7 MBytes   517 Mbits/sec
[  4]  3.0- 4.0 sec  60.9 MBytes   511 Mbits/sec
[  4]  4.0- 5.0 sec  59.9 MBytes   503 Mbits/sec
[  4]  5.0- 6.0 sec  61.1 MBytes   512 Mbits/sec
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;22客户端执行iperf指令&#34;&gt;2.2、客户端执行iperf指令&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;iperf -c XX.XX.XX.XX -p 1234 -i 1&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;其中XX.XX.XX.XX为服务端的ip。&lt;/li&gt;
&lt;li&gt;-p要和服务端设置的相同。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;iperf -c 192.168.0.8 -p 12345 -i 1
------------------------------------------------------------
Client connecting to 192.168.0.8, TCP port 12345
TCP window size: 45.0 KByte (default)
------------------------------------------------------------
[  3] local 192.168.0.10 port 52648 connected with 192.168.0.8 port 12345
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   123 MBytes  1.03 Gbits/sec
[  3]  1.0- 2.0 sec  60.1 MBytes   504 Mbits/sec
[  3]  2.0- 3.0 sec  62.0 MBytes   520 Mbits/sec
[  3]  3.0- 4.0 sec  60.4 MBytes   506 Mbits/sec
[  3]  4.0- 5.0 sec  60.2 MBytes   505 Mbits/sec
[  3]  5.0- 6.0 sec  60.6 MBytes   509 Mbits/sec
[  3]  6.0- 7.0 sec  62.1 MBytes   521 Mbits/sec
[  3]  7.0- 8.0 sec  59.4 MBytes   498 Mbits/sec
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.huaweicloud.com/kunpeng/software/iperf.html&#34;&gt;iperf&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>添加公网ip到kubernetes的apiserver操作指南</title>
      <link>https://kubesphereio.com/post/add-public-ip-to-kubernetes-apiserver-operation-guide/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/add-public-ip-to-kubernetes-apiserver-operation-guide/</guid>
      <description>&lt;p&gt;通常情况下，我们的kubernetes集群是内网环境，如果希望通过本地访问这个集群，怎么办呢？大家想到的是Kubeadm在初始化的时候会为管理员生成一个 Kubeconfig文件，把它下载下来 是不是就可以？事实证明这样不行， 因为这个集群是内网集群，Kubeconfig文件 中APIServer的地址是内网ip。解决方案很简单，把公网ip签到证书里面就可以，其中有apiServerCertSANs这个选项，只要把公网IP写到这里，再启动这个集群的时候，这个公网ip就会签到证书里。&lt;/p&gt;
&lt;h2 id=&#34;1-环境信息&#34;&gt;1. 环境信息&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;安装方式：kubeadm&lt;/li&gt;
&lt;li&gt;内网IP：192.168.0.8&lt;/li&gt;
&lt;li&gt;外网IP：139.198.19.37&lt;/li&gt;
&lt;li&gt;证书目录：/etc/kubernetes/pki&lt;/li&gt;
&lt;li&gt;kubeadm配置文件目录：/etc/kubernetes/kubeadm-config.yaml&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-查看apiserver的证书包含的ip进入到证书目录执行&#34;&gt;2. 查看apiserver的证书包含的ip,进入到证书目录执行&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;cd /etc/kubernetes/pki&lt;/code&gt;
&lt;code&gt;openssl x509 -in apiserver.crt -noout -text&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;openssl x509 -in apiserver.crt -noout -text
Certificate:
    Data:
        ................
        Validity
            Not Before: Jun  5 02:26:44 2020 GMT
            Not After : Jun  5 02:26:44 2021 GMT
        ..................
        X509v3 extensions:
            ..........
            X509v3 Subject Alternative Name:
                IP Address:192.168.0.8
    .......
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3-添加公网ip到apiserver&#34;&gt;3. 添加公网IP到apiserver&lt;/h2&gt;
&lt;p&gt;绑定的公网ip为 139.198.19.37 ，确保公网ip的防火墙已经打开6443端口&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3.1 登录到主节点，进入 /etc/kubernetes/目录下&lt;/li&gt;
&lt;li&gt;3.2 修改kubeadm-config.yaml，找到 ClusterConfiguration 中的 	certSANs (如无，在 apiServer 下添加这一配置)，如下。添加刚才绑定的 139.198.19.37 到 certSANs 下，保存文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
etcd:
  external:
    endpoints:
    - https://192.168.0.8:2379
apiServer:
  extraArgs:
    authorization-mode: Node,RBAC
  timeoutForControlPlane: 4m0s
  certSANs:
    - 192.168.0.8
    - 139.198.19.37
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;3.3 执行如下命令更新 apiserver.crt apiserver.key
注意需要把之前apiserver.crt apiserver.key做备份,进入到pki目录下，执行如下指令做备份：
&lt;code&gt;mv apiserver.crt apiserver.crt-bak&lt;/code&gt;
&lt;code&gt;mv apiserver.key apiserver.key-bak&lt;/code&gt;
备份完之后，回到/etc/kubernetes目录下，执行公网ip添加到apiserver操作指令为：
kubeadm init phase certs apiserver &amp;ndash;config kubeadm-config.yaml&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubeadm init phase certs apiserver --config kubeadm-config.yaml
[certs] Generating &amp;quot;apiserver&amp;quot; certificate and key
[certs] apiserver serving cert is signed for DNS names [192.168.0.8  139.198.19.37]
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;3.4 再次查看apiserver中证书包含的ip，指令如下,看的公网ip则操作成功。
openssl x509 -in pki/apiserver.crt -noout -text | grep 139.198.19.37&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;openssl x509 -in pki/apiserver.crt -noout -text | grep 139.198.19.37
                IP Address:192.168.0.8, IP Address:139.198.19.37
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;3.5 重启kube-apiserver
如果是高可用集群，直接杀死当前节点的kube-apiserver进程，等待kubelet拉起kube-apiserver即可。需要在三个节点执行步骤1到步骤4，逐一更新。
如果是非高可用集群，杀死kube-apiserver可能会导致服务有中断，需要在业务低峰的时候操作。
进入/etc/kubernetes/manifests目录下，mv kube-apiserver.yaml文件至别的位置，然后又移回来即可&lt;/li&gt;
&lt;li&gt;3.6 修改kubeconfig中的server ip地址为 139.198.19.37，保存之后就可以直接通过公网访问kubernetes集群
&lt;code&gt;kubectl --kubeconfig config config view&lt;/code&gt;
&lt;code&gt;kubectl --kubeconfig config get node&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-附录-证书过期处理方式&#34;&gt;4. 附录-证书过期处理方式&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;kubeadm部署的k8s集群，默认证书目录为：/etc/kubernetes/pki,如果非pki，以ssl为例，需要创建软链接。证书过期包含核心组件apiserver和node上的token。&lt;/li&gt;
&lt;li&gt;4.1 master节点-apiserver处理方式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;1、查看证书有效期
cd /etc/kubernetes
openssl x509 -in ssl/apiserver.crt -noout -enddate 
2. 更新过期证书（/etc/kubernetes） (先在master1 节点执行)
创建软连接pki -&amp;gt; ssl ： ln -s ssl/ pki    (如pki存在，可略过)

kubeadm alpha certs renew apiserver 
kubeadm alpha certs renew apiserver-kubelet-client 
kubeadm alpha certs renew front-proxy-client 
3. 更新kubeconfig（/etc/kubernetes）(master1 节点)
需更新admin.conf / scheduler.conf / controller-manager.conf / kubelet.conf

kubeadm alpha certs renew admin.conf
kubeadm alpha certs renew controller-manager.conf
kubeadm alpha certs renew scheduler.conf

特别注意：以master1为例，将如下master1替换实际的节点名称。
kubeadm alpha kubeconfig user --client-name=system:node:master1 --org=system:nodes &amp;gt; kubelet.conf

4. 如上述kubeconfig中apiserver地址非lb地址，则修改为lb地址：(master1 节点)
https://192.168.0.13:6443 -&amp;gt; https://{ lb domain or ip }:6443

5. 重启k8s master组件：(master1 节点)
docker ps -af name=k8s_kube-apiserver* -q | xargs --no-run-if-empty docker rm -f
docker ps -af name=k8s_kube-scheduler* -q | xargs --no-run-if-empty docker rm -f
docker ps -af name=k8s_kube-controller-manager* -q | xargs --no-run-if-empty docker rm -f
systemctl restart kubelet

6. 验证kubeconfig有效性及查看节点状态 (master1 节点)

kubectl get node –kubeconfig admin.conf
kubectl get node –kubeconfig scheduler.conf
kubectl get node –kubeconfig controller-manager.conf
kubectl get node –kubeconfig kubelet.conf

7. 特别注意：同步master1证书/etc/kubernetes/ssl至master2、master3的对应路径中/etc/kubernetes/ssl（同步前建议备份旧证书）
证书路径：/etc/kubernetes/ssl

8. 更新kubeconfig（/etc/kubernetes）(master2, master3)

kubeadm alpha certs renew admin.conf
kubeadm alpha certs renew controller-manager.conf
kubeadm alpha certs renew scheduler.conf

特别注意：以下命令中以master2、master3为例，将如下master2/master3替换实际的节点名称。
kubeadm alpha kubeconfig user --client-name=system:node:master2 --org=system:nodes &amp;gt; kubelet.conf （master2）
kubeadm alpha kubeconfig user --client-name=system:node:master3 --org=system:nodes &amp;gt; kubelet.conf （master3）

9. 如上述kubeconfig中apiserver地址非lb地址，则修改为lb地址：(master2、master3)

https://192.168.0.13:6443 -&amp;gt; https://{ lb domain or ip }:6443

注：涉及文件：admin.conf、controller-manager.conf、scheduler.conf、kubelet.conf

10. 重启master2、master3中对应master组件

docker ps -af name=k8s_kube-apiserver* -q | xargs --no-run-if-empty docker rm -f
docker ps -af name=k8s_kube-scheduler* -q | xargs --no-run-if-empty docker rm -f
docker ps -af name=k8s_kube-controller-manager* -q | xargs --no-run-if-empty docker rm -f
systemctl restart kubelet

11. 验证kubeconfig有效性 （master2、master3）

kubectl get node –kubeconfig admin.conf
kubectl get node –kubeconfig scheduler.conf
kubectl get node –kubeconfig controller-manager.conf
kubectl get node –kubeconfig kubelet.conf
12. 更新~/.kube/config （master1、master2、master3）

cp admin.conf ~/.kube/config
注：如node节点也需使用kubectl，将master1上的~/.kube/config拷贝至对应node节点~/.kube/config

13. 验证~/.kube/config有效性：

 kubectl get node  查看集群状态
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;4.2 node节点token证书处理方式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;1. kubeadm token list 查看输出若为空或显示日期过期，则需重新生成。

2. kubeadm token create 重新生成token

3. 记录token值,保存下来。

4. 替换node节点/etc/kubernetes/ bootstrap-kubelet.conf中token （所有node节点）

5. 删除/etc/kubernetes/kubelet.conf （所有node节点）

rm -rf /etc/kubernetes/kubelet.conf
6. 重启kubelet （所有node节点）

systemctl restart kubelet
7. 查看节点状态：

kubectl get node 验证集群状态
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>脚本化触发Kubernetes集群事件的工具-Shell-operator</title>
      <link>https://kubesphereio.com/post/scripting-the-tool-shell-operator-that-triggers-kubernetes-cluster-events/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/scripting-the-tool-shell-operator-that-triggers-kubernetes-cluster-events/</guid>
      <description>&lt;p&gt;Shell-operator是用于在Kubernetes集群中运行事件驱动脚本工具。Shell-operator通过脚本作为事件触发的钩子（hook），在Kubernetes集群事件和Shell脚本之间提供了一个转化层。触发钩子包含add, update和delete。以pod add为例通俗的话说，当新创建了一个pod时，会自动触发脚本中else部分。&lt;/p&gt;
&lt;h4 id=&#34;shell-operator特点&#34;&gt;Shell-operator特点：&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;轻松管理Kubernetes集群：可以是bash，python和kubectl。&lt;/li&gt;
&lt;li&gt;Kubernetes对象事件：钩子触发包含add, update或delete事件。&lt;/li&gt;
&lt;li&gt;对象选择器和属性过滤器：可以监视一组特定的对象并检测其属性的变化。&lt;/li&gt;
&lt;li&gt;配置简单：钩子绑定语法格式为yaml/json输出。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1环境准备只要kubernetes环境即可&#34;&gt;1、环境准备（只要Kubernetes环境即可）&lt;/h2&gt;
&lt;p&gt;Kubernetes: v1.18.3&lt;/p&gt;
&lt;h2 id=&#34;2快速开始包含bash和python实例&#34;&gt;2、快速开始（包含bash和python实例）&lt;/h2&gt;
&lt;p&gt;目录结构&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@node1 shell-operator]# tree
.
├── Dockerfile
├── hooks
│   └── pods-hook.sh
└── shell-operator-pod.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;21shell-operator最简设置步骤为&#34;&gt;2.1、Shell-operator最简设置步骤为：&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;用钩子（脚本）构建的镜像。&lt;/li&gt;
&lt;li&gt;在Kubernetes集群中创建必要的RBAC对象。&lt;/li&gt;
&lt;li&gt;使用构建的镜像运行一个pod/deployment。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;22用钩子脚本构建镜像&#34;&gt;2.2、用钩子脚本构建镜像&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;钩子是一个脚本，当执行&amp;ndash;config选项时，配置将以yaml/json格式输出。&lt;/li&gt;
&lt;li&gt;以下创建一个简单的operator将来监视所有namespaces下的所有pod，并记录新pod的名字。&lt;/li&gt;
&lt;li&gt;包含pods-hook.sh和Dockerfile&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以bash脚本为例&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pods-hook.sh&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;#!/usr/bin/env bash

if [[ $1 == &amp;quot;--config&amp;quot; ]] ; then
  cat &amp;lt;&amp;lt;EOF
configVersion: v1
kubernetes:
- apiVersion: v1
  kind: Pod
  executeHookOnEvent: [&amp;quot;Added&amp;quot;]
EOF
else
  podName=$(jq -r .[0].object.metadata.name $BINDING_CONTEXT_PATH)
  echo &amp;quot;Pod &#39;${podName}&#39; added&amp;quot;
fi
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;添加执行权限。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;chmod +x pods-hook.sh&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于flant/shell-operator:latest的基础镜像构建新的Dockerfile。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;FROM flant/shell-operator:latest
ADD pods-hook.sh /hooks
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;构建一个新的镜像。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker build -t lilinlinlin/shell-operator:monitor-pods&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;推镜像至dockerhub, 仓库名根据自身的情况而定，也可以不操作此步骤。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;docker push lilinlinlin/shell-operator:monitor-pods&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;23创建rbac对象&#34;&gt;2.3、创建RBAC对象&lt;/h3&gt;
&lt;p&gt;需要监视所有namespaces下的pods，意味着我们需要shell-operator的特定RBAC定义。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create namespace example-monitor-pods
kubectl create serviceaccount monitor-pods-acc --namespace example-monitor-pods
kubectl create clusterrole monitor-pods --verb=get,watch,list --resource=pods
kubectl create clusterrolebinding monitor-pods --clusterrole=monitor-pods --serviceaccount=example-monitor-pods:monitor-pods-acc
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;24在集群中部署shell-operator&#34;&gt;2.4、在集群中部署shell-operator&lt;/h3&gt;
&lt;p&gt;shell-operator-pod.yaml文件为&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: shell-operator
spec:
  containers:
  - name: shell-operator
    image: lilinlinlin/shell-operator:monitor-pods
    imagePullPolicy: IfNotPresent
  serviceAccountName: monitor-pods-acc
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;部署shell-operator时，pods-hook.sh脚本中if的部分就会被执行。新创建pod时，else部分就会被执行。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubectl -n example-monitor-pods apply -f shell-operator-pod.yaml&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3测试验证效果&#34;&gt;3、测试验证效果&lt;/h2&gt;
&lt;p&gt;部署一个nginx服务,查看日志。会出现Pod nginx-****** added字样，说明监视生效了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl run nginx --image=nginx
kubectl -n example-monitor-pods logs pod/shell-operator -f
...
INFO[0027] queue task HookRun:main                       operator.component=handleEvents queue=main
INFO[0030] Execute hook                                  binding=kubernetes hook=pods-hook.sh operator.component=taskRunner queue=main task=HookRun
INFO[0030] Pod &#39;nginx-775dd7f59c-hr7kj&#39; added  binding=kubernetes hook=pods-hook.sh output=stdout queue=main task=HookRun
INFO[0030] Hook executed successfully                    binding=kubernetes hook=pods-hook.sh operator.component=taskRunner queue=main task=HookRun
...
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;4python实例&#34;&gt;4、python实例&lt;/h2&gt;
&lt;p&gt;实例中直接运行else部分。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、hooks/00-hook.py
#!/usr/bin/env python

import sys

if __name__ == &amp;quot;__main__&amp;quot;:
    if len(sys.argv)&amp;gt;1 and sys.argv[1] == &amp;quot;--config&amp;quot;:
        print &#39;{&amp;quot;configVersion&amp;quot;:&amp;quot;v1&amp;quot;, &amp;quot;onStartup&amp;quot;: 10}&#39;
    else:
        print &amp;quot;OnStartup Python powered hook&amp;quot;

2、Dockerfile
FROM flant/shell-operator:latest-alpine3.11
RUN apk --no-cache add python
ADD hooks /hooks

3、shell-operator-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: shell-operator
spec:
  containers:
  - name: shell-operator
    image: registry.mycompany.com/shell-operator:startup-python
    imagePullPolicy: IfNotPresent

4、运行

docker build -t &amp;quot;registry.mycompany.com/shell-operator:startup-python&amp;quot; .
kubectl create ns example-startup-python
kubectl -n example-startup-python apply -f shell-operator-pod.yaml
kubectl -n example-startup-python logs -f po/shell-operator
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;5清理环境&#34;&gt;5、清理环境&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;kubectl delete ns example-monitor-pods
kubectl delete clusterrole monitor-pods
kubectl delete clusterrolebinding monitor-pods
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;6参考&#34;&gt;6、参考&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/flant/shell-operator&#34;&gt;shell-operator&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openebs基于k8s安装</title>
      <link>https://kubesphereio.com/post/k8s-openebs-install/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/k8s-openebs-install/</guid>
      <description>&lt;h2 id=&#34;ceph介绍&#34;&gt;ceph介绍&lt;/h2&gt;
&lt;p&gt;ceph为块存储，以下通过cephadm安装ceph集群，ceph主要有三个基本角色：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OSD 用于集群中所有数据与对象的存储，处理集群数据的复制、恢复、回填、再均衡，并向其他osd守护进程发送心跳，然后向 Monitor 提供一些监控信息。&lt;/li&gt;
&lt;li&gt;Monitor 监控整个集群的状态，维护集群的 cluster MAP 二进制表，保证集群数据的一致性。&lt;/li&gt;
&lt;li&gt;MDS (可选) 为 Ceph 文件系统提供元数据计算、缓存与同步。MDS 进程并不是必须的进程，只有需要使用 CephFS 时，才需要配置 MDS 节点。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1环境准备&#34;&gt;1、环境准备&lt;/h2&gt;
&lt;h3 id=&#34;11节点规划&#34;&gt;1.1、节点规划&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;节点  &lt;/th&gt;
&lt;th&gt;os  &lt;/th&gt;
&lt;th&gt;IP   &lt;/th&gt;
&lt;th&gt;磁盘&lt;/th&gt;
&lt;th&gt;角色&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ceph1&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.13&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;cephadm，mgr, mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph2&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.14&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph3&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.16&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;12修改hosts文件&#34;&gt;1.2、修改hosts文件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hostnamectl set-hostname ceph1  #节点1上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph2  #节点2上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph3  #节点3上执行&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;13配置hosts&#34;&gt;1.3、配置hosts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt; /etc/hosts &amp;lt;&amp;lt;EOF  
192.168.0.13 ceph1
192.168.0.14 ceph2
192.168.0.16 ceph3
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;14关闭防火墙&#34;&gt;1.4、关闭防火墙&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都要执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld
setenforce 0
sed -i &#39;s/^SELINUX=.*/SELINUX=disabled/&#39; /etc/selinux/config
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;15时间同步&#34;&gt;1.5、时间同步&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;yum install -y chrony
systemctl enable --now chronyd

以ceph1作为时间参考，以下在三个节点都操作，添加server ceph1 prefer
vi /etc/chrony.conf
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
server ceph1 prefer

重启chrony
systemctl restart chronyd
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;16dockerlvm2和python3安装&#34;&gt;1.6、docker、lvm2和python3安装&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;三个节点都需要执行：
curl -fsSL get.docker.com -o get-docker.sh 
sh get-docker.sh
systemctl enable docker
systemctl restart docker

yum install -y python3

yum -y install lvm2
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2cephadm安装ceph1&#34;&gt;2、cephadm安装(ceph1)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;官方下载方法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;git下载&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;git clone --single-branch --branch octopus https://github.com/ceph/ceph.git&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cp ceph/src/cephadm/cephadm ./&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cephadm安装(ceph1)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;./cephadm add-repo --release octopus
./cephadm install

mkdir -p /etc/ceph
cephadm bootstrap --mon-ip 192.168.0.13

安装完成提示：

             URL: https://ceph1:8443/
            User: admin
        Password: 86yvswzdzd

INFO:cephadm:You can access the Ceph CLI with:

        sudo /usr/sbin/cephadm shell --fsid 3861dbf4-fd47-11ea-a373-5254228af870 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

INFO:cephadm:Please consider enabling telemetry to help improve Ceph:

        ceph telemetry on

For more information see:

        https://docs.ceph.com/docs/master/mgr/telemetry/

INFO:cephadm:Bootstrap complete.
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;配置ceph.pub
将ceph1的/etc/ceph/ceph.pub放入到ceph2和ceph3的.ssh/authorized_keys,ceph2和ceph3下root目录下没有.ssh，得先创建，最低权限为700，authorized_keys最低权限600。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ceph指令生效&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;cephadm shell&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;添加节点&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph3&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3osd部署&#34;&gt;3、OSD部署&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cephadm shell下执行&lt;/li&gt;
&lt;li&gt;若块设备没显示，可加&amp;ndash;refresh参数&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[ceph: root@ceph1 /]# ceph orch device ls
HOST   PATH      TYPE   SIZE  DEVICE_ID     MODEL  VENDOR  ROTATIONAL  AVAIL  REJECT REASONS
ceph1  /dev/vdc  hdd   50.0G  vol-e801hi3v  n/a    0x1af4  1           True
ceph1  /dev/vda  hdd   20.0G  i-2iwis9yr    n/a    0x1af4  1           False  locked
ceph1  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
ceph2  /dev/vdc  hdd   50.0G  vol-lqcchpox  n/a    0x1af4  1           True
ceph2  /dev/vda  hdd   20.0G  i-r53b2flp    n/a    0x1af4  1           False  locked
ceph2  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  locked, Insufficient space (&amp;lt;5GB)
ceph3  /dev/vdc  hdd    100G  vol-p0eksa5m  n/a    0x1af4  1           True
ceph3  /dev/vda  hdd   20.0G  i-f6njdmtq    n/a    0x1af4  1           False  locked
ceph3  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
[ceph: root@ceph1 /]#

devcice确认true后，执行
[ceph: root@ceph1 /]# ceph orch apply osd --all-available-devices
Scheduled osd.all-available-devices update...
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;查看集群状态,ceph方式及运行容器方式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;ceph方式：
[ceph: root@ceph1 /]# ceph -s
  cluster:
    id:     af1f33f0-fd69-11ea-8530-5254228af870
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 66s)
    mgr: ceph1.rbarkj(active, since 12m), standbys: ceph2.uzdnki
    osd: 3 osds: 3 up (since 14s), 3 in (since 14s)

  data:
    pools:   1 pools, 1 pgs
    objects: 1 objects, 0 B
    usage:   3.0 GiB used, 197 GiB / 200 GiB avail
    pgs:     1 active+clean

容器方式：
[root@ceph1 ceph]# docker ps -a
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES
b417841153a9        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-osd -…&amp;quot;   5 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-osd.2
9dbd720f4a6f        prom/prometheus:v2.18.1      &amp;quot;/bin/prometheus --c…&amp;quot;   6 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-prometheus.ceph1
f860282adf85        prom/alertmanager:v0.20.0    &amp;quot;/bin/alertmanager -…&amp;quot;   6 minutes ago       Up 6 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-alertmanager.ceph1
a6cb4b354f46        ceph/ceph-grafana:6.6.2      &amp;quot;/bin/sh -c &#39;grafana…&amp;quot;   16 minutes ago      Up 16 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-grafana.ceph1
b745f57895f3        prom/node-exporter:v0.18.1   &amp;quot;/bin/node_exporter …&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-node-exporter.ceph1
b00c1a2c7ef6        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-crash…&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-crash.ceph1
99c703f18f67        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mgr -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mgr.ceph1.rbarkj
e972d3d3f13a        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mon -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mon.ceph1

&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.ceph.com/en/latest/cephadm/install/&#34;&gt;https://docs.ceph.com/en/latest/cephadm/install/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>rook基于k8s安装</title>
      <link>https://kubesphereio.com/post/k8s-rook-install/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/k8s-rook-install/</guid>
      <description>&lt;h2 id=&#34;rook介绍&#34;&gt;rook介绍&lt;/h2&gt;
&lt;p&gt;Rook是目前开源中比较流行的云原生的存储编排系统，专注于如何实现把ceph运行在Kubernetes平台上。将之前手工执行部署、初始化、配置、扩展、升级、迁移、灾难恢复、监控以及资源管理等转变为自动触发。比如集群增加一块磁盘，rook能自动初始化为一个OSD，并自动加入到合适的故障中，这个osd在kubernetes中是以pod形式运行的。&lt;/p&gt;
&lt;h2 id=&#34;1部署&#34;&gt;1、部署&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Rook-Ceph底层存储可以是：卷、分区、block模式的pv。主要包括三部分：CRD、Operator、Cluster。&lt;/li&gt;
&lt;li&gt;下载rook安装文件并部署&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;git clone --single-branch --branch release-1.3 https://github.com/rook/rook.git

cd cluster/examples/kubernetes/ceph

注意：若是重装需清空node节点的/var/lib/rook下的文件， 并且要保证挂载的卷或者分区是没有文件系统的

部署：
(1) CRD:    kubectl create -f common.yaml
(2) Operator:   kubectl create -f operator.yaml
(3) Cluster:    kubectl create -f cluster.yaml

[root@node1 ~]# kubectl get pod -n rook-ceph
NAME                                              READY   STATUS      RESTARTS   AGE
csi-cephfsplugin-l5rw5                            3/3     Running     0          8m33s
csi-cephfsplugin-lrzfj                            3/3     Running     0          22m
csi-cephfsplugin-mqbg8                            3/3     Running     0          53m
csi-cephfsplugin-provisioner-58888b54f5-fj8tf     5/5     Running     5          53m
csi-cephfsplugin-provisioner-58888b54f5-p2vpt     5/5     Running     9          53m
csi-rbdplugin-5mmsl                               3/3     Running     0          8m34s
csi-rbdplugin-d4966                               3/3     Running     0          53m
csi-rbdplugin-mcsxw                               3/3     Running     0          22m
csi-rbdplugin-provisioner-6ddbf76966-498gn        6/6     Running     11         53m
csi-rbdplugin-provisioner-6ddbf76966-z2vfq        6/6     Running     6          53m
rook-ceph-crashcollector-node1-69666f444d-5bgh9   1/1     Running     0          5m41s
rook-ceph-crashcollector-node2-6c5b88dcf5-th4xk   1/1     Running     0          6m4s
rook-ceph-crashcollector-node3-54b5c58544-hz6m8   1/1     Running     0          22m
rook-ceph-mgr-a-5d85bd689f-g2dfh                  1/1     Running     0          5m41s
rook-ceph-mon-a-76c84f876b-m62d7                  1/1     Running     0          6m38s
rook-ceph-mon-b-6dc744d5b8-bspvh                  1/1     Running     0          6m22s
rook-ceph-mon-c-67f5987779-4l8vf                  1/1     Running     0          6m4s
rook-ceph-operator-6659fb4ddd-wdxnp               1/1     Running     3          55m
rook-ceph-osd-0-57954bcb4f-8b48j                  1/1     Running     0          4m59s
rook-ceph-osd-1-7c59f96f47-xnfpc                  1/1     Running     0          4m48s
rook-ceph-osd-prepare-node1-hrzg6                 1/1     Running     0          5m38s
rook-ceph-osd-prepare-node2-c5wcl                 0/1     Completed   0          5m38s
rook-ceph-osd-prepare-node3-gkt7g                 0/1     Completed   0          5m38s
rook-discover-6ll64                               1/1     Running     0          8m34s
rook-discover-bvbsh                               1/1     Running     0          22m
rook-discover-cm5ls                               1/1     Running     0          54m
说明

(1) csi-cephfsplugin-*, csi-rbdplugin-* :   ceph-FS 和ceph-rbd CSI
(2) rook-ceph-crashcollector-*:  crash 收集器
(3) rook-ceph-mgr-*:  管理后台
(4) root-ceph-mon-*: Mon监视器，维护集群中的各种状态
(5) rook-ceph-osd-*: ceph-OSD，主要功能是数据的存储，本例中每个盘会起一个OSD
(6) rook-discorer-*:  检测符合要求的存储设备
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;12修改hosts文件&#34;&gt;1.2、修改hosts文件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hostnamectl set-hostname ceph1  #节点1上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph2  #节点2上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph3  #节点3上执行&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;13配置hosts&#34;&gt;1.3、配置hosts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt; /etc/hosts &amp;lt;&amp;lt;EOF  
192.168.0.13 ceph1
192.168.0.14 ceph2
192.168.0.16 ceph3
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;14关闭防火墙&#34;&gt;1.4、关闭防火墙&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都要执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld
setenforce 0
sed -i &#39;s/^SELINUX=.*/SELINUX=disabled/&#39; /etc/selinux/config
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;15时间同步&#34;&gt;1.5、时间同步&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;yum install -y chrony
systemctl enable --now chronyd

以ceph1作为时间参考，以下在三个节点都操作，添加server ceph1 prefer
vi /etc/chrony.conf
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
server ceph1 prefer

重启chrony
systemctl restart chronyd
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;16dockerlvm2和python3安装&#34;&gt;1.6、docker、lvm2和python3安装&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;三个节点都需要执行：
curl -fsSL get.docker.com -o get-docker.sh 
sh get-docker.sh
systemctl enable docker
systemctl restart docker

yum install -y python3

yum -y install lvm2
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2cephadm安装ceph1&#34;&gt;2、cephadm安装(ceph1)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;官方下载方法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;git下载&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;git clone --single-branch --branch octopus https://github.com/ceph/ceph.git&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cp ceph/src/cephadm/cephadm ./&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cephadm安装(ceph1)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;./cephadm add-repo --release octopus
./cephadm install

mkdir -p /etc/ceph
cephadm bootstrap --mon-ip 192.168.0.13

安装完成提示：

             URL: https://ceph1:8443/
            User: admin
        Password: 86yvswzdzd

INFO:cephadm:You can access the Ceph CLI with:

        sudo /usr/sbin/cephadm shell --fsid 3861dbf4-fd47-11ea-a373-5254228af870 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

INFO:cephadm:Please consider enabling telemetry to help improve Ceph:

        ceph telemetry on

For more information see:

        https://docs.ceph.com/docs/master/mgr/telemetry/

INFO:cephadm:Bootstrap complete.
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;配置ceph.pub
将ceph1的/etc/ceph/ceph.pub放入到ceph2和ceph3的.ssh/authorized_keys,ceph2和ceph3下root目录下没有.ssh，得先创建，最低权限为700，authorized_keys最低权限600。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ceph指令生效&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;cephadm shell&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;添加节点&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph3&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3osd部署&#34;&gt;3、OSD部署&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cephadm shell下执行&lt;/li&gt;
&lt;li&gt;若块设备没显示，可加&amp;ndash;refresh参数&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[ceph: root@ceph1 /]# ceph orch device ls
HOST   PATH      TYPE   SIZE  DEVICE_ID     MODEL  VENDOR  ROTATIONAL  AVAIL  REJECT REASONS
ceph1  /dev/vdc  hdd   50.0G  vol-e801hi3v  n/a    0x1af4  1           True
ceph1  /dev/vda  hdd   20.0G  i-2iwis9yr    n/a    0x1af4  1           False  locked
ceph1  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
ceph2  /dev/vdc  hdd   50.0G  vol-lqcchpox  n/a    0x1af4  1           True
ceph2  /dev/vda  hdd   20.0G  i-r53b2flp    n/a    0x1af4  1           False  locked
ceph2  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  locked, Insufficient space (&amp;lt;5GB)
ceph3  /dev/vdc  hdd    100G  vol-p0eksa5m  n/a    0x1af4  1           True
ceph3  /dev/vda  hdd   20.0G  i-f6njdmtq    n/a    0x1af4  1           False  locked
ceph3  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
[ceph: root@ceph1 /]#

devcice确认true后，执行
[ceph: root@ceph1 /]# ceph orch apply osd --all-available-devices
Scheduled osd.all-available-devices update...
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;查看集群状态,ceph方式及运行容器方式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;ceph方式：
[ceph: root@ceph1 /]# ceph -s
  cluster:
    id:     af1f33f0-fd69-11ea-8530-5254228af870
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 66s)
    mgr: ceph1.rbarkj(active, since 12m), standbys: ceph2.uzdnki
    osd: 3 osds: 3 up (since 14s), 3 in (since 14s)

  data:
    pools:   1 pools, 1 pgs
    objects: 1 objects, 0 B
    usage:   3.0 GiB used, 197 GiB / 200 GiB avail
    pgs:     1 active+clean

容器方式：
[root@ceph1 ceph]# docker ps -a
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES
b417841153a9        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-osd -…&amp;quot;   5 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-osd.2
9dbd720f4a6f        prom/prometheus:v2.18.1      &amp;quot;/bin/prometheus --c…&amp;quot;   6 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-prometheus.ceph1
f860282adf85        prom/alertmanager:v0.20.0    &amp;quot;/bin/alertmanager -…&amp;quot;   6 minutes ago       Up 6 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-alertmanager.ceph1
a6cb4b354f46        ceph/ceph-grafana:6.6.2      &amp;quot;/bin/sh -c &#39;grafana…&amp;quot;   16 minutes ago      Up 16 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-grafana.ceph1
b745f57895f3        prom/node-exporter:v0.18.1   &amp;quot;/bin/node_exporter …&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-node-exporter.ceph1
b00c1a2c7ef6        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-crash…&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-crash.ceph1
99c703f18f67        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mgr -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mgr.ceph1.rbarkj
e972d3d3f13a        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mon -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mon.ceph1

&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.ceph.com/en/latest/cephadm/install/&#34;&gt;https://docs.ceph.com/en/latest/cephadm/install/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>基于cephadm安装ceph</title>
      <link>https://kubesphereio.com/post/k8s-ceph-install/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/k8s-ceph-install/</guid>
      <description>&lt;h2 id=&#34;ceph介绍&#34;&gt;ceph介绍&lt;/h2&gt;
&lt;p&gt;ceph为块存储，以下通过cephadm安装ceph集群，ceph主要有三个基本角色：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OSD 用于集群中所有数据与对象的存储，处理集群数据的复制、恢复、回填、再均衡，并向其他osd守护进程发送心跳，然后向 Monitor 提供一些监控信息。&lt;/li&gt;
&lt;li&gt;Monitor 监控整个集群的状态，维护集群的 cluster MAP 二进制表，保证集群数据的一致性。&lt;/li&gt;
&lt;li&gt;MDS (可选) 为 Ceph 文件系统提供元数据计算、缓存与同步。MDS 进程并不是必须的进程，只有需要使用 CephFS 时，才需要配置 MDS 节点。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1环境准备&#34;&gt;1、环境准备&lt;/h2&gt;
&lt;h3 id=&#34;11节点规划&#34;&gt;1.1、节点规划&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;节点  &lt;/th&gt;
&lt;th&gt;os  &lt;/th&gt;
&lt;th&gt;IP   &lt;/th&gt;
&lt;th&gt;磁盘&lt;/th&gt;
&lt;th&gt;角色&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ceph1&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.13&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;cephadm，mgr, mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph2&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.14&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph3&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.16&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;12修改hosts文件&#34;&gt;1.2、修改hosts文件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hostnamectl set-hostname ceph1  #节点1上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph2  #节点2上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph3  #节点3上执行&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;13配置hosts&#34;&gt;1.3、配置hosts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt; /etc/hosts &amp;lt;&amp;lt;EOF  
192.168.0.13 ceph1
192.168.0.14 ceph2
192.168.0.16 ceph3
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;14关闭防火墙&#34;&gt;1.4、关闭防火墙&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都要执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld
setenforce 0
sed -i &#39;s/^SELINUX=.*/SELINUX=disabled/&#39; /etc/selinux/config
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;15时间同步&#34;&gt;1.5、时间同步&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;yum install -y chrony
systemctl enable --now chronyd

以ceph1作为时间参考，以下在三个节点都操作，添加server ceph1 prefer
vi /etc/chrony.conf
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
server ceph1 prefer

重启chrony
systemctl restart chronyd
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;16dockerlvm2和python3安装&#34;&gt;1.6、docker、lvm2和python3安装&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;三个节点都需要执行：
curl -fsSL get.docker.com -o get-docker.sh 
sh get-docker.sh
systemctl enable docker
systemctl restart docker

yum install -y python3

yum -y install lvm2
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2cephadm安装ceph1&#34;&gt;2、cephadm安装(ceph1)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;官方下载方法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;git下载&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;git clone --single-branch --branch octopus https://github.com/ceph/ceph.git&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cp ceph/src/cephadm/cephadm ./&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cephadm安装(ceph1)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;./cephadm add-repo --release octopus
./cephadm install

mkdir -p /etc/ceph
cephadm bootstrap --mon-ip 192.168.0.13

安装完成提示：

             URL: https://ceph1:8443/
            User: admin
        Password: 86yvswzdzd

INFO:cephadm:You can access the Ceph CLI with:

        sudo /usr/sbin/cephadm shell --fsid 3861dbf4-fd47-11ea-a373-5254228af870 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

INFO:cephadm:Please consider enabling telemetry to help improve Ceph:

        ceph telemetry on

For more information see:

        https://docs.ceph.com/docs/master/mgr/telemetry/

INFO:cephadm:Bootstrap complete.
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;配置ceph.pub
将ceph1的/etc/ceph/ceph.pub放入到ceph2和ceph3的.ssh/authorized_keys,ceph2和ceph3下root目录下没有.ssh，得先创建，最低权限为700，authorized_keys最低权限600。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ceph指令生效&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;cephadm shell&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;添加节点&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph3&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3osd部署&#34;&gt;3、OSD部署&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cephadm shell下执行&lt;/li&gt;
&lt;li&gt;若块设备没显示，可加&amp;ndash;refresh参数&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[ceph: root@ceph1 /]# ceph orch device ls
HOST   PATH      TYPE   SIZE  DEVICE_ID     MODEL  VENDOR  ROTATIONAL  AVAIL  REJECT REASONS
ceph1  /dev/vdc  hdd   50.0G  vol-e801hi3v  n/a    0x1af4  1           True
ceph1  /dev/vda  hdd   20.0G  i-2iwis9yr    n/a    0x1af4  1           False  locked
ceph1  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
ceph2  /dev/vdc  hdd   50.0G  vol-lqcchpox  n/a    0x1af4  1           True
ceph2  /dev/vda  hdd   20.0G  i-r53b2flp    n/a    0x1af4  1           False  locked
ceph2  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  locked, Insufficient space (&amp;lt;5GB)
ceph3  /dev/vdc  hdd    100G  vol-p0eksa5m  n/a    0x1af4  1           True
ceph3  /dev/vda  hdd   20.0G  i-f6njdmtq    n/a    0x1af4  1           False  locked
ceph3  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
[ceph: root@ceph1 /]#

devcice确认true后，执行
[ceph: root@ceph1 /]# ceph orch apply osd --all-available-devices
Scheduled osd.all-available-devices update...
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;查看集群状态,ceph方式及运行容器方式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;ceph方式：
[ceph: root@ceph1 /]# ceph -s
  cluster:
    id:     af1f33f0-fd69-11ea-8530-5254228af870
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 66s)
    mgr: ceph1.rbarkj(active, since 12m), standbys: ceph2.uzdnki
    osd: 3 osds: 3 up (since 14s), 3 in (since 14s)

  data:
    pools:   1 pools, 1 pgs
    objects: 1 objects, 0 B
    usage:   3.0 GiB used, 197 GiB / 200 GiB avail
    pgs:     1 active+clean

容器方式：
[root@ceph1 ceph]# docker ps -a
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES
b417841153a9        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-osd -…&amp;quot;   5 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-osd.2
9dbd720f4a6f        prom/prometheus:v2.18.1      &amp;quot;/bin/prometheus --c…&amp;quot;   6 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-prometheus.ceph1
f860282adf85        prom/alertmanager:v0.20.0    &amp;quot;/bin/alertmanager -…&amp;quot;   6 minutes ago       Up 6 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-alertmanager.ceph1
a6cb4b354f46        ceph/ceph-grafana:6.6.2      &amp;quot;/bin/sh -c &#39;grafana…&amp;quot;   16 minutes ago      Up 16 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-grafana.ceph1
b745f57895f3        prom/node-exporter:v0.18.1   &amp;quot;/bin/node_exporter …&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-node-exporter.ceph1
b00c1a2c7ef6        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-crash…&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-crash.ceph1
99c703f18f67        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mgr -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mgr.ceph1.rbarkj
e972d3d3f13a        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mon -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mon.ceph1

&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.ceph.com/en/latest/cephadm/install/&#34;&gt;https://docs.ceph.com/en/latest/cephadm/install/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>k8s-default-Storage-Class搭建</title>
      <link>https://kubesphereio.com/post/k8s-default-storage-class-installer/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/k8s-default-storage-class-installer/</guid>
      <description>&lt;h1 id=&#34;k8s-default-storage-class搭建&#34;&gt;k8s default Storage Class搭建&lt;/h1&gt;
&lt;p&gt;在k8s中，StorageClass为动态存储，存储大小设置不确定，对存储并发要求高和读写速度要求高等方面有很大优势；pv为静态存储，存储大小要确定。而default Storage Class的作用为pvc文件没有标识任何和storageclass相关联的信息，但通过annotations属性关联起来。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kubesphereio.com/img/storageclass.png&#34; alt=&#34;storageClass&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-创建storageclass&#34;&gt;1. 创建storageClass&lt;/h2&gt;
&lt;p&gt;要使用 StorageClass，我们就得安装对应的自动配置程序，比如我们这里存储后端使用的是 nfs，那么我们就需要使用到一个 nfs-client 的自动配置程序，我们也叫它 Provisioner，这个程序使用我们已经配置好的 nfs 服务器，来自动创建持久卷，也就是自动帮我们创建 PV。
nfs服务器参考博客&lt;a href=&#34;https://kubesphereio.com/post/linux-nfs-install/&#34;&gt;nfs服务器搭建&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;11-nfs-client-provisioner&#34;&gt;1.1 nfs-client-provisioner&lt;/h3&gt;
&lt;p&gt;前提：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes 1.9+&lt;/li&gt;
&lt;li&gt;Existing NFS Share&lt;/li&gt;
&lt;li&gt;helm&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;111-安装nfs-client-provisioner指令&#34;&gt;1.1.1 安装nfs-client-provisioner指令：&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;helm install --name nfs-client --set nfs.server=192.168.0.9 --set nfs.path=/nfsdatas stable/nfs-client-provisioner&lt;/code&gt;
如果安装报错，显示没有该helm的stable，在机器上添加helm 源
&lt;code&gt;helm repo add stable http://mirror.azure.cn/kubernetes/charts/&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;112-卸载指令&#34;&gt;1.1.2 卸载指令：&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;helm delete nfs-client&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;113-验证storageclass是否存在&#34;&gt;1.1.3 验证storageClass是否存在&lt;/h4&gt;
&lt;p&gt;在相应安装nfs-client-provisioner机器上执行：&lt;code&gt;kubectl get sc&lt;/code&gt;即可，如下所示：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@i-cj1r8a8m ~]# kubectl get sc
NAME              PROVISIONER                                   AGE
nfs-client    cluster.local/my-release-nfs-client-provisioner   1h
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2-defaultstorageclass&#34;&gt;2. DefaultStorageClass&lt;/h2&gt;
&lt;p&gt;在定义StorageClass时，可以在Annotation中添加一个键值对：storageclass.kubernetes.io/is-default-class: true，那么此StorageClass就变成默认的StorageClass了。&lt;/p&gt;
&lt;h3 id=&#34;21-第一种方法&#34;&gt;2.1 第一种方法&lt;/h3&gt;
&lt;p&gt;在这个PVC对象中添加一个声明StorageClass对象的标识，这里我们可以利用一个annotations属性来标识，如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvctest
  annotations:
    volume.beta.kubernetes.io/storage-class: &amp;quot;nfs-client&amp;quot;
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 10Mi
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;22-第二种方法&#34;&gt;2.2 第二种方法&lt;/h3&gt;
&lt;p&gt;用 kubectl patch 命令来更新：
&lt;code&gt;kubectl patch storageclass nfs-client -p &#39;{&amp;quot;metadata&amp;quot;: {&amp;quot;annotations&amp;quot;:{&amp;quot;storageclass.kubernetes.io/is-default-class&amp;quot;:&amp;quot;true&amp;quot;}}}&#39;&lt;/code&gt;
最后结果中包含default为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@i-cj1r8a8m ~]# kubectl get sc
NAME                 PROVISIONER                                   AGE
nfs-client (default)cluster.local/my-release-nfs-client-provisioner 2h
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>k8s-service-认知</title>
      <link>https://kubesphereio.com/post/k8s-service-cognize/</link>
      <pubDate>Mon, 29 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/k8s-service-cognize/</guid>
      <description></description>
    </item>
    
    <item>
      <title>k8s-ConfigMap认知</title>
      <link>https://kubesphereio.com/post/k8s-configmap-cognize/</link>
      <pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/k8s-configmap-cognize/</guid>
      <description>&lt;h1 id=&#34;k8s-configmap-认知&#34;&gt;k8s configMap 认知&lt;/h1&gt;
&lt;p&gt;许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息，而ConfigMap作用是保存配置信息，格式为键值对，可以单独一个key/value使用，也可以多个key/value构成的文件使用。数据不包含敏感信息的字符串。ConfigMap必须在Pod引用它之前创建;Pod只能使用同一个命名空间内的ConfigMap。&lt;/p&gt;
&lt;h2 id=&#34;1-常见configmap创建方式&#34;&gt;1 常见configMap创建方式&lt;/h2&gt;
&lt;h3 id=&#34;11-从key-value字符串创建configmap&#34;&gt;1.1 从key-value字符串创建ConfigMap&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl create configmap config1 --from-literal=config.test=good&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;12-从目录创建rootconfigmaptest1和rootconfigmaptest2-中&#34;&gt;1.2 从目录创建,/root/configmap/test1和/root/configmap/test2 中&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;vi test1
a:a1
vi test2
b:b1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create configmap special-config --from-file=/root/configmap/&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;13-通过pod形式创建最常用的方式configtestyaml内容如下&#34;&gt;1.3 通过pod形式创建,最常用的方式configtest.yaml内容如下：&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;kind: ConfigMap
apiVersion: v1
metadata:
  name: configtest
  namespace: default
data:
  test.property.1: a1
  test.property.2: b2
  test.property.file: |-
    property.1=a1
    property.2=b2
    property.3=c3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f configtest.yaml&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-常见查看configmap方式&#34;&gt;2. 常见查看configmap方式&lt;/h2&gt;
&lt;p&gt;以上面所示的第三种方式创建的configmap为例，名为：configtest。configmap可以简称cm。&lt;/p&gt;
&lt;h3 id=&#34;21--o-json格式&#34;&gt;2.1 &amp;ldquo;-o json&amp;quot;格式，&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl get cm configtest -o json&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;22-go模板的格式&#34;&gt;2.2 go模板的格式&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;-o go-template=&#39;{{.data}}&#39;格式
kubectl get configmap configtest -o go-template=&#39;{{.data}}&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;3-常见使用configmap场景&#34;&gt;3. 常见使用configmap场景&lt;/h2&gt;
&lt;p&gt;通过多种方式在Pod中使用，比如设置环境变量、设置容器命令行参数、在Volume中创建配置文件等。
&lt;font color=#DC143C &gt;说 明 : &lt;/font&gt;
镜像以：buysbox镜像为例，如果能连国外的网，选择gcr.io/google_containers/busybox，不能连国外的网但能国内的外网使用：busybox:1.28.4这个镜像。
下面所有command里面都加了sleep,方便大家进容器查看配置是否起作用。&lt;/p&gt;
&lt;h3 id=&#34;31-configmap先创建好以下创建两种类型&#34;&gt;3.1 configmap先创建好,以下创建两种类型。&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm&lt;/code&gt;
&lt;code&gt;kubectl create configmap env-config --from-literal=log_level=INFO&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;32-环境参数注意busybox镜像选择及sleep时间test-podyaml&#34;&gt;3.2 环境参数，注意busybox镜像选择及sleep时间,test-pod.yaml。&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ &amp;quot;/bin/sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;sleep 60 ;env&amp;quot; ]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.type
      envFrom:
        - configMapRef:
            name: env-config
  restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f test-pod.yaml&lt;/code&gt;
进入对应的容器里，输入ENV则会包含如下结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SPECIAL_LEVEL_KEY=very
SPECIAL_TYPE_KEY=charm
log_level=INFO
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;33-命令行参数注意busybox镜像选择及sleep时间dapi-test-podyaml&#34;&gt;3.3 命令行参数，注意busybox镜像选择及sleep时间,dapi-test-pod.yaml。&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ &amp;quot;/bin/sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;sleep 60 ;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&amp;quot; ]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.type
  restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f dapi-test-pod.yaml&lt;/code&gt;
进入容器中，执行&lt;code&gt;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&lt;/code&gt;指令得到结果：
very charm&lt;/p&gt;
&lt;h3 id=&#34;34-volume将configmap作为文件或目录直接挂载注意busybox镜像选择及sleep时间当存在同名文件时直接覆盖掉vol-test-podyaml&#34;&gt;3.4 volume将ConfigMap作为文件或目录直接挂载，注意busybox镜像选择及sleep时间,当存在同名文件时，直接覆盖掉，vol-test-pod.yaml。&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: vol-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: [ &amp;quot;/bin/sh&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;sleep 60 ;cat /etc/config/special.how&amp;quot; ]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: special-config
  restartPolicy: Never
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f vol-test-pod.yaml&lt;/code&gt;
进入容器，cat的结果：very&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>glusterfs安装</title>
      <link>https://kubesphereio.com/post/k8s-glusterfs-install/</link>
      <pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/k8s-glusterfs-install/</guid>
      <description>&lt;h2 id=&#34;ceph介绍&#34;&gt;ceph介绍&lt;/h2&gt;
&lt;p&gt;ceph为块存储，以下通过cephadm安装ceph集群，ceph主要有三个基本角色：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OSD 用于集群中所有数据与对象的存储，处理集群数据的复制、恢复、回填、再均衡，并向其他osd守护进程发送心跳，然后向 Monitor 提供一些监控信息。&lt;/li&gt;
&lt;li&gt;Monitor 监控整个集群的状态，维护集群的 cluster MAP 二进制表，保证集群数据的一致性。&lt;/li&gt;
&lt;li&gt;MDS (可选) 为 Ceph 文件系统提供元数据计算、缓存与同步。MDS 进程并不是必须的进程，只有需要使用 CephFS 时，才需要配置 MDS 节点。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1环境准备&#34;&gt;1、环境准备&lt;/h2&gt;
&lt;h3 id=&#34;11节点规划&#34;&gt;1.1、节点规划&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;节点  &lt;/th&gt;
&lt;th&gt;os  &lt;/th&gt;
&lt;th&gt;IP   &lt;/th&gt;
&lt;th&gt;磁盘&lt;/th&gt;
&lt;th&gt;角色&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ceph1&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.13&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;cephadm，mgr, mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph2&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.14&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ceph3&lt;/td&gt;
&lt;td&gt;centos&lt;/td&gt;
&lt;td&gt;192.168.0.16&lt;/td&gt;
&lt;td&gt;vdc&lt;/td&gt;
&lt;td&gt;mon,osd&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;12修改hosts文件&#34;&gt;1.2、修改hosts文件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hostnamectl set-hostname ceph1  #节点1上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph2  #节点2上执行&lt;/li&gt;
&lt;li&gt;hostnamectl set-hostname ceph3  #节点3上执行&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;13配置hosts&#34;&gt;1.3、配置hosts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt; /etc/hosts &amp;lt;&amp;lt;EOF  
192.168.0.13 ceph1
192.168.0.14 ceph2
192.168.0.16 ceph3
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;14关闭防火墙&#34;&gt;1.4、关闭防火墙&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;三个节点都要执行&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;systemctl disable --now firewalld
setenforce 0
sed -i &#39;s/^SELINUX=.*/SELINUX=disabled/&#39; /etc/selinux/config
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;15时间同步&#34;&gt;1.5、时间同步&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;yum install -y chrony
systemctl enable --now chronyd

以ceph1作为时间参考，以下在三个节点都操作，添加server ceph1 prefer
vi /etc/chrony.conf
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
server ceph1 prefer

重启chrony
systemctl restart chronyd
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;16dockerlvm2和python3安装&#34;&gt;1.6、docker、lvm2和python3安装&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;三个节点都需要执行：
curl -fsSL get.docker.com -o get-docker.sh 
sh get-docker.sh
systemctl enable docker
systemctl restart docker

yum install -y python3

yum -y install lvm2
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;2cephadm安装ceph1&#34;&gt;2、cephadm安装(ceph1)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;官方下载方法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;git下载&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;git clone --single-branch --branch octopus https://github.com/ceph/ceph.git&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cp ceph/src/cephadm/cephadm ./&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cephadm安装(ceph1)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;./cephadm add-repo --release octopus
./cephadm install

mkdir -p /etc/ceph
cephadm bootstrap --mon-ip 192.168.0.13

安装完成提示：

             URL: https://ceph1:8443/
            User: admin
        Password: 86yvswzdzd

INFO:cephadm:You can access the Ceph CLI with:

        sudo /usr/sbin/cephadm shell --fsid 3861dbf4-fd47-11ea-a373-5254228af870 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

INFO:cephadm:Please consider enabling telemetry to help improve Ceph:

        ceph telemetry on

For more information see:

        https://docs.ceph.com/docs/master/mgr/telemetry/

INFO:cephadm:Bootstrap complete.
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;配置ceph.pub
将ceph1的/etc/ceph/ceph.pub放入到ceph2和ceph3的.ssh/authorized_keys,ceph2和ceph3下root目录下没有.ssh，得先创建，最低权限为700，authorized_keys最低权限600。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ceph指令生效&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;cephadm shell&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;添加节点&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ceph orch host add ceph3&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3osd部署&#34;&gt;3、OSD部署&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cephadm shell下执行&lt;/li&gt;
&lt;li&gt;若块设备没显示，可加&amp;ndash;refresh参数&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[ceph: root@ceph1 /]# ceph orch device ls
HOST   PATH      TYPE   SIZE  DEVICE_ID     MODEL  VENDOR  ROTATIONAL  AVAIL  REJECT REASONS
ceph1  /dev/vdc  hdd   50.0G  vol-e801hi3v  n/a    0x1af4  1           True
ceph1  /dev/vda  hdd   20.0G  i-2iwis9yr    n/a    0x1af4  1           False  locked
ceph1  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
ceph2  /dev/vdc  hdd   50.0G  vol-lqcchpox  n/a    0x1af4  1           True
ceph2  /dev/vda  hdd   20.0G  i-r53b2flp    n/a    0x1af4  1           False  locked
ceph2  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  locked, Insufficient space (&amp;lt;5GB)
ceph3  /dev/vdc  hdd    100G  vol-p0eksa5m  n/a    0x1af4  1           True
ceph3  /dev/vda  hdd   20.0G  i-f6njdmtq    n/a    0x1af4  1           False  locked
ceph3  /dev/vdb  hdd   4096M                n/a    0x1af4  1           False  Insufficient space (&amp;lt;5GB), locked
[ceph: root@ceph1 /]#

devcice确认true后，执行
[ceph: root@ceph1 /]# ceph orch apply osd --all-available-devices
Scheduled osd.all-available-devices update...
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;查看集群状态,ceph方式及运行容器方式&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;ceph方式：
[ceph: root@ceph1 /]# ceph -s
  cluster:
    id:     af1f33f0-fd69-11ea-8530-5254228af870
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 66s)
    mgr: ceph1.rbarkj(active, since 12m), standbys: ceph2.uzdnki
    osd: 3 osds: 3 up (since 14s), 3 in (since 14s)

  data:
    pools:   1 pools, 1 pgs
    objects: 1 objects, 0 B
    usage:   3.0 GiB used, 197 GiB / 200 GiB avail
    pgs:     1 active+clean

容器方式：
[root@ceph1 ceph]# docker ps -a
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES
b417841153a9        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-osd -…&amp;quot;   5 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-osd.2
9dbd720f4a6f        prom/prometheus:v2.18.1      &amp;quot;/bin/prometheus --c…&amp;quot;   6 minutes ago       Up 5 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-prometheus.ceph1
f860282adf85        prom/alertmanager:v0.20.0    &amp;quot;/bin/alertmanager -…&amp;quot;   6 minutes ago       Up 6 minutes                            ceph-af1f33f0-fd69-11ea-8530-5254228af870-alertmanager.ceph1
a6cb4b354f46        ceph/ceph-grafana:6.6.2      &amp;quot;/bin/sh -c &#39;grafana…&amp;quot;   16 minutes ago      Up 16 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-grafana.ceph1
b745f57895f3        prom/node-exporter:v0.18.1   &amp;quot;/bin/node_exporter …&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-node-exporter.ceph1
b00c1a2c7ef6        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-crash…&amp;quot;   17 minutes ago      Up 17 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-crash.ceph1
99c703f18f67        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mgr -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mgr.ceph1.rbarkj
e972d3d3f13a        ceph/ceph:v15                &amp;quot;/usr/bin/ceph-mon -…&amp;quot;   18 minutes ago      Up 18 minutes                           ceph-af1f33f0-fd69-11ea-8530-5254228af870-mon.ceph1

&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.ceph.com/en/latest/cephadm/install/&#34;&gt;https://docs.ceph.com/en/latest/cephadm/install/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Linux中nfs搭建</title>
      <link>https://kubesphereio.com/post/linux-nfs-install/</link>
      <pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/linux-nfs-install/</guid>
      <description>&lt;h1 id=&#34;linux-下nfs-服务器的搭建及配置&#34;&gt;Linux 下NFS 服务器的搭建及配置&lt;/h1&gt;
&lt;p&gt;nfs是网络存储文件系统，客户端通过网络访问不同主机上磁盘的数据，用于unix系统。&lt;/p&gt;
&lt;p&gt;演示nfs机器信息，分为服务端和客户端介绍，以下服务端的ip请根据自己环境来替换。
服务端：192.168.0.9，客户端：192.168.0.10&lt;/p&gt;
&lt;h2 id=&#34;11服务端安装19216809&#34;&gt;1.1服务端安装（192.168.0.9）&lt;/h2&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;centos:&lt;/font&gt;
&lt;code&gt;sudo yum install nfs-utils -y&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;ubuntu16.04:&lt;/font&gt;
&lt;code&gt;sudo apt install nfs-kernel-server&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;12服务端配置&#34;&gt;1.2服务端配置&lt;/h2&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;centos:&lt;/font&gt;
rpcbind服务的开机自启和启动：
&lt;code&gt;sudo systemctl enable rpcbind;sudo systemctl restart rpcbind&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;nfs服务的开机自启和启动：
&lt;code&gt;sudo systemctl enable nfs;sudo systemctl restart nfs&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;ubuntu16.04:&lt;/font&gt;
&lt;code&gt;sudo systemctl enable nfs-kernel-server;sudo systemctl restart nfs-kernel-server&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;13配置共享目录&#34;&gt;1.3配置共享目录&lt;/h2&gt;
&lt;p&gt;目录为服务端目录，后续存储的数据在该目录下
&lt;code&gt;sudo mkdir -p /nfsdatas&lt;/code&gt;
&lt;code&gt;sudo chmod 755 /nfsdatas&lt;/code&gt;
&lt;font color=#DC143C &gt;重要:&lt;/font&gt; 根据上面创建的目录，相应配置导出目录
&lt;code&gt;sudo vi /etc/exports&lt;/code&gt;
添加如下配置再保存，重启nfs服务即可：
/nfsdatas/ 192.168.0.0/16(rw,sync,no_root_squash,no_all_squash)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;/nfsdatas:共享目录位置。&lt;/li&gt;
&lt;li&gt;192.168.0.0/24：客户端IP范围，*代表所有。&lt;/li&gt;
&lt;li&gt;rw：权限设置，可读可写。&lt;/li&gt;
&lt;li&gt;sync：同步共享目录。&lt;/li&gt;
&lt;li&gt;no_root_squash: 可以使用root授权。&lt;/li&gt;
&lt;li&gt;no_all_squash: 可以使用普通用户授权&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;centos:&lt;/font&gt;
&lt;code&gt;systemctl restart nfs&lt;/code&gt;
&lt;font color=#DC143C &gt;ubuntu16.04:&lt;/font&gt;
&lt;code&gt;sudo systemctl restart nfs-kernel-server&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;21客户端安装192168010&#34;&gt;2.1客户端安装（192.168.0.10）&lt;/h2&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;centos:&lt;/font&gt;
&lt;code&gt;sudo yum install nfs-utils -y&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;ubuntu16.04:&lt;/font&gt;
&lt;code&gt;sudo apt install nfs-common&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;22客户端开机自启和启动即可&#34;&gt;2.2客户端开机自启和启动即可&lt;/h2&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;centos:&lt;/font&gt;
&lt;code&gt;sudo systemctl enable rpcbind&lt;/code&gt;
&lt;code&gt;sudo systemctl start rpcbind&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;font color=#DC143C &gt;ubuntu16.04:&lt;/font&gt;
&lt;code&gt;sudo systemctl enable nfs-common;sudo systemctl restart nfs-common&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;23客户端验证及测试&#34;&gt;2.3客户端验证及测试&lt;/h2&gt;
&lt;p&gt;检查服务端的共享目录：
&lt;code&gt;showmount -e 192.168.0.9&lt;/code&gt;
客户端创建目录
&lt;code&gt;sudo mkdir -p /tmp/nfsdata&lt;/code&gt;
挂载指令：
&lt;code&gt;sudo mount -t nfs 192.168.0.9:/nfsdatas /tmp/nfsdata&lt;/code&gt;
然后进入/tmp/nfsdata目录下，新建文件
&lt;code&gt;sudo touch test&lt;/code&gt;
之后在nfs服务端192.168.0.9的/nfsdatas目录查看是否有test文件
卸载指令：不要在/tmp/nfsdata目录下执行卸载指令。
&lt;code&gt;umount /tmp/nfsdata&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;cenots和ubuntu脚本部署&#34;&gt;cenots和ubuntu脚本部署&lt;/h2&gt;
&lt;p&gt;下面内容添加：vi nfs-install.sh
加权限和执行：chmod +x nfs-install.sh &amp;amp;&amp;amp; ./nfs-install.sh
以下脚本安装的服务端目录为：/nfsdatas,如果需要修改的话，脚本内容需要修改。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/bash

function centostest(){
    yum clean all;yum makecache
    yum install nfs-utils -y
    systemctl enable rpcbind;sudo systemctl restart rpcbind
    systemctl enable nfs;sudo systemctl restart nfs
    mkdir -p /nfsdatas
    chmod 755 /nfsdatas
    echo &amp;quot;/nfsdatas/ 192.168.0.0/16(rw,sync,no_root_squash,no_all_squash)&amp;quot; &amp;gt; /etc/exports
    systemctl restart nfs
    showmount -e localhost
    if [[ $? -eq 0 ]]; then
        #statements
        str=&amp;quot;successsful!&amp;quot;
        echo -e &amp;quot;\033[30;47m$str\033[0m&amp;quot;  
    else
        str=&amp;quot;failed!&amp;quot;
        echo -e &amp;quot;\033[31;47m$str\033[0m&amp;quot;
        exit
    fi
}

function ubuntutest(){
    apt-get update
    sudo apt install nfs-kernel-server
    sudo systemctl enable nfs-kernel-server;sudo systemctl restart nfs-kernel-server
    mkdir -p /nfsdatas
    chmod 755 /nfsdatas
    echo &amp;quot;/nfsdatas/ 192.168.0.0/16(rw,sync,no_root_squash,no_all_squash)&amp;quot; &amp;gt; /etc/exports
    sudo systemctl restart nfs-kernel-server
    showmount -e localhost
    if [[ $? -eq 0 ]]; then
        #statements
        str=&amp;quot;successsful!&amp;quot;
        echo -e &amp;quot;\033[30;47m$str\033[0m&amp;quot;  
    else
        str=&amp;quot;failed!&amp;quot;
        echo -e &amp;quot;\033[31;47m$str\033[0m&amp;quot;
        exit
    fi
}

cat /etc/redhat-release

if [[ $? -eq 0 ]]; then
    str=&amp;quot;centos!&amp;quot;
    echo -e &amp;quot;\033[30;47m$str\033[0m&amp;quot;
    centostest
else
    str=&amp;quot;ubuntu!&amp;quot;
    echo -e &amp;quot;\033[31;47m$str\033[0m&amp;quot;
    ubuntutest
fi
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Markdown语法入门</title>
      <link>https://kubesphereio.com/post/markdown-syntax/</link>
      <pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/markdown-syntax/</guid>
      <description>&lt;p&gt;This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.&lt;/p&gt;
&lt;h2 id=&#34;headings&#34;&gt;Headings&lt;/h2&gt;
&lt;p&gt;The following HTML &lt;code&gt;&amp;lt;h1&amp;gt;&lt;/code&gt;—&lt;code&gt;&amp;lt;h6&amp;gt;&lt;/code&gt; elements represent six levels of section headings. &lt;code&gt;&amp;lt;h1&amp;gt;&lt;/code&gt; is the highest section level while &lt;code&gt;&amp;lt;h6&amp;gt;&lt;/code&gt; is the lowest.&lt;/p&gt;
&lt;h1 id=&#34;h1&#34;&gt;H1&lt;/h1&gt;
&lt;h2 id=&#34;h2&#34;&gt;H2&lt;/h2&gt;
&lt;h3 id=&#34;h3&#34;&gt;H3&lt;/h3&gt;
&lt;h4 id=&#34;h4&#34;&gt;H4&lt;/h4&gt;
&lt;h5 id=&#34;h5&#34;&gt;H5&lt;/h5&gt;
&lt;h6 id=&#34;h6&#34;&gt;H6&lt;/h6&gt;
&lt;h2 id=&#34;paragraph&#34;&gt;Paragraph&lt;/h2&gt;
&lt;p&gt;Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.&lt;/p&gt;
&lt;p&gt;Itatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.&lt;/p&gt;
&lt;h2 id=&#34;blockquotes&#34;&gt;Blockquotes&lt;/h2&gt;
&lt;p&gt;The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a &lt;code&gt;footer&lt;/code&gt; or &lt;code&gt;cite&lt;/code&gt; element, and optionally with in-line changes such as annotations and abbreviations.&lt;/p&gt;
&lt;h4 id=&#34;blockquote-without-attribution&#34;&gt;Blockquote without attribution&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Tiam, ad mint andaepu dandae nostion secatur sequo quae.
&lt;strong&gt;Note&lt;/strong&gt; that you can use &lt;em&gt;Markdown syntax&lt;/em&gt; within a blockquote.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;blockquote-with-attribution&#34;&gt;Blockquote with attribution&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Don&amp;rsquo;t communicate by sharing memory, share memory by communicating.&lt;/p&gt;
— &lt;cite&gt;Rob Pike&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;tables&#34;&gt;Tables&lt;/h2&gt;
&lt;p&gt;Tables aren&amp;rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Age&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Bob&lt;/td&gt;
&lt;td&gt;27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Alice&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;inline-markdown-within-tables&#34;&gt;Inline Markdown within tables&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Inline   &lt;/th&gt;
&lt;th&gt;Markdown   &lt;/th&gt;
&lt;th&gt;In   &lt;/th&gt;
&lt;th&gt;Table&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;italics&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;bold&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;del&gt;strikethrough&lt;/del&gt;   &lt;/td&gt;
&lt;td&gt;&lt;code&gt;code&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;code-blocks&#34;&gt;Code Blocks&lt;/h2&gt;
&lt;h4 id=&#34;code-block-with-backticks&#34;&gt;Code block with backticks&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;html
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang=&amp;quot;en&amp;quot;&amp;gt;
&amp;lt;head&amp;gt;
  &amp;lt;meta charset=&amp;quot;UTF-8&amp;quot;&amp;gt;
  &amp;lt;title&amp;gt;Example HTML5 Document&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
  &amp;lt;p&amp;gt;Test&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;code-block-indented-with-four-spaces&#34;&gt;Code block indented with four spaces&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang=&amp;quot;en&amp;quot;&amp;gt;
&amp;lt;head&amp;gt;
  &amp;lt;meta charset=&amp;quot;UTF-8&amp;quot;&amp;gt;
  &amp;lt;title&amp;gt;Example HTML5 Document&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
  &amp;lt;p&amp;gt;Test&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;code-block-with-hugos-internal-highlight-shortcode&#34;&gt;Code block with Hugo&amp;rsquo;s internal highlight shortcode&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-html&#34; data-lang=&#34;html&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;!DOCTYPE html&amp;gt;&lt;/span&gt;
&amp;lt;&lt;span style=&#34;color:#f92672&#34;&gt;html&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;lang&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;en&amp;#34;&lt;/span&gt;&amp;gt;
&amp;lt;&lt;span style=&#34;color:#f92672&#34;&gt;head&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span style=&#34;color:#f92672&#34;&gt;meta&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;charset&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;UTF-8&amp;#34;&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span style=&#34;color:#f92672&#34;&gt;title&lt;/span&gt;&amp;gt;Example HTML5 Document&amp;lt;/&lt;span style=&#34;color:#f92672&#34;&gt;title&lt;/span&gt;&amp;gt;
&amp;lt;/&lt;span style=&#34;color:#f92672&#34;&gt;head&lt;/span&gt;&amp;gt;
&amp;lt;&lt;span style=&#34;color:#f92672&#34;&gt;body&lt;/span&gt;&amp;gt;
  &amp;lt;&lt;span style=&#34;color:#f92672&#34;&gt;p&lt;/span&gt;&amp;gt;Test&amp;lt;/&lt;span style=&#34;color:#f92672&#34;&gt;p&lt;/span&gt;&amp;gt;
&amp;lt;/&lt;span style=&#34;color:#f92672&#34;&gt;body&lt;/span&gt;&amp;gt;
&amp;lt;/&lt;span style=&#34;color:#f92672&#34;&gt;html&lt;/span&gt;&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;list-types&#34;&gt;List Types&lt;/h2&gt;
&lt;h4 id=&#34;ordered-list&#34;&gt;Ordered List&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;First item&lt;/li&gt;
&lt;li&gt;Second item&lt;/li&gt;
&lt;li&gt;Third item&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;unordered-list&#34;&gt;Unordered List&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;List item&lt;/li&gt;
&lt;li&gt;Another item&lt;/li&gt;
&lt;li&gt;And another item&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;nested-list&#34;&gt;Nested list&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Item&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;First Sub-item&lt;/li&gt;
&lt;li&gt;Second Sub-item&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;other-elements--abbr-sub-sup-kbd-mark&#34;&gt;Other Elements — abbr, sub, sup, kbd, mark&lt;/h2&gt;
&lt;p&gt;&lt;abbr title=&#34;Graphics Interchange Format&#34;&gt;GIF&lt;/abbr&gt; is a bitmap image format.&lt;/p&gt;
&lt;p&gt;H&lt;sub&gt;2&lt;/sub&gt;O&lt;/p&gt;
&lt;p&gt;X&lt;sup&gt;n&lt;/sup&gt; + Y&lt;sup&gt;n&lt;/sup&gt; = Z&lt;sup&gt;n&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Press &lt;kbd&gt;&lt;kbd&gt;CTRL&lt;/kbd&gt;+&lt;kbd&gt;ALT&lt;/kbd&gt;+&lt;kbd&gt;Delete&lt;/kbd&gt;&lt;/kbd&gt; to end the session.&lt;/p&gt;
&lt;p&gt;Most &lt;mark&gt;salamanders&lt;/mark&gt; are nocturnal, and hunt for insects, worms, and other small creatures.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The above quote is excerpted from Rob Pike&amp;rsquo;s &lt;a href=&#34;https://www.youtube.com/watch?v=PAAkCSZUG1c&#34;&gt;talk&lt;/a&gt; during Gopherfest, November 18, 2015. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</description>
    </item>
    
    <item>
      <title>终端录屏软件入门</title>
      <link>https://kubesphereio.com/post/asciinema/</link>
      <pubDate>Tue, 05 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/asciinema/</guid>
      <description>&lt;h2 id=&#34;1-asciinema-终端录屏&#34;&gt;1. asciinema 终端录屏&lt;/h2&gt;
&lt;p&gt;asciinema是一个在终端下非常棒的录制分享软件，基于文本的录屏工具，对终端输入输出进行捕捉， 然后以文本的形式来记录和回放！对多种系统都支持。&lt;/p&gt;
&lt;h2 id=&#34;2-asciinema安装&#34;&gt;2. asciinema安装&lt;/h2&gt;
&lt;p&gt;各种安装方法如连接，以下按centos为例进行：
&lt;a href=&#34;https://asciinema.org/docs/installation&#34;&gt;https://asciinema.org/docs/installation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;yum install -y epel-release &lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;yum install -y asciinema&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;检查是否成功，看asciinema版本&lt;/p&gt;
&lt;p&gt;&lt;code&gt;asciinema --version&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;3-指令及常见用法&#34;&gt;3. 指令及常见用法&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;[root@i-cvswezkk ~]# asciinema --help
usage: asciinema [-h] [--version] {rec,play,upload,auth} ...

Record and share your terminal sessions, the right way.

positional arguments:
  {rec,play,upload,auth}
    rec         Record terminal session                                 # 记录终端会话
    play        Replay terminal session                                 # 播放重播终端会话
    upload      Upload locally saved terminal session to asciinema.org  #上传本地保存的终端会话到asciinema.org
    auth                Manage recordings on asciinema.org account      # 管理asciinema.org帐户上的记录

optional arguments:
  -h, --help            show this help message and exit
  --version             show program&#39;s version number and exit
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;31-各个指令具体含义&#34;&gt;3.1 各个指令具体含义：&lt;/h3&gt;
&lt;p&gt;.cast和.json文件是一样的，注册asciinema就需要邮箱即可，然后邮箱认证即可，可能收到邮箱的时间长点。
记录终端并将其上传到asciinema.org&lt;/p&gt;
&lt;p&gt;&lt;code&gt;asciinema rec&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;将终端记录到本地文件&lt;/p&gt;
&lt;p&gt;&lt;code&gt;asciinema rec demo.cast&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;记录终端并将其上传到asciinema.org，指定标题：&amp;ldquo;my aslog1&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;asciinema rec -t &amp;quot;my aslog1&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;将终端记录到本地文件，将空闲时间限制到最大2.5秒&lt;/p&gt;
&lt;p&gt;&lt;code&gt;asciinema rec -i 2.5 demo.cast&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;从本地文件重放终端记录&lt;/p&gt;
&lt;p&gt;&lt;code&gt;asciinema play demo.cast&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;asciinema还提供了一个可以管理asciinema个人账户所拥有的会话文件的功能, 命令为&lt;/p&gt;
&lt;p&gt;&lt;code&gt;asciinema auth&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;执行命令&amp;quot;asciinema auth&amp;quot;命令后, 会返回一个网络地址, 点击这个地址就会打开asciinema个人账号注册界面
本地修改记录文件&lt;/p&gt;
&lt;p&gt;&lt;code&gt;vi demo.cast&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;本地修改记录文件再重新上传至asciinema.org，&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# asciinema upload /root/260132.json
View the recording at:

    https://asciinema.org/a/M1K9rrUHl5D0q3TOVDhtRcJIn
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;4浏览器访问效果&#34;&gt;4.浏览器访问效果&lt;/h2&gt;
&lt;p&gt;双击对应的一个录屏，有share  download settings，且settings可以设置参数
&lt;img src=&#34;https://kubesphereio.com/img/asciinema.png&#34; alt=&#34;asciinema-demo效果图&#34;&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title></title>
      <link>https://kubesphereio.com/post/terraform-docking-qingcloud-installation-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kubesphereio.com/post/terraform-docking-qingcloud-installation-k8s/</guid>
      <description>&lt;p&gt;title = &amp;ldquo;terraform对接qingcloud安装K8s&amp;rdquo;
date = &amp;ldquo;2019-08-13&amp;rdquo;
tags = [
&amp;ldquo;iptables&amp;rdquo;,
&amp;ldquo;Linux tools&amp;rdquo;
]
+++&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用Terraform作用是不需要用户在iaas平台上单独创建机器，配置好参数之后由Terraform自动创建，实现一键自动化部署。&lt;/li&gt;
&lt;li&gt;单节点的安装主要分为三部分，var.tf、kubesphere.tf和install.sh，以下文件内容只需要修改API密钥值就可以部署Ks集群。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;执行步骤说明&#34;&gt;执行步骤说明&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;安装terraform工具&lt;/li&gt;
&lt;li&gt;创建一个目录，把var.tf、kubesphere.tf和install.sh三个文件放到该目录下。&lt;/li&gt;
&lt;li&gt;修改iaas的API密码&lt;/li&gt;
&lt;li&gt;进入目录下执行&lt;code&gt;terraform init&lt;/code&gt;指令，显示成功。&lt;/li&gt;
&lt;li&gt;init成功之后，然后执行&lt;code&gt;terraform apply&lt;/code&gt;即可就开始创建机器，安装Ks。&lt;/li&gt;
&lt;li&gt;删除机器指令操作为&lt;code&gt;terraform destroy&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;1terraform安装单独一台机器&#34;&gt;1、terraform安装（单独一台机器）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;以centos操作系统为例来安装，需要执行以下指令即可。&lt;/li&gt;
&lt;li&gt;其余操作系统，参考&lt;a href=&#34;https://learn.hashicorp.com/tutorials/terraform/install-cli?in=terraform/aws-get-started&#34;&gt;terraform&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sudo yum install -y yum-utils
sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo
sudo yum -y install terraform
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;是否安装成功及版本输出&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;terraform version&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;2vartf文件说明&#34;&gt;2、var.tf文件说明&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;access_key和secret_key为必填项。点击iaas用户&amp;ndash;》API密钥&amp;ndash;》创建即可，然后把两个参数填入到下面的配置文件中。zone可以根据自己需求来修改，默认pek3a。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;terraform {
  required_providers {
    qingcloud = {
      source = &amp;quot;shaowenchen/qingcloud&amp;quot;
      version = &amp;quot;1.2.6&amp;quot;
    }
  }
}

variable &amp;quot;access_key&amp;quot; {
  default = &amp;quot;***&amp;quot;
}

variable &amp;quot;secret_key&amp;quot; {
  default = &amp;quot;***&amp;quot;
}

variable &amp;quot;zone&amp;quot; {
  default = &amp;quot;pek3a&amp;quot;
}

provider &amp;quot;qingcloud&amp;quot; {
  access_key = &amp;quot;${var.access_key}&amp;quot;
  secret_key = &amp;quot;${var.secret_key}&amp;quot;
  zone = &amp;quot;${var.zone}&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;3kubespheretf文件说明&#34;&gt;3、kubesphere.tf文件说明&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;resource qingcloud_eip为创建外网ip，也可以用已存在外网IP，如果用已存在外网ip，就不需要qingcloud_eip resource模块。&lt;/li&gt;
&lt;li&gt;qingcloud_security_group为创建防火墙，也可以使用已存在防火墙，同理。&lt;/li&gt;
&lt;li&gt;qingcloud_security_group_rule为创建防火墙开放的端口。&lt;/li&gt;
&lt;li&gt;qingcloud_keypair为密钥创建，此处注释掉，用密码形式。&lt;/li&gt;
&lt;li&gt;qingcloud_instance创建机器，包含名字，操作系统，内存，cpu，磁盘，密码，绑定外网IP，防火墙，子网和类型等。&lt;/li&gt;
&lt;li&gt;null_resource 和install_kubesphere包含文件拷贝及执行命令。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;resource &amp;quot;qingcloud_eip&amp;quot; &amp;quot;init&amp;quot;{
  name = &amp;quot;tf_eip&amp;quot;
  description = &amp;quot;&amp;quot;
  billing_mode = &amp;quot;traffic&amp;quot;
  bandwidth = 20
  need_icp = 0
}

resource &amp;quot;qingcloud_security_group&amp;quot; &amp;quot;basic&amp;quot;{
  name = &amp;quot;防火墙&amp;quot;
  description = &amp;quot;这是第一个防火墙&amp;quot;
}

resource &amp;quot;qingcloud_security_group_rule&amp;quot; &amp;quot;openport&amp;quot; {
  security_group_id = &amp;quot;${qingcloud_security_group.basic.id}&amp;quot;
  protocol = &amp;quot;tcp&amp;quot;
  priority = 0
  action = &amp;quot;accept&amp;quot;
  direction = 0
  from_port = 22
  to_port = 40000
}

# qingcloud_keypair upload an SSH public key
# In this example, upload ~/.ssh/id_rsa.pub content.
# You may not have this file in your system, you will need to create your own SSH key.
#resource &amp;quot;qingcloud_keypair&amp;quot; &amp;quot;arthur&amp;quot;{
#  name = &amp;quot;arthur&amp;quot;
#  public_key = &amp;quot;${file(&amp;quot;~/.ssh/id_rsa.pub&amp;quot;)}&amp;quot;
#}

resource &amp;quot;qingcloud_instance&amp;quot; &amp;quot;init&amp;quot;{
  count = 1
  name = &amp;quot;master-${count.index}&amp;quot;
  image_id = &amp;quot;centos76x64a&amp;quot;
  cpu = &amp;quot;16&amp;quot;
  memory = &amp;quot;32768&amp;quot;
  instance_class = &amp;quot;0&amp;quot;
  managed_vxnet_id=&amp;quot;vxnet-0&amp;quot;
#  keypair_ids = [&amp;quot;${qingcloud_keypair.arthur.id}&amp;quot;]
  login_passwd = &amp;quot;Qcloud@123&amp;quot;
  security_group_id =&amp;quot;${qingcloud_security_group.basic.id}&amp;quot;
  eip_id = &amp;quot;${qingcloud_eip.init.id}&amp;quot;
}

resource &amp;quot;null_resource&amp;quot; &amp;quot;install_kubesphere&amp;quot; {
  provisioner &amp;quot;file&amp;quot; {
    destination = &amp;quot;./install.sh&amp;quot;
    source      = &amp;quot;./install.sh&amp;quot;

    connection {
      type        = &amp;quot;ssh&amp;quot;
      user        = &amp;quot;root&amp;quot;
      host        = &amp;quot;${qingcloud_eip.init.addr}&amp;quot;
      password    = &amp;quot;Qcloud@123&amp;quot;
#      private_key = &amp;quot;${file(&amp;quot;~/.ssh/id_rsa&amp;quot;)}&amp;quot;
      port        = &amp;quot;22&amp;quot;
    }
  }

  provisioner &amp;quot;remote-exec&amp;quot; {
    inline = [
      &amp;quot;sh install.sh&amp;quot;
    ]

    connection {
      type        = &amp;quot;ssh&amp;quot;
      user        = &amp;quot;root&amp;quot;
      host        = &amp;quot;${qingcloud_eip.init.addr}&amp;quot;
      password    = &amp;quot;Qcloud@123&amp;quot;
 #     private_key = &amp;quot;${file(&amp;quot;~/.ssh/id_rsa&amp;quot;)}&amp;quot;
      port        = &amp;quot;22&amp;quot;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;4installsh文件说明&#34;&gt;4、install.sh文件说明&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;具体执行命令&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;curl -O -k https://kubernetes.pek3b.qingstor.com/tools/kubekey/kk
chmod +x kk
yum install -y vim openssl socat conntrack ipset
echo -e &#39;yes\n&#39; | /root/kk create cluster --with-kubesphere
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;5执行及删除&#34;&gt;5、执行及删除&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;进入目录，执行init，出现successfully字样。&lt;/li&gt;
&lt;li&gt;terraform apply, 输入yes开始安装。&lt;/li&gt;
&lt;li&gt;terraform destroy&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[root@node4 ~]# cd terraform
[root@node4 terraform]# ls
install.sh  kubesphere.tf  var.tf
[root@node4 terraform]# terraform init

Initializing the backend...

Initializing provider plugins...
Terraform has been successfully initialized!

[root@node4 terraform]# terraform apply

An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create
Enter a value: yes
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;6参考&#34;&gt;6、参考&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/yunify/terraform-provider-qingcloud&#34;&gt;https://github.com/yunify/terraform-provider-qingcloud&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>